# 企业级大模型落地指南

## 背景介绍

在过去的几年中，大型语言模型 (LLM) 展现出了惊人的泛化能力。然而，在企业级落地时，直接裸调 API 往往会面临严重的“幻觉”问题（Hallucination），即模型会一本正经地胡说八道。

## RAG 架构的优势

为了解决幻觉问题，业界提出了 RAG (Retrieval-Augmented Generation，检索增强生成) 架构。其核心理念是：

1. **外挂知识库**：将企业的专有数据（如财务报表、产品手册、规章制度）通过 Embedding 向量化。
2. **相关性检索**：当用户提问时，先去知识库里搜索与提问最相关的 Top-K 个段落。
3. **上下文增强**：将检索到的段落连同用户的问题一起送入 LLM 模型，强制要求大模型“基于给定的上下文作答”。

## 高级 RAG 技巧

传统的朴素 RAG 往往会在检索步骤（Retriever）丢失很多重要的语义信息。为了提升召回率，我们通常需要：

- **混合检索 (Hybrid Search)**：结合文本词法特征的 BM25 算法和语义特征的 Dense Vector 算法，再利用 RRF (Reciprocal Rank Fusion) 算法合并。
- **重排序 (Reranking)**：利用基于 Cross-Encoder 架构的轻量级模型对召回结果进行二次排序，提升 Top-1 的准确度。
- **查询改写 (Query Rewriting)**：对于用户模糊简短的口语化提问，先使用大模型将其拓展、改写为更加完善的搜索语句。
