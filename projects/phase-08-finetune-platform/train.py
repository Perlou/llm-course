"""
è®­ç»ƒè„šæœ¬
æ‰§è¡Œ LoRA/QLoRA å¾®è°ƒ
"""

import argparse
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from rich.console import Console
from rich.table import Table

from scripts.training_utils import (
    load_config,
    get_peft_config,
    get_quantization_config,
    get_training_arguments,
)
from scripts.model_utils import load_base_model, apply_lora, get_model_info


console = Console()


def create_dataset(data_config, tokenizer, max_seq_length: int):
    """åˆ›å»ºè®­ç»ƒæ•°æ®é›†"""
    from datasets import load_dataset

    train_file = data_config.get("train_file", "./data/processed/train.json")
    eval_file = data_config.get("eval_file", "./data/processed/eval.json")

    dataset = load_dataset(
        "json",
        data_files={
            "train": train_file,
            "eval": eval_file if os.path.exists(eval_file) else train_file,
        },
    )

    def format_example(example):
        """æ ¼å¼åŒ–å•ä¸ªæ ·æœ¬"""
        messages = example.get("messages", [])
        text = ""
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                text += "<|im_start|>user\n" + content + "<|im_end|>\n"
            elif role == "assistant":
                text += "<|im_start|>assistant\n" + content + "<|im_end|>\n"
        return {"text": text}

    dataset = dataset.map(format_example)

    def tokenize(example):
        return tokenizer(
            example["text"],
            truncation=True,
            max_length=max_seq_length,
            padding="max_length",
        )

    dataset = dataset.map(tokenize, batched=True)

    return dataset


def main():
    parser = argparse.ArgumentParser(description="LoRA/QLoRA å¾®è°ƒè®­ç»ƒ")
    parser.add_argument("--config", "-c", default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®ï¼‰")
    parser.add_argument("--resume", help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")

    args = parser.parse_args()

    console.print("\n[bold blue]ğŸš€ LoRA å¾®è°ƒè®­ç»ƒ[/bold blue]\n")

    # åŠ è½½é…ç½®
    console.print(f"åŠ è½½é…ç½®: {args.config}")
    config = load_config(args.config)

    model_cfg = config.get("model", {})
    lora_cfg = config.get("lora", {})
    quant_cfg = config.get("quantization", {})
    training_cfg = config.get("training", {})
    data_cfg = config.get("data", {})

    # è¾“å‡ºç›®å½•
    output_dir = args.output or training_cfg.get("output_dir", "./outputs")
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    model_name = model_cfg.get("name", "Qwen/Qwen2.5-1.5B-Instruct")
    quant_config = get_quantization_config(quant_cfg)

    model, tokenizer = load_base_model(
        model_name,
        quantization_config=quant_config,
        trust_remote_code=model_cfg.get("trust_remote_code", True),
    )

    # åº”ç”¨ LoRA
    lora_config = get_peft_config(lora_cfg)
    model = apply_lora(model, lora_config)

    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    info = get_model_info(model)

    table = Table(title="æ¨¡å‹ä¿¡æ¯")
    table.add_column("å±æ€§", style="cyan")
    table.add_column("å€¼", style="green")

    table.add_row("æ€»å‚æ•°", f"{info['total_parameters']:,}")
    table.add_row("å¯è®­ç»ƒå‚æ•°", f"{info['trainable_parameters']:,}")
    table.add_row("å¯è®­ç»ƒæ¯”ä¾‹", f"{info['trainable_percentage']:.2f}%")
    table.add_row("æ•°æ®ç±»å‹", info["dtype"])
    table.add_row("è®¾å¤‡", info["device"])

    console.print(table)

    # åˆ›å»ºæ•°æ®é›†
    console.print("\nå‡†å¤‡æ•°æ®é›†...")
    max_seq_length = training_cfg.get("max_seq_length", 1024)
    dataset = create_dataset(data_cfg, tokenizer, max_seq_length)

    console.print(f"è®­ç»ƒé›†: {len(dataset['train'])} æ ·æœ¬")
    console.print(f"éªŒè¯é›†: {len(dataset['eval'])} æ ·æœ¬")

    # è®­ç»ƒå‚æ•°
    training_args = get_training_arguments(training_cfg, output_dir)

    # åˆ›å»º Trainer
    from trl import SFTTrainer

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["eval"],
        tokenizer=tokenizer,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
    )

    # å¼€å§‹è®­ç»ƒ
    console.print("\n[bold green]å¼€å§‹è®­ç»ƒ...[/bold green]\n")

    if args.resume:
        trainer.train(resume_from_checkpoint=args.resume)
    else:
        trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(output_dir, "checkpoint-final")
    trainer.save_model(final_path)
    console.print(f"\n[green]âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: {final_path}[/green]")


if __name__ == "__main__":
    main()
