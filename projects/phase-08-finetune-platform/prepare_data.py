"""
æ•°æ®å‡†å¤‡è„šæœ¬
å¤„ç†å’ŒéªŒè¯è®­ç»ƒæ•°æ®
"""

import argparse
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from rich.console import Console
from rich.table import Table

from scripts.dataset_utils import (
    load_json,
    save_json,
    convert_alpaca_to_messages,
    convert_sharegpt_to_messages,
    validate_sample,
    split_dataset,
    get_dataset_stats,
    format_prompt,
)


console = Console()


def main():
    parser = argparse.ArgumentParser(description="æ•°æ®å‡†å¤‡è„šæœ¬")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", default="./data/processed", help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "--format",
        "-f",
        choices=["alpaca", "sharegpt"],
        default="alpaca",
        help="è¾“å…¥æ•°æ®æ ¼å¼",
    )
    parser.add_argument("--train-ratio", type=float, default=0.9, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--preview", type=int, default=2, help="é¢„è§ˆæ ·æœ¬æ•°")
    parser.add_argument("--max-samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°")

    args = parser.parse_args()

    console.print("\n[bold blue]ğŸ“Š æ•°æ®å‡†å¤‡å·¥å…·[/bold blue]\n")

    # åŠ è½½æ•°æ®
    console.print(f"åŠ è½½æ•°æ®: {args.input}")
    try:
        data = load_json(args.input)
    except Exception as e:
        console.print(f"[red]åŠ è½½å¤±è´¥: {e}[/red]")
        return

    console.print(f"åŸå§‹æ ·æœ¬æ•°: {len(data)}")

    # é™åˆ¶æ ·æœ¬æ•°
    if args.max_samples and len(data) > args.max_samples:
        data = data[: args.max_samples]
        console.print(f"é™åˆ¶åæ ·æœ¬æ•°: {len(data)}")

    # éªŒè¯æ•°æ®
    valid_data = []
    for sample in data:
        if validate_sample(sample, args.format):
            valid_data.append(sample)

    console.print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(valid_data)}")

    if len(valid_data) < len(data):
        console.print(
            f"[yellow]è¿‡æ»¤æ‰ {len(data) - len(valid_data)} ä¸ªæ— æ•ˆæ ·æœ¬[/yellow]"
        )

    # è½¬æ¢æ ¼å¼
    console.print(f"\nè½¬æ¢æ ¼å¼: {args.format} -> messages")
    converted_data = []

    for sample in valid_data:
        if args.format == "alpaca":
            converted = convert_alpaca_to_messages(sample)
        else:
            converted = convert_sharegpt_to_messages(sample)
        converted_data.append(converted)

    # åˆ†å‰²æ•°æ®é›†
    train_data, eval_data = split_dataset(converted_data, args.train_ratio)

    console.print(f"\næ•°æ®é›†åˆ†å‰²:")
    console.print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    console.print(f"  éªŒè¯é›†: {len(eval_data)} æ ·æœ¬")

    # ä¿å­˜æ•°æ®
    os.makedirs(args.output, exist_ok=True)

    train_path = os.path.join(args.output, "train.json")
    eval_path = os.path.join(args.output, "eval.json")

    save_json(train_data, train_path)
    save_json(eval_data, eval_path)

    console.print(f"\n[green]âœ… æ•°æ®ä¿å­˜å®Œæˆ[/green]")
    console.print(f"  è®­ç»ƒé›†: {train_path}")
    console.print(f"  éªŒè¯é›†: {eval_path}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = get_dataset_stats(valid_data, args.format)

    table = Table(title="æ•°æ®é›†ç»Ÿè®¡")
    table.add_column("æŒ‡æ ‡", style="cyan")
    table.add_column("å€¼", style="green")

    table.add_row("æ€»æ ·æœ¬æ•°", str(stats["total_samples"]))
    table.add_row("æœ‰æ•ˆæ ·æœ¬æ•°", str(stats["valid_samples"]))
    table.add_row("å¹³å‡æŒ‡ä»¤é•¿åº¦", str(stats["avg_instruction_len"]))
    table.add_row("å¹³å‡è¾“å‡ºé•¿åº¦", str(stats["avg_output_len"]))

    console.print("\n")
    console.print(table)

    # é¢„è§ˆæ ·æœ¬
    if args.preview > 0:
        console.print(f"\n[bold]æ ·æœ¬é¢„è§ˆ:[/bold]\n")
        for i, sample in enumerate(valid_data[: args.preview]):
            console.print(f"--- æ ·æœ¬ {i + 1} ---")
            console.print(format_prompt(sample, args.format))
            console.print()


if __name__ == "__main__":
    main()
