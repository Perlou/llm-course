#!/usr/bin/env python3
"""
MediMind - æ•°æ®åŠ è½½è„šæœ¬

åŠ è½½åŒ»å­¦æ–‡æ¡£ã€è¯å“æ•°æ®ã€æ£€éªŒæŒ‡æ ‡æ•°æ®åˆ°å‘é‡åº“å’Œæ•°æ®åº“ã€‚
"""

import json
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core import (
    DocumentParser,
    Chunker,
    get_embedder,
    get_vector_store,
)
from src.utils import log, setup_logger


def load_medical_docs(docs_dir: str = "data/medical_docs"):
    """
    åŠ è½½åŒ»å­¦æ–‡æ¡£åˆ°å‘é‡åº“
    
    Args:
        docs_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
    """
    log.info("=" * 50)
    log.info("å¼€å§‹åŠ è½½åŒ»å­¦æ–‡æ¡£...")
    
    parser = DocumentParser()
    chunker = Chunker()
    embedder = get_embedder()
    vector_store = get_vector_store()
    
    docs_path = Path(docs_dir)
    if not docs_path.exists():
        log.warning(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
        return
    
    # è§£ææ‰€æœ‰æ–‡æ¡£
    documents = parser.parse_directory(str(docs_path))
    
    if not documents:
        log.warning("æ²¡æœ‰æ‰¾åˆ°å¯è§£æçš„æ–‡æ¡£")
        return
    
    # åˆ†å—å’ŒåµŒå…¥
    all_chunks = []
    for doc in documents:
        chunks = chunker.chunk_document(
            doc_id=doc.id,
            doc_title=doc.title,
            content=doc.content,
            source=doc.source,
            metadata=doc.metadata,
        )
        all_chunks.extend(chunks)
    
    log.info(f"å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
    
    # ç”ŸæˆåµŒå…¥å‘é‡
    log.info("ç”ŸæˆåµŒå…¥å‘é‡...")
    texts = [chunk.content for chunk in all_chunks]
    embeddings = embedder.embed_documents(texts)
    
    # å­˜å‚¨åˆ°å‘é‡åº“
    log.info("å­˜å‚¨åˆ°å‘é‡åº“...")
    vector_store.add(
        ids=[chunk.id for chunk in all_chunks],
        embeddings=embeddings.tolist(),
        documents=texts,
        metadatas=[
            {
                "doc_id": chunk.doc_id,
                "doc_title": chunk.doc_title,
                "source": chunk.source,
                "chunk_index": chunk.chunk_index,
            }
            for chunk in all_chunks
        ],
    )
    
    log.info(f"âœ… åŒ»å­¦æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {vector_store.count()} ä¸ªæ–‡æ¡£å—")


def load_drug_data(drug_file: str = "data/drug_db/drugs.json"):
    """
    åŠ è½½è¯å“æ•°æ®
    
    Args:
        drug_file: è¯å“æ•°æ®æ–‡ä»¶è·¯å¾„
    """
    log.info("=" * 50)
    log.info("å¼€å§‹åŠ è½½è¯å“æ•°æ®...")
    
    drug_path = Path(drug_file)
    if not drug_path.exists():
        log.warning(f"è¯å“æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {drug_file}")
        return
    
    with open(drug_path, "r", encoding="utf-8") as f:
        drugs = json.load(f)
    
    log.info(f"åŠ è½½äº† {len(drugs)} æ¡è¯å“æ•°æ®")
    
    # ä¹Ÿå¯ä»¥å°†è¯å“ä¿¡æ¯å­˜å…¥å‘é‡åº“ä»¥æ”¯æŒè¯­ä¹‰æœç´¢
    embedder = get_embedder()
    vector_store = get_vector_store()
    
    # ä¸ºæ¯ä¸ªè¯å“ç”Ÿæˆæè¿°æ–‡æœ¬
    texts = []
    ids = []
    metadatas = []
    
    for drug in drugs:
        # ç»„åˆè¯å“æè¿°
        desc = f"""è¯å“åç§°ï¼š{drug['name']}
é€šç”¨åï¼š{drug.get('generic_name', '')}
åˆ†ç±»ï¼š{drug.get('category', '')}
é€‚åº”ç—‡ï¼š{drug.get('indications', '')}
ç”¨æ³•ç”¨é‡ï¼š{drug.get('dosage', '')}
ä¸è‰¯ååº”ï¼š{drug.get('side_effects', '')}
ç¦å¿Œï¼š{drug.get('contraindications', '')}
æ³¨æ„äº‹é¡¹ï¼š{drug.get('precautions', '')}"""
        
        texts.append(desc)
        ids.append(f"drug_{drug['id']}")
        metadatas.append({
            "type": "drug",
            "drug_id": drug['id'],
            "name": drug['name'],
            "is_otc": drug.get('is_otc', False),
        })
    
    # ç”ŸæˆåµŒå…¥å¹¶å­˜å‚¨
    embeddings = embedder.embed_documents(texts)
    vector_store.add(
        ids=ids,
        embeddings=embeddings.tolist(),
        documents=texts,
        metadatas=metadatas,
    )
    
    log.info(f"âœ… è¯å“æ•°æ®åŠ è½½å®Œæˆ")


def load_lab_indices(indices_file: str = "data/lab_indices/indices.json"):
    """
    åŠ è½½æ£€éªŒæŒ‡æ ‡æ•°æ®
    
    Args:
        indices_file: æ£€éªŒæŒ‡æ ‡æ–‡ä»¶è·¯å¾„
    """
    log.info("=" * 50)
    log.info("å¼€å§‹åŠ è½½æ£€éªŒæŒ‡æ ‡æ•°æ®...")
    
    indices_path = Path(indices_file)
    if not indices_path.exists():
        log.warning(f"æ£€éªŒæŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {indices_file}")
        return
    
    with open(indices_path, "r", encoding="utf-8") as f:
        indices = json.load(f)
    
    log.info(f"åŠ è½½äº† {len(indices)} æ¡æ£€éªŒæŒ‡æ ‡æ•°æ®")
    
    embedder = get_embedder()
    vector_store = get_vector_store()
    
    texts = []
    ids = []
    metadatas = []
    
    for index in indices:
        # ç»„åˆæŒ‡æ ‡æè¿°
        normal_range = index.get('normal_range', {})
        range_str = ", ".join([f"{k}: {v}" for k, v in normal_range.items()])
        
        desc = f"""æ£€éªŒæŒ‡æ ‡ï¼š{index['name']}ï¼ˆ{index.get('abbreviation', '')}ï¼‰
ç±»åˆ«ï¼š{index.get('category', '')}
å•ä½ï¼š{index.get('unit', '')}
æ­£å¸¸èŒƒå›´ï¼š{range_str}
è¯´æ˜ï¼š{index.get('description', '')}
å‡é«˜å«ä¹‰ï¼š{index.get('high_meaning', '')}
é™ä½å«ä¹‰ï¼š{index.get('low_meaning', '')}"""
        
        texts.append(desc)
        ids.append(f"lab_{index['id']}")
        metadatas.append({
            "type": "lab_index",
            "index_id": index['id'],
            "name": index['name'],
            "category": index.get('category', ''),
        })
    
    embeddings = embedder.embed_documents(texts)
    vector_store.add(
        ids=ids,
        embeddings=embeddings.tolist(),
        documents=texts,
        metadatas=metadatas,
    )
    
    log.info(f"âœ… æ£€éªŒæŒ‡æ ‡æ•°æ®åŠ è½½å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    setup_logger(level="INFO")
    
    log.info("=" * 50)
    log.info("MediMind æ•°æ®åŠ è½½è„šæœ¬")
    log.info("=" * 50)
    
    try:
        # åŠ è½½å„ç±»æ•°æ®
        load_medical_docs()
        load_drug_data()
        load_lab_indices()
        
        # æœ€ç»ˆç»Ÿè®¡
        vector_store = get_vector_store()
        log.info("=" * 50)
        log.info(f"ğŸ‰ æ•°æ®åŠ è½½å®Œæˆï¼")
        log.info(f"ğŸ“Š å‘é‡åº“æ€»æ–‡æ¡£æ•°: {vector_store.count()}")
        log.info("=" * 50)
        
    except Exception as e:
        log.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
