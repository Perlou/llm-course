# RAG（检索增强生成）完全指南：从零开始深入理解

---

## 目录

1. [什么是RAG](#1-什么是rag)
2. [为什么需要RAG](#2-为什么需要rag)
3. [RAG核心架构](#3-rag核心架构)
4. [RAG工作流程详解](#4-rag工作流程详解)
5. [关键技术深度解析](#5-关键技术深度解析)
6. [RAG vs 微调](#6-rag-vs-微调)
7. [应用场景](#7-应用场景)
8. [常见挑战与优化策略](#8-常见挑战与优化策略)
9. [代码实践](#9-代码实践)
10. [总结与展望](#10-总结与展望)

---

## 1. 什么是RAG

### 1.1 定义

**RAG（Retrieval-Augmented Generation，检索增强生成）** 是一种结合了**信息检索**和**文本生成**的AI技术架构。它通过在生成回答之前，先从外部知识库中检索相关信息，然后将这些信息作为上下文提供给大语言模型（LLM），从而生成更准确、更可靠的回答。

### 1.2 核心思想

```
用户问题 → 检索相关文档 → 结合文档+问题 → LLM生成回答
```

用一个简单的比喻：

> RAG就像是一个**开卷考试**的学生。面对问题时，他不仅依靠自己记忆中的知识（LLM的参数知识），还会翻阅参考书籍（外部知识库）来找到最准确的答案。

### 1.3 RAG的起源

RAG概念由Facebook AI Research（FAIR）在2020年的论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》中首次提出。

---

## 2. 为什么需要RAG

### 2.1 LLM的固有局限性

| 问题             | 描述                                      | RAG如何解决                  |
| ---------------- | ----------------------------------------- | ---------------------------- |
| **知识截止**     | LLM的训练数据有时间限制，无法获取最新信息 | 实时检索最新文档             |
| **幻觉问题**     | LLM可能生成看似合理但实际错误的内容       | 基于真实文档生成，可追溯来源 |
| **领域知识不足** | 通用模型在专业领域表现有限                | 接入专业知识库               |
| **隐私数据**     | 企业私有数据不在公开训练集中              | 检索私有知识库               |
| **上下文限制**   | LLM上下文窗口有限                         | 只检索最相关的内容           |

### 2.2 RAG的核心优势

```
┌─────────────────────────────────────────────────────────────┐
│                      RAG 核心优势                            │
├─────────────────────────────────────────────────────────────┤
│  ✅ 知识可更新：无需重新训练模型，只需更新知识库              │
│  ✅ 可解释性：答案可追溯到具体文档来源                        │
│  ✅ 成本效益：比微调模型更经济                                │
│  ✅ 减少幻觉：基于事实文档生成                                │
│  ✅ 领域适应：快速适配特定业务场景                            │
└─────────────────────────────────────────────────────────────┘
```

---

## 3. RAG核心架构

### 3.1 三大核心组件

```
┌──────────────────────────────────────────────────────────────────┐
│                         RAG 系统架构                              │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐          │
│   │   知识库     │    │   检索器     │    │   生成器     │          │
│   │ (Knowledge  │───▶│ (Retriever) │───▶│ (Generator) │          │
│   │    Base)    │    │             │    │             │          │
│   └─────────────┘    └─────────────┘    └─────────────┘          │
│         │                   │                   │                 │
│         ▼                   ▼                   ▼                 │
│   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐          │
│   │ • 文档存储   │    │ • 向量检索   │    │ • LLM模型    │          │
│   │ • 向量索引   │    │ • 相似度匹配 │    │ • Prompt工程 │          │
│   │ • 元数据     │    │ • 重排序     │    │ • 答案生成   │          │
│   └─────────────┘    └─────────────┘    └─────────────┘          │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

### 3.2 组件详解

#### 3.2.1 知识库（Knowledge Base）

知识库是RAG系统的"大脑仓库"，存储着所有可检索的信息。

**组成要素：**

- **原始文档**：PDF、Word、网页、数据库等
- **向量索引**：文档的向量化表示
- **元数据**：文档标题、来源、时间戳等附加信息

#### 3.2.2 检索器（Retriever）

检索器负责根据用户查询找到最相关的文档片段。

**常见检索方法：**

| 方法                   | 原理                   | 优点               | 缺点           |
| ---------------------- | ---------------------- | ------------------ | -------------- |
| **稀疏检索（BM25）**   | 基于关键词匹配和TF-IDF | 速度快，可解释性强 | 无法理解语义   |
| **稠密检索（Dense）**  | 基于向量相似度         | 理解语义相似性     | 需要向量化计算 |
| **混合检索（Hybrid）** | 结合稀疏和稠密         | 综合优势           | 实现复杂度高   |

#### 3.2.3 生成器（Generator）

生成器是RAG系统的"表达者"，基于检索到的上下文生成最终回答。

**核心要素：**

- 大语言模型（如GPT-4、Claude、LLaMA等）
- Prompt模板设计
- 上下文整合策略

---

## 4. RAG工作流程详解

### 4.1 整体流程图

```
                              RAG 完整工作流程

═══════════════════════════════════════════════════════════════════════

  【离线阶段：索引构建】

  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
  │ 原始文档  │───▶│ 文档加载  │───▶│ 文本分块  │───▶│ 向量化   │
  │ (PDF/TXT │    │ Loading  │    │ Chunking │    │ Embedding│
  │  /HTML)  │    │          │    │          │    │          │
  └──────────┘    └──────────┘    └──────────┘    └────┬─────┘
                                                        │
                                                        ▼
                                                 ┌──────────┐
                                                 │ 向量数据库│
                                                 │ (Vector  │
                                                 │   DB)    │
                                                 └────┬─────┘
                                                      │
═══════════════════════════════════════════════════════════════════════

  【在线阶段：检索生成】

  ┌──────────┐    ┌──────────┐         ┌──────────┐
  │ 用户问题  │───▶│ 问题向量化│────────▶│ 相似度   │
  │          │    │          │         │ 检索     │
  └──────────┘    └──────────┘         └────┬─────┘
                                             │
                                             ▼
  ┌──────────┐    ┌──────────┐    ┌──────────────┐
  │ 最终回答  │◀───│ LLM生成  │◀───│ 构建Prompt   │
  │          │    │          │    │ (问题+上下文) │
  └──────────┘    └──────────┘    └──────────────┘

═══════════════════════════════════════════════════════════════════════
```

### 4.2 阶段一：索引构建（Indexing）

这是**离线阶段**，目的是将原始文档转化为可检索的向量索引。

#### 步骤1：文档加载（Document Loading）

```python
# 支持多种文档格式
文档类型：
├── PDF文件
├── Word文档（.docx）
├── 纯文本（.txt）
├── Markdown（.md）
├── HTML网页
├── CSV/Excel表格
└── 数据库记录
```

#### 步骤2：文本分块（Chunking）

**为什么需要分块？**

- LLM上下文窗口有限
- 太长的文本检索精度下降
- 提高检索的相关性

**常见分块策略：**

| 策略             | 描述               | 适用场景         |
| ---------------- | ------------------ | ---------------- |
| **固定大小分块** | 按固定字符数切分   | 通用场景         |
| **语义分块**     | 按段落、章节切分   | 结构化文档       |
| **递归分块**     | 多层级递归切分     | 长文档处理       |
| **滑动窗口**     | 重叠分块保留上下文 | 需要上下文连贯性 |

```
示例：滑动窗口分块（窗口大小=100，重叠=20）

原文：[----------------------------------------200字-----------------------------------------]

分块结果：
Chunk 1: [--------100字--------]
Chunk 2:              [--------100字--------]
Chunk 3:                           [--------100字--------]
              ↑重叠20字↑         ↑重叠20字↑
```

#### 步骤3：向量嵌入（Embedding）

将文本转换为高维向量，使语义相似的文本在向量空间中距离相近。

```
文本: "机器学习是人工智能的一个分支"
       ↓ Embedding模型
向量: [0.12, -0.34, 0.56, ..., 0.89]  # 通常768-1536维
```

**常用Embedding模型：**

| 模型                          | 维度    | 特点            |
| ----------------------------- | ------- | --------------- |
| OpenAI text-embedding-3-small | 1536    | 性能优秀，需API |
| BGE-large-zh                  | 1024    | 中文效果好      |
| sentence-transformers         | 384-768 | 开源免费        |
| Cohere Embed                  | 1024    | 多语言支持      |

#### 步骤4：存储到向量数据库

**主流向量数据库对比：**

| 数据库       | 类型   | 特点                 |
| ------------ | ------ | -------------------- |
| **Pinecone** | 云服务 | 全托管，易用性强     |
| **Milvus**   | 开源   | 功能强大，可自托管   |
| **Chroma**   | 开源   | 轻量级，适合原型开发 |
| **Weaviate** | 开源   | 支持多模态           |
| **FAISS**    | 库     | Meta开源，性能优秀   |
| **Qdrant**   | 开源   | Rust实现，性能好     |

### 4.3 阶段二：检索（Retrieval）

这是**在线阶段**的第一步，根据用户问题检索相关文档。

#### 步骤1：查询向量化

```
用户问题: "什么是机器学习？"
    ↓ 使用相同的Embedding模型
查询向量: [0.15, -0.32, 0.58, ..., 0.87]
```

#### 步骤2：相似度检索

**常用相似度计算方法：**

```
┌─────────────────────────────────────────────────────────────────┐
│                      相似度计算方法                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. 余弦相似度（Cosine Similarity）                              │
│                                                                  │
│              A · B           Σ(Ai × Bi)                         │
│     cos(θ) = ───── = ─────────────────────────                  │
│              |A||B|   √Σ(Ai²) × √Σ(Bi²)                        │
│                                                                  │
│     范围：[-1, 1]，1表示完全相同                                 │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  2. 欧氏距离（Euclidean Distance）                               │
│                                                                  │
│     d(A,B) = √Σ(Ai - Bi)²                                       │
│                                                                  │
│     范围：[0, ∞)，0表示完全相同                                  │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  3. 点积（Dot Product）                                          │
│                                                                  │
│     A · B = Σ(Ai × Bi)                                          │
│                                                                  │
│     适用于归一化向量                                             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### 步骤3：返回Top-K结果

```
检索结果（Top-3）：
┌────────────────────────────────────────────────────┐
│ Rank │ 相似度  │ 文档片段                          │
├────────────────────────────────────────────────────┤
│  1   │ 0.92   │ "机器学习是一种人工智能技术..."    │
│  2   │ 0.87   │ "ML使计算机能够从数据中学习..."    │
│  3   │ 0.81   │ "深度学习是机器学习的子领域..."    │
└────────────────────────────────────────────────────┘
```

### 4.4 阶段三：生成（Generation）

将检索结果与用户问题组合，通过LLM生成最终回答。

#### Prompt模板示例

```markdown
你是一个专业的问答助手。请基于以下参考信息回答用户问题。

【参考信息】
{context}

【用户问题】
{question}

【回答要求】

1. 只使用参考信息中的内容回答
2. 如果参考信息不足以回答，请明确说明
3. 在回答中标注信息来源

请回答：
```

---

## 5. 关键技术深度解析

### 5.1 文档分块策略详解

#### 5.1.1 分块参数选择

```
┌─────────────────────────────────────────────────────────┐
│                   分块参数指南                           │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  chunk_size（分块大小）：                                │
│  ├── 推荐范围：256 - 1024 tokens                        │
│  ├── 小分块：检索精确，但可能丢失上下文                  │
│  └── 大分块：上下文完整，但可能引入噪声                  │
│                                                          │
│  chunk_overlap（重叠大小）：                             │
│  ├── 推荐范围：chunk_size的10%-20%                      │
│  └── 作用：保持分块间的上下文连贯性                      │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

#### 5.1.2 高级分块技术

**1. 父子文档检索（Parent-Child Retrieval）**

```
原理：
┌────────────────────────────────────────┐
│           父文档（完整段落）             │
│  ┌─────────┬─────────┬─────────┐       │
│  │ 子块1   │ 子块2   │ 子块3   │       │
│  └─────────┴─────────┴─────────┘       │
└────────────────────────────────────────┘

检索时：使用小的子块进行精确匹配
返回时：返回更大的父文档保证上下文完整
```

**2. 句子窗口检索（Sentence Window Retrieval）**

```
检索粒度：单个句子
返回内容：句子 + 前后N个句子作为窗口

示例：检索到第5句 → 返回第3-7句
```

### 5.2 Embedding模型选择

#### 5.2.1 评估维度

| 维度             | 说明                  |
| ---------------- | --------------------- |
| **语义理解能力** | 是否准确捕捉文本含义  |
| **多语言支持**   | 对中文等非英语的支持  |
| **推理速度**     | 向量化的效率          |
| **向量维度**     | 影响存储和检索速度    |
| **成本**         | API调用费用或部署成本 |

#### 5.2.2 中文场景推荐

```
优先推荐：
1. BGE系列（bge-large-zh-v1.5）
   - 专为中文优化
   - 在中文评测榜单表现优异

2. M3E系列
   - 中文场景效果好
   - 开源可商用

3. OpenAI text-embedding-3-large
   - 多语言能力强
   - 需要API调用
```

### 5.3 检索优化技术

#### 5.3.1 混合检索（Hybrid Search）

```
┌─────────────────────────────────────────────────────────┐
│                     混合检索流程                         │
├─────────────────────────────────────────────────────────┤
│                                                          │
│    用户查询                                              │
│       │                                                  │
│       ├────────────────┬────────────────┐               │
│       ▼                ▼                ▼               │
│  ┌─────────┐     ┌─────────┐     ┌─────────┐           │
│  │ 稀疏检索 │     │ 稠密检索 │     │ 关键词   │           │
│  │ (BM25)  │     │ (向量)  │     │ 过滤    │           │
│  └────┬────┘     └────┬────┘     └────┬────┘           │
│       │               │               │                 │
│       └───────────────┼───────────────┘                 │
│                       ▼                                  │
│               ┌─────────────┐                           │
│               │  结果融合    │                           │
│               │ (RRF/加权)  │                           │
│               └─────────────┘                           │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

**RRF（Reciprocal Rank Fusion）公式：**

```
RRF_score(d) = Σ 1/(k + rank_i(d))

其中：
- d: 文档
- k: 常数（通常为60）
- rank_i(d): 文档d在第i个检索结果中的排名
```

#### 5.3.2 重排序（Reranking）

```
两阶段检索：

第一阶段：召回（Recall）
├── 目标：快速获取大量候选文档
├── 方法：向量检索
└── 结果：Top-100候选

第二阶段：精排（Rerank）
├── 目标：精确排序候选文档
├── 方法：Cross-Encoder模型
└── 结果：Top-5最终结果
```

**常用Reranker模型：**

- Cohere Rerank
- bge-reranker-large
- cross-encoder/ms-marco-MiniLM-L-6-v2

#### 5.3.3 查询改写（Query Rewriting）

```
┌─────────────────────────────────────────────────────────┐
│                    查询改写策略                          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  1. 查询扩展（Query Expansion）                          │
│     原查询："ML模型"                                     │
│     扩展后："机器学习模型 machine learning model"        │
│                                                          │
│  2. 假设性文档嵌入（HyDE）                               │
│     原查询："什么是RAG？"                                │
│     生成假设答案："RAG是一种结合检索和生成的技术..."     │
│     用假设答案去检索                                     │
│                                                          │
│  3. 多查询生成（Multi-Query）                            │
│     原查询："RAG原理"                                    │
│     生成多个变体：                                       │
│     - "RAG技术的工作原理是什么？"                        │
│     - "检索增强生成是如何工作的？"                       │
│     - "RAG的核心机制解释"                                │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

---

## 6. RAG vs 微调

### 6.1 对比分析

| 维度         | RAG                         | 微调（Fine-tuning） |
| ------------ | --------------------------- | ------------------- |
| **知识更新** | ✅ 即时更新，修改知识库即可 | ❌ 需要重新训练     |
| **成本**     | ✅ 相对较低                 | ❌ 训练成本高       |
| **可解释性** | ✅ 可追溯信息来源           | ❌ 黑盒推理         |
| **幻觉控制** | ✅ 基于事实文档             | ⚠️ 仍可能产生幻觉   |
| **实时性**   | ✅ 可获取最新信息           | ❌ 受限于训练数据   |
| **推理延迟** | ⚠️ 检索增加延迟             | ✅ 直接生成         |
| **风格定制** | ⚠️ 有限                     | ✅ 可深度定制       |
| **长期记忆** | ⚠️ 依赖检索质量             | ✅ 内化到参数中     |

### 6.2 选择建议

```
选择 RAG 的场景：
├── 知识需要频繁更新
├── 需要可追溯的信息来源
├── 预算有限
├── 快速原型验证
└── 处理企业私有数据

选择 微调 的场景：
├── 需要特定的回答风格
├── 需要学习特定的推理模式
├── 数据相对静态
└── 对延迟要求极高

最佳实践：RAG + 微调 结合使用
```

---

## 7. 应用场景

### 7.1 典型应用

```
┌─────────────────────────────────────────────────────────────────┐
│                        RAG 应用场景                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  📚 知识库问答                                                   │
│     └── 企业文档检索、产品手册查询、FAQ系统                      │
│                                                                  │
│  💬 智能客服                                                     │
│     └── 基于历史工单和知识库的自动回复                           │
│                                                                  │
│  📖 文档分析                                                     │
│     └── 合同审核、法规查询、学术论文问答                         │
│                                                                  │
│  🔍 语义搜索                                                     │
│     └── 代码搜索、商品搜索、内容推荐                             │
│                                                                  │
│  📝 内容创作                                                     │
│     └── 基于参考资料的文章生成、报告撰写                         │
│                                                                  │
│  🏥 垂直领域                                                     │
│     └── 医疗问诊辅助、法律咨询、金融分析                         │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 7.2 行业案例

| 行业     | 应用         | 知识库来源         |
| -------- | ------------ | ------------------ |
| **金融** | 投研分析助手 | 研报、财报、新闻   |
| **医疗** | 临床决策支持 | 医学文献、诊疗指南 |
| **法律** | 法律咨询助手 | 法规、判例、合同   |
| **教育** | 智能教学助手 | 教材、题库、论文   |
| **电商** | 商品推荐问答 | 商品信息、评论     |

---

## 8. 常见挑战与优化策略

### 8.1 挑战与解决方案

```
┌─────────────────────────────────────────────────────────────────┐
│                     常见挑战与解决方案                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  挑战1：检索质量差                                               │
│  ├── 表现：返回不相关文档                                        │
│  └── 解决方案：                                                  │
│      ├── 优化Embedding模型选择                                  │
│      ├── 调整分块策略                                           │
│      ├── 引入重排序（Reranker）                                 │
│      └── 使用混合检索                                           │
│                                                                  │
│  挑战2：上下文丢失                                               │
│  ├── 表现：回答不完整或断章取义                                  │
│  └── 解决方案：                                                  │
│      ├── 增加chunk重叠度                                        │
│      ├── 使用父子文档检索                                       │
│      └── 增加检索数量Top-K                                      │
│                                                                  │
│  挑战3：延迟过高                                                 │
│  ├── 表现：响应时间长                                            │
│  └── 解决方案：                                                  │
│      ├── 使用更快的向量数据库                                   │
│      ├── 预计算常见查询                                         │
│      ├── 异步并行检索                                           │
│      └── 使用更小的Embedding模型                                │
│                                                                  │
│  挑战4：答案不一致                                               │
│  ├── 表现：相同问题不同回答                                      │
│  └── 解决方案：                                                  │
│      ├── 优化Prompt模板                                         │
│      ├── 降低温度参数                                           │
│      └── 增加few-shot示例                                       │
│                                                                  │
│  挑战5：多跳推理困难                                             │
│  ├── 表现：需要整合多个信息源的问题回答差                        │
│  └── 解决方案：                                                  │
│      ├── 迭代检索（Iterative Retrieval）                        │
│      ├── 查询分解                                               │
│      └── 使用Agent架构                                          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 8.2 评估指标

| 指标                    | 说明                   | 计算方式                        |
| ----------------------- | ---------------------- | ------------------------------- |
| **召回率（Recall）**    | 检索到的相关文档比例   | 检索到的相关文档 / 所有相关文档 |
| **精确率（Precision）** | 检索结果中相关文档比例 | 相关文档 / 检索到的文档         |
| **MRR**                 | 平均倒数排名           | 1/第一个相关结果的排名          |
| **NDCG**                | 归一化折损累计增益     | 考虑排名位置的相关性评分        |
| **Answer Accuracy**     | 答案准确性             | 人工评估或自动化评测            |
| **Faithfulness**        | 答案忠实度             | 答案是否基于检索内容            |

---

## 9. 代码实践

### 9.1 使用LangChain实现简单RAG

```python
# 安装依赖
# pip install langchain langchain-openai chromadb

from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 1. 加载文档
loader = TextLoader("knowledge.txt", encoding="utf-8")
documents = loader.load()

# 2. 文本分块
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,       # 每块500个字符
    chunk_overlap=50,     # 重叠50个字符
    separators=["\n\n", "\n", "。", "！", "？", "，", " "]
)
chunks = text_splitter.split_documents(documents)
print(f"分块数量: {len(chunks)}")

# 3. 创建向量存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. 创建检索器
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # 返回前3个最相关的文档
)

# 5. 创建RAG链
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 6. 提问
query = "什么是机器学习？"
result = qa_chain({"query": query})

print("回答:", result["result"])
print("\n来源文档:")
for doc in result["source_documents"]:
    print(f"- {doc.page_content[:100]}...")
```

### 9.2 使用LlamaIndex实现RAG

```python
# pip install llama-index

from llama_index import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    ServiceContext
)
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding

# 1. 加载文档
documents = SimpleDirectoryReader("./data").load_data()

# 2. 配置服务上下文
llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
embed_model = OpenAIEmbedding()

service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
    chunk_size=512,
    chunk_overlap=50
)

# 3. 创建索引
index = VectorStoreIndex.from_documents(
    documents,
    service_context=service_context
)

# 4. 创建查询引擎
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact"
)

# 5. 查询
response = query_engine.query("什么是RAG？")
print(response)
```

### 9.3 自定义RAG实现（不依赖框架）

```python
import numpy as np
from typing import List, Tuple
import openai

class SimpleRAG:
    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.documents = []
        self.embeddings = []

    def add_documents(self, texts: List[str]):
        """添加文档到知识库"""
        self.documents.extend(texts)
        # 批量生成embeddings
        for text in texts:
            embedding = self._get_embedding(text)
            self.embeddings.append(embedding)

    def _get_embedding(self, text: str) -> List[float]:
        """获取文本的embedding向量"""
        response = openai.Embedding.create(
            model="text-embedding-3-small",
            input=text
        )
        return response['data'][0]['embedding']

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """计算余弦相似度"""
        a = np.array(a)
        b = np.array(b)
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """检索最相关的文档"""
        query_embedding = self._get_embedding(query)

        # 计算与所有文档的相似度
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((self.documents[i], sim))

        # 按相似度排序并返回top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def query(self, question: str, top_k: int = 3) -> str:
        """RAG查询：检索 + 生成"""
        # 1. 检索相关文档
        relevant_docs = self.retrieve(question, top_k)
        context = "\n\n".join([doc for doc, _ in relevant_docs])

        # 2. 构建prompt
        prompt = f"""基于以下参考信息回答问题。

参考信息:
{context}

问题: {question}

回答:"""

        # 3. 调用LLM生成答案
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        return response.choices[0].message.content

# 使用示例
rag = SimpleRAG(api_key="your-api-key")

# 添加知识
rag.add_documents([
    "RAG是检索增强生成的缩写，它结合了信息检索和文本生成技术。",
    "机器学习是人工智能的一个分支，让计算机能够从数据中学习。",
    "向量数据库专门用于存储和检索向量数据，支持高效的相似性搜索。"
])

# 查询
answer = rag.query("什么是RAG？")
print(answer)
```

---

## 10. 总结与展望

### 10.1 核心要点回顾

```
┌─────────────────────────────────────────────────────────────────┐
│                        RAG 核心要点                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  📌 RAG = 检索（Retrieval）+ 增强（Augmented）+ 生成（Generation）│
│                                                                  │
│  📌 三大组件：知识库 → 检索器 → 生成器                           │
│                                                                  │
│  📌 两个阶段：离线索引 + 在线检索生成                            │
│                                                                  │
│  📌 关键技术：                                                   │
│     ├── 文档分块（Chunking）                                    │
│     ├── 向量嵌入（Embedding）                                   │
│     ├── 相似度检索（Retrieval）                                 │
│     └── Prompt工程（Prompt Engineering）                        │
│                                                                  │
│  📌 优化方向：                                                   │
│     ├── 混合检索                                                │
│     ├── 重排序                                                  │
│     ├── 查询改写                                                │
│     └── 父子文档检索                                            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 10.2 未来发展趋势

| 趋势                  | 描述                             |
| --------------------- | -------------------------------- |
| **多模态RAG**         | 支持图片、视频、音频等多模态检索 |
| **Agent-based RAG**   | 结合Agent实现更复杂的推理流程    |
| **自适应RAG**         | 动态决定是否需要检索             |
| **图RAG（GraphRAG）** | 结合知识图谱增强检索             |
| **长上下文与RAG融合** | 结合超长上下文窗口优化           |

### 10.3 学习资源推荐

**论文：**

- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020)
- Lost in the Middle: How Language Models Use Long Contexts (2023)

**开源框架：**

- LangChain: https://github.com/langchain-ai/langchain
- LlamaIndex: https://github.com/run-llama/llama_index
- Haystack: https://github.com/deepset-ai/haystack

**向量数据库：**

- Milvus: https://milvus.io/
- Chroma: https://www.trychroma.com/
- Pinecone: https://www.pinecone.io/

---

## 附录：RAG系统架构速查表

```
┌──────────────────────────────────────────────────────────────────────┐
│                        RAG 系统速查表                                 │
├──────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  组件选型建议：                                                        │
│  ┌─────────────┬────────────────────────────────────────────────┐    │
│  │   组件      │   推荐选项                                      │    │
│  ├─────────────┼────────────────────────────────────────────────┤    │
│  │ Embedding   │ OpenAI / BGE / M3E / Cohere                    │    │
│  │ 向量数据库  │ Milvus / Chroma / Pinecone / Qdrant            │    │
│  │ LLM        │ GPT-4 / Claude / LLaMA / Qwen                   │    │
│  │ 框架       │ LangChain / LlamaIndex / Haystack               │    │
│  │ Reranker   │ Cohere / BGE-reranker / cross-encoder          │    │
│  └─────────────┴────────────────────────────────────────────────┘    │
│                                                                       │
│  参数参考：                                                           │
│  ├── chunk_size: 256-1024 tokens                                    │
│  ├── chunk_overlap: 10%-20% of chunk_size                           │
│  ├── top_k: 3-10                                                     │
│  └── temperature: 0-0.3 (事实性任务)                                 │
│                                                                       │
│  评估指标：                                                           │
│  ├── 检索质量: Recall@K, MRR, NDCG                                  │
│  └── 生成质量: Accuracy, Faithfulness, Relevance                    │
│                                                                       │
└──────────────────────────────────────────────────────────────────────┘
```

---

**文档版本**: v1.0  
**最后更新**: 2024年  
**作者**: AI Assistant

---

> 💡 **提示**：RAG技术正在快速发展，建议持续关注最新研究进展和最佳实践。
