"""
æ–‡æ¡£è½¬æ¢
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£æ–‡æ¡£è½¬æ¢çš„åœºæ™¯
    2. æŒæ¡å…ƒæ•°æ®å¤„ç†
    3. å­¦ä¼šæ–‡æ¡£è¿‡æ»¤å’Œæ¸…æ´—

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Document Transformersï¼šæ–‡æ¡£è½¬æ¢å™¨
    - å…ƒæ•°æ®å¢å¼ºï¼šæ·»åŠ /ä¿®æ”¹å…ƒæ•°æ®
    - å†…å®¹æ¸…æ´—ï¼šå»é™¤å™ªå£°

å‰ç½®çŸ¥è¯†ï¼š
    - 02-text-splitters.py

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install langchain langchain-community
"""

import os
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£è½¬æ¢æ¦‚è¿° ====================


def transformer_overview():
    """æ–‡æ¡£è½¬æ¢æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æ¡£è½¬æ¢æ¦‚è¿°")
    print("=" * 60)

    print("""
    æ–‡æ¡£è½¬æ¢çš„åº”ç”¨åœºæ™¯ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. å…ƒæ•°æ®å¢å¼º
       - æ·»åŠ æ¥æºã€æ—¶é—´æˆ³
       - è®¡ç®—æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
    
    2. å†…å®¹æ¸…æ´—
       - å»é™¤å¤šä½™ç©ºç™½
       - è¿‡æ»¤æ— å…³å†…å®¹
    
    3. æ ¼å¼è½¬æ¢
       - HTML è½¬çº¯æ–‡æœ¬
       - æå–è¡¨æ ¼æ•°æ®
    
    4. å»é‡
       - åˆ é™¤é‡å¤æ–‡æ¡£
       - åˆå¹¶ç›¸ä¼¼å†…å®¹
    
    è½¬æ¢æµç¨‹ï¼š
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ åŸå§‹Doc â”‚ â”€â–¶ â”‚ Transformer â”‚ â”€â–¶ â”‚ è½¬æ¢Doc â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå…ƒæ•°æ®å¤„ç† ====================


def metadata_handling():
    """å…ƒæ•°æ®å¤„ç†"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå…ƒæ•°æ®å¤„ç†")
    print("=" * 60)

    from langchain_core.documents import Document
    from datetime import datetime

    # åŸå§‹æ–‡æ¡£
    docs = [
        Document(
            page_content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€", metadata={"source": "file1.txt"}
        ),
        Document(page_content="æœºå™¨å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®", metadata={"source": "file2.txt"}),
    ]

    # æ·»åŠ å…ƒæ•°æ®
    def add_metadata(docs):
        """æ·»åŠ é¢å¤–å…ƒæ•°æ®"""
        for doc in docs:
            doc.metadata["processed_at"] = datetime.now().isoformat()
            doc.metadata["char_count"] = len(doc.page_content)
            doc.metadata["word_count"] = len(doc.page_content.split())
        return docs

    enriched_docs = add_metadata(docs)

    print("ğŸ“Œ å…ƒæ•°æ®å¢å¼ºåï¼š")
    for doc in enriched_docs:
        print(f"  å†…å®¹: {doc.page_content[:30]}...")
        print(f"  å…ƒæ•°æ®: {doc.metadata}\n")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå†…å®¹æ¸…æ´— ====================


def content_cleaning():
    """å†…å®¹æ¸…æ´—"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå†…å®¹æ¸…æ´—")
    print("=" * 60)

    from langchain_core.documents import Document
    import re

    def clean_document(doc: Document) -> Document:
        """æ¸…æ´—æ–‡æ¡£å†…å®¹"""
        content = doc.page_content

        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r"\s+", " ", content)

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r"[^\w\s\u4e00-\u9fff.,!?ï¼Œã€‚ï¼ï¼Ÿ]", "", content)

        # å»é™¤é¦–å°¾ç©ºç™½
        content = content.strip()

        return Document(
            page_content=content, metadata={**doc.metadata, "cleaned": True}
        )

    # æµ‹è¯•
    dirty_doc = Document(
        page_content="  è¿™æ˜¯   ä¸€æ®µ\n\n\n  å¾ˆä¹±çš„   æ–‡æœ¬!!!@#$%  ",
        metadata={"source": "test.txt"},
    )

    clean_doc = clean_document(dirty_doc)

    print("ğŸ“Œ æ¸…æ´—å‰åå¯¹æ¯”ï¼š")
    print(f"  åŸå§‹: '{dirty_doc.page_content}'")
    print(f"  æ¸…æ´—å: '{clean_doc.page_content}'")


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šæ–‡æ¡£è¿‡æ»¤ ====================


def document_filtering():
    """æ–‡æ¡£è¿‡æ»¤"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šæ–‡æ¡£è¿‡æ»¤")
    print("=" * 60)

    from langchain_core.documents import Document

    docs = [
        Document(
            page_content="è¿™æ˜¯ä¸€æ®µæœ‰æ„ä¹‰çš„é•¿æ–‡æœ¬ï¼ŒåŒ…å«å¾ˆå¤šæœ‰ç”¨çš„ä¿¡æ¯ã€‚",
            metadata={"type": "article"},
        ),
        Document(page_content="çŸ­", metadata={"type": "fragment"}),
        Document(
            page_content="è¿™æ˜¯ä¸€æ®µå¹¿å‘Šå†…å®¹ï¼Œè´­ä¹°è¯·è”ç³»...", metadata={"type": "ad"}
        ),
        Document(
            page_content="æŠ€æœ¯æ–‡æ¡£ï¼šPython å‡½æ•°å®šä¹‰ä½¿ç”¨ def å…³é”®å­—ã€‚",
            metadata={"type": "doc"},
        ),
    ]

    def filter_documents(docs, min_length=10, exclude_types=None):
        """è¿‡æ»¤æ–‡æ¡£"""
        exclude_types = exclude_types or []
        filtered = []

        for doc in docs:
            # é•¿åº¦è¿‡æ»¤
            if len(doc.page_content) < min_length:
                continue

            # ç±»å‹è¿‡æ»¤
            if doc.metadata.get("type") in exclude_types:
                continue

            filtered.append(doc)

        return filtered

    filtered = filter_documents(docs, min_length=10, exclude_types=["ad"])

    print("ğŸ“Œ è¿‡æ»¤ç»“æœï¼š")
    print(f"  åŸå§‹æ–‡æ¡£æ•°: {len(docs)}")
    print(f"  è¿‡æ»¤å: {len(filtered)}")
    for doc in filtered:
        print(f"    - [{doc.metadata['type']}] {doc.page_content[:30]}...")


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæ–‡æ¡£å»é‡ ====================


def document_deduplication():
    """æ–‡æ¡£å»é‡"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šæ–‡æ¡£å»é‡")
    print("=" * 60)

    from langchain_core.documents import Document
    import hashlib

    def deduplicate_docs(docs):
        """åŸºäºå†…å®¹å“ˆå¸Œå»é‡"""
        seen = set()
        unique = []

        for doc in docs:
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()

            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(doc)

        return unique

    docs = [
        Document(page_content="äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"),
        Document(page_content="æœºå™¨å­¦ä¹ æ˜¯ AI çš„åˆ†æ”¯"),
        Document(page_content="äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"),  # é‡å¤
        Document(page_content="æ·±åº¦å­¦ä¹ å¾ˆé‡è¦"),
    ]

    unique_docs = deduplicate_docs(docs)

    print("ğŸ“Œ å»é‡ç»“æœï¼š")
    print(f"  åŸå§‹: {len(docs)} ç¯‡")
    print(f"  å»é‡å: {len(unique_docs)} ç¯‡")


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´è½¬æ¢ç®¡é“ ====================


def transformation_pipeline():
    """å®Œæ•´è½¬æ¢ç®¡é“"""
    print("\n" + "=" * 60)
    print("ç¬¬å…­éƒ¨åˆ†ï¼šå®Œæ•´è½¬æ¢ç®¡é“")
    print("=" * 60)

    from langchain_core.documents import Document
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    import re

    def create_pipeline():
        """åˆ›å»ºæ–‡æ¡£å¤„ç†ç®¡é“"""

        def clean(docs):
            for doc in docs:
                doc.page_content = re.sub(r"\s+", " ", doc.page_content).strip()
            return docs

        def filter_short(docs, min_len=20):
            return [d for d in docs if len(d.page_content) >= min_len]

        def add_metadata(docs):
            for i, doc in enumerate(docs):
                doc.metadata["chunk_id"] = i
                doc.metadata["length"] = len(doc.page_content)
            return docs

        return clean, filter_short, add_metadata

    # ä½¿ç”¨ç®¡é“
    raw_docs = [
        Document(page_content="  è¿™æ˜¯    ä¸€æ®µé•¿æ–‡æœ¬  ", metadata={}),
        Document(page_content="çŸ­", metadata={}),
    ]

    clean, filter_short, add_meta = create_pipeline()

    result = add_meta(filter_short(clean(raw_docs)))

    print("ğŸ“Œ ç®¡é“å¤„ç†ç»“æœï¼š")
    for doc in result:
        print(f"  å†…å®¹: {doc.page_content}")
        print(f"  å…ƒæ•°æ®: {doc.metadata}")


# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šè‡ªå®šä¹‰æ¸…æ´—å™¨
        å®ç°ä¸€ä¸ªç§»é™¤ URL å’Œ Email çš„æ¸…æ´—å‡½æ•°ã€‚

    ç»ƒä¹  2ï¼šè¯­è¨€æ£€æµ‹è¿‡æ»¤
        è¿‡æ»¤æ‰éä¸­æ–‡çš„æ–‡æ¡£ã€‚

    ç»ƒä¹  3ï¼šç›¸ä¼¼åº¦å»é‡
        ä½¿ç”¨ç¼–è¾‘è·ç¦»æˆ–å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œæ¨¡ç³Šå»é‡ã€‚

    æ€è€ƒé¢˜ï¼š
        1. è¿‡åº¦æ¸…æ´—ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
        2. å¦‚ä½•ä¿ç•™é‡è¦çš„æ ¼å¼ä¿¡æ¯ï¼Ÿ
    """)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ–‡æ¡£è½¬æ¢")
    print("=" * 60)

    try:
        transformer_overview()
        metadata_handling()
        content_cleaning()
        document_filtering()
        document_deduplication()
        transformation_pipeline()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š04-embeddings-basics.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
