"""
æ–‡æœ¬åˆ†å‰²ç­–ç•¥
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£æ–‡æœ¬åˆ†å‰²çš„é‡è¦æ€§
    2. æŒæ¡å¸¸ç”¨åˆ†å‰²ç­–ç•¥
    3. å­¦ä¼šé€‰æ‹©åˆé€‚çš„åˆ†å‰²å‚æ•°

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Chunkï¼šåˆ†å‰²åçš„æ–‡æœ¬å—
    - Chunk Sizeï¼šå—å¤§å°
    - Chunk Overlapï¼šå—é‡å 

å‰ç½®çŸ¥è¯†ï¼š
    - 01-document-loaders.py

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install langchain langchain-community tiktoken
"""

import os
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ºä»€ä¹ˆè¦åˆ†å‰² ====================


def why_split():
    """ä¸ºä»€ä¹ˆè¦åˆ†å‰²æ–‡æœ¬"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ºä»€ä¹ˆè¦åˆ†å‰²æ–‡æœ¬")
    print("=" * 60)

    print("""
    åˆ†å‰²æ–‡æœ¬çš„åŸå› ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. LLM ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
       - GPT-3.5: ~16K tokens
       - GPT-4: ~128K tokens
       - é•¿æ–‡æ¡£éœ€è¦åˆ†å—å¤„ç†
    
    2. æ£€ç´¢ç²¾åº¦
       - å°å—æ›´å®¹æ˜“ç²¾ç¡®åŒ¹é…
       - å¤§å—å¯èƒ½åŒ…å«ä¸ç›¸å…³å†…å®¹
    
    3. æˆæœ¬æ§åˆ¶
       - åªæ£€ç´¢ç›¸å…³çš„å°å—
       - å‡å°‘ token æ¶ˆè€—
    
    åˆ†å‰²ç­–ç•¥è€ƒè™‘å› ç´ ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Chunk Sizeï¼ˆå—å¤§å°ï¼‰                        â”‚
    â”‚  - å¤ªå°ï¼šä¸Šä¸‹æ–‡ä¸å®Œæ•´                        â”‚
    â”‚  - å¤ªå¤§ï¼šæ£€ç´¢ä¸ç²¾ç¡®ï¼Œæˆæœ¬é«˜                   â”‚
    â”‚  - æ¨èï¼š500-1000 å­—ç¬¦                       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Chunk Overlapï¼ˆé‡å ï¼‰                       â”‚
    â”‚  - é˜²æ­¢è¾¹ç•Œå¤„ä¿¡æ¯ä¸¢å¤±                        â”‚
    â”‚  - é€šå¸¸è®¾ä¸º chunk_size çš„ 10-20%            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå­—ç¬¦åˆ†å‰²å™¨ ====================


def character_splitter():
    """å­—ç¬¦åˆ†å‰²å™¨"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå­—ç¬¦åˆ†å‰²å™¨")
    print("=" * 60)

    from langchain.text_splitter import CharacterTextSplitter

    text = """äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚ä»åŒ»ç–—è¯Šæ–­åˆ°è‡ªåŠ¨é©¾é©¶ï¼ŒAI æŠ€æœ¯çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚

æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚å®ƒæ¨¡æ‹Ÿäººè„‘ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†è®©æœºå™¨ç†è§£äººç±»è¯­è¨€ã€‚GPTã€BERT ç­‰æ¨¡å‹åœ¨æ–‡æœ¬ç†è§£æ–¹é¢å–å¾—äº†å·¨å¤§çªç ´ã€‚"""

    splitter = CharacterTextSplitter(
        separator="\n\n",
        chunk_size=100,
        chunk_overlap=20,
        length_function=len,
    )

    chunks = splitter.split_text(text)

    print(f"ğŸ“Œ åˆ†å‰²ç»“æœï¼ˆchunk_size=100, overlap=20ï¼‰ï¼š")
    for i, chunk in enumerate(chunks):
        print(f"\nå— {i + 1} ({len(chunk)} å­—ç¬¦):")
        print(f"  {chunk[:50]}...")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šé€’å½’å­—ç¬¦åˆ†å‰²å™¨ ====================


def recursive_splitter():
    """é€’å½’å­—ç¬¦åˆ†å‰²å™¨"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šé€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ˆæ¨èï¼‰")
    print("=" * 60)

    from langchain.text_splitter import RecursiveCharacterTextSplitter

    text = """# äººå·¥æ™ºèƒ½æ¦‚è¿°

## å®šä¹‰
äººå·¥æ™ºèƒ½æ˜¯ç ”ç©¶å¦‚ä½•ä½¿è®¡ç®—æœºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚

## åº”ç”¨é¢†åŸŸ
1. è‡ªç„¶è¯­è¨€å¤„ç†
2. è®¡ç®—æœºè§†è§‰
3. æœºå™¨äººæŠ€æœ¯

## å‘å±•å†ç¨‹
äººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡å‘å±•æµªæ½®ï¼Œä»ä¸“å®¶ç³»ç»Ÿåˆ°æ·±åº¦å­¦ä¹ ã€‚"""

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=20,
        separators=["\n## ", "\n", " ", ""],  # æŒ‰ä¼˜å…ˆçº§å°è¯•
    )

    chunks = splitter.split_text(text)

    print("ğŸ“Œ é€’å½’åˆ†å‰²ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼š")
    for i, chunk in enumerate(chunks):
        print(f"\nå— {i + 1}:")
        print(f"  {chunk}")


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šToken åˆ†å‰²å™¨ ====================


def token_splitter():
    """Token åˆ†å‰²å™¨"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šToken åˆ†å‰²å™¨")
    print("=" * 60)

    try:
        from langchain.text_splitter import TokenTextSplitter

        text = (
            "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ GPT å’Œ BERT å·²ç»å¹¿æ³›åº”ç”¨ã€‚"
        )

        splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=5)

        chunks = splitter.split_text(text)

        print("ğŸ“Œ æŒ‰ Token æ•°é‡åˆ†å‰²ï¼š")
        for i, chunk in enumerate(chunks):
            print(f"  å— {i + 1}: {chunk}")

    except Exception as e:
        print(f"âš ï¸ éœ€è¦å®‰è£… tiktoken: pip install tiktoken")
        print(f"   é”™è¯¯: {e}")


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç‰¹å®šæ ¼å¼åˆ†å‰²å™¨ ====================


def specialized_splitters():
    """ç‰¹å®šæ ¼å¼åˆ†å‰²å™¨"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šç‰¹å®šæ ¼å¼åˆ†å‰²å™¨")
    print("=" * 60)

    print("""
    LangChain æä¾›çš„ä¸“ç”¨åˆ†å‰²å™¨ï¼š
    
    | åˆ†å‰²å™¨                    | ç”¨é€”                |
    |--------------------------|---------------------|
    | MarkdownTextSplitter     | Markdown æ–‡æ¡£       |
    | PythonCodeTextSplitter   | Python ä»£ç          |
    | RecursiveJsonSplitter    | JSON æ•°æ®           |
    | HTMLSectionSplitter      | HTML é¡µé¢           |
    | LatexTextSplitter        | LaTeX æ–‡æ¡£          |
    """)

    # Markdown åˆ†å‰²ç¤ºä¾‹
    from langchain.text_splitter import MarkdownTextSplitter

    md_text = """# æ ‡é¢˜

## ç¬¬ä¸€èŠ‚
è¿™æ˜¯ç¬¬ä¸€èŠ‚çš„å†…å®¹ã€‚

## ç¬¬äºŒèŠ‚
è¿™æ˜¯ç¬¬äºŒèŠ‚çš„å†…å®¹ã€‚"""

    splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)
    chunks = splitter.split_text(md_text)

    print("ğŸ“Œ Markdown åˆ†å‰²ç»“æœï¼š")
    for i, chunk in enumerate(chunks):
        print(f"  å— {i + 1}: {chunk[:40]}...")


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ ====================


def best_practices():
    """æœ€ä½³å®è·µ"""
    print("\n" + "=" * 60)
    print("ç¬¬å…­éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
    é€‰æ‹©åˆ†å‰²ç­–ç•¥çš„å»ºè®®ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. é€šç”¨æ–‡æœ¬ï¼šRecursiveCharacterTextSplitter
       - chunk_size: 500-1000
       - overlap: 50-100
    
    2. ä»£ç æ–‡ä»¶ï¼šç›¸åº”è¯­è¨€çš„ CodeSplitter
       - ä¿æŒå‡½æ•°/ç±»å®Œæ•´æ€§
    
    3. ç»“æ„åŒ–æ–‡æ¡£ï¼ˆMarkdown/HTMLï¼‰ï¼šå¯¹åº”åˆ†å‰²å™¨
       - æŒ‰ç« èŠ‚/æ ‡é¢˜åˆ†å‰²
    
    4. éœ€è¦ç²¾ç¡®æ§åˆ¶ Tokenï¼šTokenTextSplitter
       - é¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶
    
    è°ƒä¼˜æŠ€å·§ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    - æµ‹è¯•ä¸åŒ chunk_sizeï¼Œè¯„ä¼°æ£€ç´¢æ•ˆæœ
    - overlap ä¸è¦å¤ªå¤§ï¼Œé¿å…å†—ä½™
    - ä¿ç•™è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
    """)


# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå¯¹æ¯”åˆ†å‰²æ•ˆæœ
        ç”¨ä¸åŒ chunk_size åˆ†å‰²åŒä¸€æ–‡æ¡£ï¼Œå¯¹æ¯”ç»“æœã€‚

    ç»ƒä¹  2ï¼šä»£ç åˆ†å‰²
        ç”¨ PythonCodeTextSplitter åˆ†å‰²ä¸€ä¸ª Python æ–‡ä»¶ã€‚

    ç»ƒä¹  3ï¼šToken è®¡ç®—
        è®¡ç®—åˆ†å‰²åå„å—çš„ token æ•°é‡ã€‚

    æ€è€ƒé¢˜ï¼š
        1. chunk_size å¦‚ä½•å½±å“ RAG æ•ˆæœï¼Ÿ
        2. ä»€ä¹ˆæƒ…å†µä¸‹éœ€è¦æ›´å¤§çš„ overlapï¼Ÿ
    """)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ–‡æœ¬åˆ†å‰²ç­–ç•¥")
    print("=" * 60)

    try:
        why_split()
        character_splitter()
        recursive_splitter()
        token_splitter()
        specialized_splitters()
        best_practices()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š03-document-transformers.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
