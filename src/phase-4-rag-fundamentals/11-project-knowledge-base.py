"""
å®æˆ˜é¡¹ç›®ï¼šä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
============================

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç»¼åˆè¿ç”¨ RAG æ‰€å­¦çŸ¥è¯†
    2. æ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“ç³»ç»Ÿ
    3. å®ç°å¤šè½®å¯¹è¯é—®ç­”

é¡¹ç›®åŠŸèƒ½ï¼š
    - æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²
    - å‘é‡å­˜å‚¨å’Œæ£€ç´¢
    - å¤šè½®å¯¹è¯é—®ç­”
    - æ¥æºå¼•ç”¨

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install langchain langchain-openai chromadb python-dotenv
"""

import os
from dotenv import load_dotenv

load_dotenv()


# ==================== çŸ¥è¯†åº“ç³»ç»Ÿç±» ====================


class PersonalKnowledgeBase:
    """ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, persist_directory: str = None):
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        from langchain_chroma import Chroma
        from langchain_core.messages import HumanMessage, AIMessage

        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-3.5-turbo")
        self.persist_dir = persist_directory
        self.vectorstore = None
        self.history = []

        # å¦‚æœæœ‰æŒä¹…åŒ–ç›®å½•ï¼Œå°è¯•åŠ è½½
        if persist_directory and os.path.exists(persist_directory):
            self.vectorstore = Chroma(
                persist_directory=persist_directory, embedding_function=self.embeddings
            )
            print(f"âœ… å·²åŠ è½½çŸ¥è¯†åº“: {persist_directory}")

    def add_documents(self, file_paths: list):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        from langchain_community.document_loaders import TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_chroma import Chroma

        all_docs = []

        for path in file_paths:
            if os.path.exists(path):
                loader = TextLoader(path, encoding="utf-8")
                docs = loader.load()
                all_docs.extend(docs)
                print(f"  âœ“ åŠ è½½: {path}")

        if not all_docs:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            return

        # åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(all_docs)
        print(f"  âœ“ åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")

        # å­˜å‚¨
        if self.vectorstore:
            self.vectorstore.add_documents(chunks)
        else:
            self.vectorstore = Chroma.from_documents(
                chunks, self.embeddings, persist_directory=self.persist_dir
            )

        print(f"âœ… å·²æ·»åŠ  {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def _rewrite_question(self, question: str) -> str:
        """æ”¹å†™é—®é¢˜"""
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser

        if not self.history:
            return question

        prompt = ChatPromptTemplate.from_template("""
åŸºäºå¯¹è¯å†å²ï¼Œå°†é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹çš„å®Œæ•´é—®é¢˜ã€‚

å†å²ï¼š
{history}

é—®é¢˜ï¼š{question}

ç‹¬ç«‹é—®é¢˜ï¼š""")

        chain = prompt | self.llm | StrOutputParser()

        history_text = "\n".join(
            [
                f"{'ç”¨æˆ·' if i % 2 == 0 else 'AI'}: {msg.content}"
                for i, msg in enumerate(self.history[-4:])  # åªç”¨æœ€è¿‘2è½®
            ]
        )

        return chain.invoke({"history": history_text, "question": question})

    def chat(self, question: str) -> dict:
        """é—®ç­”"""
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.messages import HumanMessage, AIMessage

        if not self.vectorstore:
            return {"answer": "âŒ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£", "sources": []}

        # æ”¹å†™é—®é¢˜
        standalone = self._rewrite_question(question)

        # æ£€ç´¢
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.invoke(standalone)

        if not docs:
            return {"answer": "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯", "sources": []}

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join(
            [f"[{i + 1}] {d.page_content}" for i, d in enumerate(docs)]
        )

        sources = [d.metadata.get("source", "unknown") for d in docs]

        # ç”Ÿæˆå›ç­”
        prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ã€‚åœ¨å›ç­”æœ«å°¾æ ‡æ³¨å¼•ç”¨çš„ä¿¡æ¯ç¼–å·ã€‚

å‚è€ƒä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š""")

        chain = prompt | self.llm | StrOutputParser()
        answer = chain.invoke({"context": context, "question": standalone})

        # æ›´æ–°å†å²
        self.history.append(HumanMessage(content=question))
        self.history.append(AIMessage(content=answer))

        return {"answer": answer, "sources": list(set(sources))}

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.history = []
        print("âœ“ å¯¹è¯å†å²å·²æ¸…ç©º")


# ==================== æ¼”ç¤º ====================


def demo():
    """æ¼”ç¤ºçŸ¥è¯†åº“ç³»ç»Ÿ"""
    print("=" * 60)
    print("ğŸš€ ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    import tempfile

    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    temp_dir = tempfile.mkdtemp()

    docs_content = {
        "python.txt": """
Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºã€‚
Python çš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç å¯è¯»æ€§ï¼Œä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ã€‚
Python æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‘½ä»¤å¼ã€å‡½æ•°å¼ç¼–ç¨‹ã€‚
Python æ‹¥æœ‰ä¸°å¯Œçš„æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åº“ç”Ÿæ€ç³»ç»Ÿã€‚
""",
        "ml.txt": """
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚
ç›‘ç£å­¦ä¹ ä½¿ç”¨å¸¦æ ‡ç­¾çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
æ— ç›‘ç£å­¦ä¹ ä»æ— æ ‡ç­¾æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚
æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ã€‚
""",
    }

    for name, content in docs_content.items():
        with open(os.path.join(temp_dir, name), "w") as f:
            f.write(content)

    # åˆ›å»ºçŸ¥è¯†åº“
    print("\nğŸ“š åˆ›å»ºçŸ¥è¯†åº“...")
    kb = PersonalKnowledgeBase()

    # æ·»åŠ æ–‡æ¡£
    print("\nğŸ“„ æ·»åŠ æ–‡æ¡£...")
    kb.add_documents([os.path.join(temp_dir, name) for name in docs_content])

    # æµ‹è¯•é—®ç­”
    print("\nğŸ’¬ å¤šè½®å¯¹è¯æµ‹è¯•ï¼š")

    questions = [
        "Python æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å®ƒæ˜¯è°åˆ›å»ºçš„ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
    ]

    for q in questions:
        result = kb.chat(q)
        print(f"\nç”¨æˆ·: {q}")
        print(f"AI: {result['answer']}")
        print(f"æ¥æº: {result['sources']}")

    # æ¸…ç†
    import shutil

    shutil.rmtree(temp_dir)


# ==================== äº¤äº’æ¨¡å¼ä»£ç  ====================


def interactive_code():
    """äº¤äº’æ¨¡å¼ä»£ç ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("äº¤äº’æ¨¡å¼ä»£ç ç¤ºä¾‹")
    print("=" * 60)

    code = """
# ä½¿ç”¨ç¤ºä¾‹

from knowledge_base import PersonalKnowledgeBase

# åˆ›å»ºçŸ¥è¯†åº“ï¼ˆæ”¯æŒæŒä¹…åŒ–ï¼‰
kb = PersonalKnowledgeBase(persist_directory="./my_kb")

# æ·»åŠ æ–‡æ¡£
kb.add_documents([
    "./docs/python.txt",
    "./docs/ml.txt",
])

# äº¤äº’å¼é—®ç­”
print("çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ (è¾“å…¥ 'quit' é€€å‡º)")

while True:
    question = input("\\nä½ : ")
    
    if question.lower() == 'quit':
        break
    
    if question.lower() == 'clear':
        kb.clear_history()
        continue
    
    result = kb.chat(question)
    print(f"AI: {result['answer']}")
    print(f"æ¥æº: {', '.join(result['sources'])}")
"""
    print(code)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® OPENAI_API_KEY")
        return

    try:
        demo()
        interactive_code()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return

    print("\n" + "=" * 60)
    print("ğŸ‰ Phase 4 RAG åŸºç¡€è¯¾ç¨‹å…¨éƒ¨å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼šè¿›å…¥ Phase 5 å­¦ä¹  RAG é«˜çº§æŠ€æœ¯")
    print("=" * 60)


if __name__ == "__main__":
    main()
