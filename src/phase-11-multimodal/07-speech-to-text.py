"""
è¯­éŸ³è½¬æ–‡å­—
==========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ä½¿ç”¨ Whisper API è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
    2. å¤„ç†ä¸åŒæ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶
    3. å®ç°å®æ—¶è¯­éŸ³è½¬å†™

æ ¸å¿ƒæ¦‚å¿µï¼š
    - ASR (Automatic Speech Recognition)
    - Whisperï¼šOpenAI çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹
    - éŸ³é¢‘é¢„å¤„ç†

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install openai pydub
    - éœ€è¦ ffmpegï¼ˆéŸ³é¢‘å¤„ç†ï¼‰
"""

import os
from typing import List
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯­éŸ³è¯†åˆ«æ¦‚è¿° ====================


def introduction():
    """è¯­éŸ³è¯†åˆ«æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯­éŸ³è¯†åˆ«æ¦‚è¿°")
    print("=" * 60)

    print("""
    ğŸ“Œ Whisper æ¨¡å‹ç‰¹ç‚¹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â€¢ å¤šè¯­è¨€æ”¯æŒï¼š100+ ç§è¯­è¨€                              â”‚
    â”‚ â€¢ è‡ªåŠ¨æ£€æµ‹è¯­è¨€                                          â”‚
    â”‚ â€¢ æ”¯æŒç¿»è¯‘ï¼ˆéè‹±è¯­ â†’ è‹±è¯­ï¼‰                            â”‚
    â”‚ â€¢ æ”¯æŒæ—¶é—´æˆ³                                            â”‚
    â”‚ â€¢ å¯å¤„ç†å™ªéŸ³å’Œå£éŸ³                                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼š
    - mp3, mp4, mpeg, mpga, m4a, wav, webm

    ğŸ“Œ API é€‰é¡¹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ transcriptionsâ”‚ è¯­éŸ³è½¬æ–‡å­—ï¼ˆä¿æŒåŸè¯­è¨€ï¼‰             â”‚
    â”‚ translations  â”‚ è¯­éŸ³ç¿»è¯‘ä¸ºè‹±è¯­                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€ä½¿ç”¨ ====================


def basic_usage():
    """åŸºç¡€ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šWhisper API åŸºç¡€ä½¿ç”¨")
    print("=" * 60)

    code = '''
from openai import OpenAI

client = OpenAI()

def transcribe_audio(audio_path: str, language: str = None) -> str:
    """è¯­éŸ³è½¬æ–‡å­—"""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language=language,  # å¯é€‰ï¼Œå¦‚ "zh", "en"
            response_format="text"  # text, json, srt, vtt
        )

    return response

def transcribe_with_timestamps(audio_path: str) -> dict:
    """å¸¦æ—¶é—´æˆ³çš„è½¬å†™"""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["word", "segment"]
        )

    return response

# ä½¿ç”¨ç¤ºä¾‹
# text = transcribe_audio("meeting.mp3", language="zh")
# result = transcribe_with_timestamps("interview.wav")
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯­éŸ³ç¿»è¯‘ ====================


def translation():
    """è¯­éŸ³ç¿»è¯‘"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯­éŸ³ç¿»è¯‘ä¸ºè‹±è¯­")
    print("=" * 60)

    code = '''
def translate_audio(audio_path: str) -> str:
    """å°†éè‹±è¯­è¯­éŸ³ç¿»è¯‘ä¸ºè‹±è¯­æ–‡æœ¬"""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.translations.create(
            model="whisper-1",
            file=audio_file,
            response_format="text"
        )

    return response

# ä¸­æ–‡è¯­éŸ³ â†’ è‹±æ–‡æ–‡æœ¬
# english_text = translate_audio("chinese_audio.mp3")
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šéŸ³é¢‘é¢„å¤„ç† ====================


def audio_preprocessing():
    """éŸ³é¢‘é¢„å¤„ç†"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šéŸ³é¢‘é¢„å¤„ç†")
    print("=" * 60)

    code = '''
from pydub import AudioSegment
import math

def convert_audio(input_path: str, output_path: str, format: str = "mp3"):
    """è½¬æ¢éŸ³é¢‘æ ¼å¼"""
    audio = AudioSegment.from_file(input_path)
    audio.export(output_path, format=format)
    return output_path

def split_audio(audio_path: str, chunk_duration_ms: int = 60000) -> list:
    """
    åˆ†å‰²é•¿éŸ³é¢‘
    Whisper é™åˆ¶ï¼šæœ€å¤§ 25MB
    å»ºè®®ï¼šæŒ‰åˆ†é’Ÿåˆ†å‰²
    """
    audio = AudioSegment.from_file(audio_path)
    duration = len(audio)
    chunks = []

    for i in range(0, duration, chunk_duration_ms):
        chunk = audio[i:i + chunk_duration_ms]
        chunk_path = f"chunk_{i // chunk_duration_ms}.mp3"
        chunk.export(chunk_path, format="mp3")
        chunks.append(chunk_path)

    return chunks

def transcribe_long_audio(audio_path: str) -> str:
    """è½¬å†™é•¿éŸ³é¢‘"""
    # åˆ†å‰²éŸ³é¢‘
    chunks = split_audio(audio_path, chunk_duration_ms=60000)

    # ä¾æ¬¡è½¬å†™
    full_text = []
    for chunk_path in chunks:
        text = transcribe_audio(chunk_path)
        full_text.append(text)
        os.remove(chunk_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶

    return " ".join(full_text)
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šä¸ LLM é›†æˆ ====================


def llm_integration():
    """ä¸ LLM é›†æˆ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šè¯­éŸ³ + LLM é›†æˆ")
    print("=" * 60)

    code = '''
def voice_chat(audio_path: str, system_prompt: str = "") -> str:
    """è¯­éŸ³å¯¹è¯ï¼šè¯­éŸ³è¾“å…¥ â†’ LLM å¤„ç† â†’ æ–‡æœ¬å›å¤"""
    # 1. è¯­éŸ³è½¬æ–‡å­—
    user_text = transcribe_audio(audio_path, language="zh")

    # 2. LLM å¤„ç†
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_text})

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )

    return {
        "user_text": user_text,
        "assistant_text": response.choices[0].message.content
    }

def meeting_assistant(audio_path: str) -> dict:
    """ä¼šè®®åŠ©æ‰‹ï¼šè½¬å†™ + æ‘˜è¦ + å¾…åŠ"""
    # 1. è½¬å†™
    transcript = transcribe_audio(audio_path)

    # 2. LLM åˆ†æ
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": f"""åˆ†æä»¥ä¸‹ä¼šè®®è®°å½•ï¼š

{transcript}

è¯·æä¾›ï¼š
1. ä¼šè®®æ‘˜è¦ï¼ˆ3-5å¥è¯ï¼‰
2. å…³é”®å†³ç­–
3. å¾…åŠäº‹é¡¹ï¼ˆæ˜ç¡®çš„ action itemsï¼‰
4. ä¸‹ä¸€æ­¥è®¡åˆ’"""
            }
        ]
    )

    return {
        "transcript": transcript,
        "analysis": response.choices[0].message.content
    }
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªæ’­å®¢è½¬å†™å·¥å…·
    ç»ƒä¹  2ï¼šæ„å»ºå¤šè¯­è¨€ä¼šè®®è½¬å†™ç³»ç»Ÿ

    æ€è€ƒé¢˜ï¼šå¦‚ä½•æé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®ç‡ï¼Ÿ
    ç­”æ¡ˆï¼š
    1. æä¾›è¯­è¨€å‚æ•°ï¼Œé¿å…è‡ªåŠ¨æ£€æµ‹
    2. éŸ³é¢‘é¢„å¤„ç†ï¼ˆé™å™ªã€å¢ç›Šï¼‰
    3. ä½¿ç”¨é«˜è´¨é‡éº¦å…‹é£å½•åˆ¶
    4. æä¾› prompt å¼•å¯¼ï¼ˆä¸“ä¸šæœ¯è¯­ç­‰ï¼‰
    """)


def main():
    introduction()
    basic_usage()
    translation()
    audio_preprocessing()
    llm_integration()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š08-text-to-speech.py")


if __name__ == "__main__":
    main()
