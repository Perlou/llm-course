"""
è¯­éŸ³è½¬æ–‡å­—
==========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ä½¿ç”¨ Whisper API è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
    2. å¤„ç†ä¸åŒæ ¼å¼çš„éŸ³é¢‘æ–‡ä»¶
    3. å®ç°å®æ—¶è¯­éŸ³è½¬å†™

æ ¸å¿ƒæ¦‚å¿µï¼š
    - ASR (Automatic Speech Recognition)
    - Whisperï¼šOpenAI çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹
    - éŸ³é¢‘é¢„å¤„ç†

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install openai pydub
    - éœ€è¦ ffmpegï¼ˆéŸ³é¢‘å¤„ç†ï¼‰

ğŸ“Œ Gemini è¿ç§»è¯´æ˜ï¼š
    Gemini SDKå½“å‰ä¸æä¾›è¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚

    æ¨èæ›¿ä»£æ–¹æ¡ˆï¼š
    1. Google Cloud Speech-to-Text APIï¼ˆä¼ä¸šçº§ï¼Œé«˜å‡†ç¡®åº¦ï¼‰
    2. SpeechRecognitionåº“ï¼ˆå…è´¹ï¼Œæ˜“ç”¨ï¼‰
    3. faster-whisperï¼ˆæœ¬åœ°é«˜æ€§èƒ½ï¼‰

    æœ¬æ–‡ä»¶ä¿ç•™æ•™å­¦ä»·å€¼ï¼Œå±•ç¤ºè¯­éŸ³è½¬æ–‡å­—çš„æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯ã€‚
    ç¤ºä¾‹ä»£ç ä½¿ç”¨OpenAI Whisper APIæ¼”ç¤ºã€‚
"""

import os
from typing import List
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯­éŸ³è¯†åˆ«æ¦‚è¿° ====================


def introduction():
    """è¯­éŸ³è¯†åˆ«æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯­éŸ³è¯†åˆ«æ¦‚è¿°")
    print("=" * 60)

    print("""
    ğŸ“Œ Whisper æ¨¡å‹ç‰¹ç‚¹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â€¢ å¤šè¯­è¨€æ”¯æŒï¼š100+ ç§è¯­è¨€                              â”‚
    â”‚ â€¢ è‡ªåŠ¨æ£€æµ‹è¯­è¨€                                          â”‚
    â”‚ â€¢ æ”¯æŒç¿»è¯‘ï¼ˆéè‹±è¯­ â†’ è‹±è¯­ï¼‰                            â”‚
    â”‚ â€¢ æ”¯æŒæ—¶é—´æˆ³                                            â”‚
    â”‚ â€¢ å¯å¤„ç†å™ªéŸ³å’Œå£éŸ³                                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼š
    - mp3, mp4, mpeg, mpga, m4a, wav, webm

    ğŸ“Œ API é€‰é¡¹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ transcriptionsâ”‚ è¯­éŸ³è½¬æ–‡å­—ï¼ˆä¿æŒåŸè¯­è¨€ï¼‰             â”‚
    â”‚ translations  â”‚ è¯­éŸ³ç¿»è¯‘ä¸ºè‹±è¯­                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€ä½¿ç”¨ ====================


def basic_usage():
    """åŸºç¡€ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šWhisper API åŸºç¡€ä½¿ç”¨")
    print("=" * 60)

    code = '''
from openai import OpenAI

client = OpenAI()

def transcribe_audio(audio_path: str, language: str = None) -> str:
    """è¯­éŸ³è½¬æ–‡å­—"""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language=language,  # å¯é€‰ï¼Œå¦‚ "zh", "en"
            response_format="text"  # text, json, srt, vtt
        )

    return response

def transcribe_with_timestamps(audio_path: str) -> dict:
    """å¸¦æ—¶é—´æˆ³çš„è½¬å†™"""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["word", "segment"]
        )

    return response

# ä½¿ç”¨ç¤ºä¾‹
# text = transcribe_audio("meeting.mp3", language="zh")
# result = transcribe_with_timestamps("interview.wav")
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯­éŸ³ç¿»è¯‘ ====================


def translation():
    """è¯­éŸ³ç¿»è¯‘"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¯­éŸ³ç¿»è¯‘ä¸ºè‹±è¯­")
    print("=" * 60)

    code = '''
def translate_audio(audio_path: str) -> str:
    """å°†éè‹±è¯­è¯­éŸ³ç¿»è¯‘ä¸ºè‹±è¯­æ–‡æœ¬"""
    with open(audio_path, "rb") as audio_file:
        response = client.audio.translations.create(
            model="whisper-1",
            file=audio_file,
            response_format="text"
        )

    return response

# ä¸­æ–‡è¯­éŸ³ â†’ è‹±æ–‡æ–‡æœ¬
# english_text = translate_audio("chinese_audio.mp3")
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šéŸ³é¢‘é¢„å¤„ç† ====================


def audio_preprocessing():
    """éŸ³é¢‘é¢„å¤„ç†"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šéŸ³é¢‘é¢„å¤„ç†")
    print("=" * 60)

    code = '''
from pydub import AudioSegment
import math

def convert_audio(input_path: str, output_path: str, format: str = "mp3"):
    """è½¬æ¢éŸ³é¢‘æ ¼å¼"""
    audio = AudioSegment.from_file(input_path)
    audio.export(output_path, format=format)
    return output_path

def split_audio(audio_path: str, chunk_duration_ms: int = 60000) -> list:
    """
    åˆ†å‰²é•¿éŸ³é¢‘
    Whisper é™åˆ¶ï¼šæœ€å¤§ 25MB
    å»ºè®®ï¼šæŒ‰åˆ†é’Ÿåˆ†å‰²
    """
    audio = AudioSegment.from_file(audio_path)
    duration = len(audio)
    chunks = []

    for i in range(0, duration, chunk_duration_ms):
        chunk = audio[i:i + chunk_duration_ms]
        chunk_path = f"chunk_{i // chunk_duration_ms}.mp3"
        chunk.export(chunk_path, format="mp3")
        chunks.append(chunk_path)

    return chunks

def transcribe_long_audio(audio_path: str) -> str:
    """è½¬å†™é•¿éŸ³é¢‘"""
    # åˆ†å‰²éŸ³é¢‘
    chunks = split_audio(audio_path, chunk_duration_ms=60000)

    # ä¾æ¬¡è½¬å†™
    full_text = []
    for chunk_path in chunks:
        text = transcribe_audio(chunk_path)
        full_text.append(text)
        os.remove(chunk_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶

    return " ".join(full_text)
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šä¸ LLM é›†æˆ ====================


def llm_integration():
    """ä¸ LLM é›†æˆ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šè¯­éŸ³ + LLM é›†æˆ")
    print("=" * 60)

    code = '''
def voice_chat(audio_path: str, system_prompt: str = "") -> str:
    """è¯­éŸ³å¯¹è¯ï¼šè¯­éŸ³è¾“å…¥ â†’ LLM å¤„ç† â†’ æ–‡æœ¬å›å¤"""
    # 1. è¯­éŸ³è½¬æ–‡å­—
    user_text = transcribe_audio(audio_path, language="zh")

    # 2. LLM å¤„ç†
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_text})

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )

    return {
        "user_text": user_text,
        "assistant_text": response.choices[0].message.content
    }

def meeting_assistant(audio_path: str) -> dict:
    """ä¼šè®®åŠ©æ‰‹ï¼šè½¬å†™ + æ‘˜è¦ + å¾…åŠ"""
    # 1. è½¬å†™
    transcript = transcribe_audio(audio_path)

    # 2. LLM åˆ†æ
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": f"""åˆ†æä»¥ä¸‹ä¼šè®®è®°å½•ï¼š

{transcript}

è¯·æä¾›ï¼š
1. ä¼šè®®æ‘˜è¦ï¼ˆ3-5å¥è¯ï¼‰
2. å…³é”®å†³ç­–
3. å¾…åŠäº‹é¡¹ï¼ˆæ˜ç¡®çš„ action itemsï¼‰
4. ä¸‹ä¸€æ­¥è®¡åˆ’"""
            }
        ]
    )

    return {
        "transcript": transcript,
        "analysis": response.choices[0].message.content
    }
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªæ’­å®¢è½¬å†™å·¥å…·

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from openai import OpenAI
        from pydub import AudioSegment
        import os
        from typing import Dict
        
        class PodcastTranscriber:
            '''æ’­å®¢è½¬å†™å·¥å…·'''
            
            def __init__(self, api_key: str = None):
                self.client = OpenAI(api_key=api_key)
            
            def split_audio(
                self, 
                audio_path: str, 
                chunk_minutes: int = 10
            ) -> list:
                '''åˆ†å‰²é•¿éŸ³é¢‘'''
                audio = AudioSegment.from_file(audio_path)
                chunk_ms = chunk_minutes * 60 * 1000
                chunks = []
                
                for i in range(0, len(audio), chunk_ms):
                    chunk = audio[i:i + chunk_ms]
                    chunk_path = f"temp_chunk_{i // chunk_ms}.mp3"
                    chunk.export(chunk_path, format="mp3")
                    chunks.append(chunk_path)
                
                return chunks
            
            def transcribe_chunk(
                self, 
                audio_path: str,
                language: str = None
            ) -> Dict:
                '''è½¬å†™å•ä¸ªç‰‡æ®µ'''
                with open(audio_path, "rb") as f:
                    response = self.client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        language=language,
                        response_format="verbose_json",
                        timestamp_granularities=["segment"]
                    )
                return response
            
            def transcribe_podcast(
                self, 
                audio_path: str,
                language: str = "zh",
                output_format: str = "text"
            ) -> Dict:
                '''è½¬å†™å®Œæ•´æ’­å®¢'''
                # åˆ†å‰²éŸ³é¢‘
                chunks = self.split_audio(audio_path)
                
                full_text = []
                segments = []
                
                for i, chunk_path in enumerate(chunks):
                    result = self.transcribe_chunk(chunk_path, language)
                    full_text.append(result.text)
                    
                    # è°ƒæ•´æ—¶é—´æˆ³åç§»
                    offset = i * 10 * 60  # æ¯å—10åˆ†é’Ÿ
                    for seg in result.segments:
                        segments.append({
                            'start': seg.start + offset,
                            'end': seg.end + offset,
                            'text': seg.text
                        })
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.remove(chunk_path)
                
                return {
                    'text': ' '.join(full_text),
                    'segments': segments,
                    'duration': segments[-1]['end'] if segments else 0
                }
            
            def generate_summary(self, transcript: str) -> str:
                '''ç”Ÿæˆæ‘˜è¦'''
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{
                        "role": "user",
                        "content": f'''ä¸ºè¿™æœŸæ’­å®¢ç”Ÿæˆæ‘˜è¦ï¼š

{transcript[:8000]}

è¯·æä¾›ï¼š
1. ä¸»é¢˜æ¦‚è¿°ï¼ˆ2-3å¥è¯ï¼‰
2. æ ¸å¿ƒè§‚ç‚¹ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
3. ç²¾å½©ç‰‡æ®µï¼ˆå¼•ç”¨åŸæ–‡ï¼‰
4. æ¨èæ”¶å¬ç†ç”±'''
                    }]
                )
                return response.choices[0].message.content
        
        # ä½¿ç”¨ç¤ºä¾‹
        # transcriber = PodcastTranscriber()
        # result = transcriber.transcribe_podcast("podcast.mp3")
        # summary = transcriber.generate_summary(result['text'])
        ```
    
    ç»ƒä¹  2ï¼šæ„å»ºå¤šè¯­è¨€ä¼šè®®è½¬å†™ç³»ç»Ÿ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        class MultiLangMeetingTranscriber:
            '''å¤šè¯­è¨€ä¼šè®®è½¬å†™ç³»ç»Ÿ'''
            
            def __init__(self, api_key: str = None):
                self.client = OpenAI(api_key=api_key)
            
            def detect_language(self, audio_path: str) -> str:
                '''æ£€æµ‹éŸ³é¢‘è¯­è¨€'''
                with open(audio_path, "rb") as f:
                    # Whisper ä¼šè¿”å›æ£€æµ‹åˆ°çš„è¯­è¨€
                    response = self.client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="verbose_json"
                    )
                return response.language
            
            def transcribe_meeting(
                self, 
                audio_path: str,
                target_language: str = "zh"
            ) -> Dict:
                '''è½¬å†™ä¼šè®®å¹¶ç¿»è¯‘'''
                # 1. è½¬å†™åŸæ–‡
                with open(audio_path, "rb") as f:
                    original = self.client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="verbose_json",
                        timestamp_granularities=["segment"]
                    )
                
                # 2. å¦‚æœä¸æ˜¯ç›®æ ‡è¯­è¨€ï¼Œè¿›è¡Œç¿»è¯‘
                translated_text = None
                if original.language != target_language:
                    response = self.client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{
                            "role": "user",
                            "content": f"å°†ä»¥ä¸‹{original.language}æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼š\\n\\n{original.text}"
                        }]
                    )
                    translated_text = response.choices[0].message.content
                
                # 3. æå–ä¼šè®®ä¿¡æ¯
                analysis = self.analyze_meeting(original.text)
                
                return {
                    'original_language': original.language,
                    'original_text': original.text,
                    'translated_text': translated_text,
                    'segments': original.segments,
                    'analysis': analysis
                }
            
            def analyze_meeting(self, transcript: str) -> Dict:
                '''åˆ†æä¼šè®®å†…å®¹'''
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{
                        "role": "user",
                        "content": f'''åˆ†æè¿™ä¸ªä¼šè®®è®°å½•ï¼Œè¿”å› JSONï¼š

{transcript}

{{
    "summary": "ä¼šè®®æ‘˜è¦",
    "participants": ["å‘è¨€äººåˆ—è¡¨ï¼ˆå¦‚å¯è¯†åˆ«ï¼‰"],
    "key_decisions": ["å…³é”®å†³ç­–"],
    "action_items": [
        {{"assignee": "è´Ÿè´£äºº", "task": "ä»»åŠ¡", "deadline": "æˆªæ­¢æ—¥æœŸ"}}
    ],
    "next_steps": ["ä¸‹ä¸€æ­¥è®¡åˆ’"]
}}'''
                    }]
                )
                
                import json
                return json.loads(response.choices[0].message.content)
        
        # ä½¿ç”¨ç¤ºä¾‹
        # transcriber = MultiLangMeetingTranscriber()
        # result = transcriber.transcribe_meeting("meeting.mp3", target_language="zh")
        # print(f"åŸè¯­è¨€: {result['original_language']}")
        ```

    æ€è€ƒé¢˜ï¼šå¦‚ä½•æé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®ç‡ï¼Ÿ

        âœ… ç­”ï¼š
        1. æŒ‡å®šè¯­è¨€ - æä¾› language å‚æ•°é¿å…è‡ªåŠ¨æ£€æµ‹é”™è¯¯
        2. éŸ³é¢‘é¢„å¤„ç† - é™å™ªã€å¢ç›Šã€å»é™¤é™éŸ³
        3. é«˜è´¨é‡å½•éŸ³ - ä½¿ç”¨å¥½çš„éº¦å…‹é£ï¼Œå‡å°‘ç¯å¢ƒå™ªéŸ³
        4. æä¾› prompt - å‘ŠçŸ¥ä¸“ä¸šæœ¯è¯­ã€äººåç­‰ä¸Šä¸‹æ–‡
        5. åˆ†æ®µå¤„ç† - é•¿éŸ³é¢‘åˆ†å‰²ååˆ†åˆ«å¤„ç†
        6. åå¤„ç†æ ¡æ­£ - ç”¨ LLM çº æ­£å¸¸è§é”™è¯¯å’Œä¸“ä¸šæœ¯è¯­
    """)


def main():
    introduction()
    basic_usage()
    translation()
    audio_preprocessing()
    llm_integration()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š08-text-to-speech.py")


if __name__ == "__main__":
    main()
