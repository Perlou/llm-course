"""
è§†é¢‘ç†è§£
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. äº†è§£è§†é¢‘ç†è§£çš„æ–¹æ³•å’ŒæŒ‘æˆ˜
    2. ä½¿ç”¨å¤šæ¨¡æ€ LLM åˆ†æè§†é¢‘å†…å®¹
    3. å®ç°è§†é¢‘æ‘˜è¦å’Œé—®ç­”

æ ¸å¿ƒæ¦‚å¿µï¼š
    - å¸§é‡‡æ ·ï¼šä»è§†é¢‘ä¸­æå–å…³é”®å¸§
    - æ—¶åºç†è§£ï¼šç†è§£è§†é¢‘çš„æ—¶é—´é¡ºåº
    - è§†é¢‘é—®ç­”ï¼šåŸºäºè§†é¢‘å†…å®¹å›ç­”é—®é¢˜

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install google-generativeai pillow opencv-python

ğŸ“Œ Gemini è¿ç§»è¯´æ˜ï¼š
    æœ¬æ–‡ä»¶å±•ç¤ºè§†é¢‘ç†è§£çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆé€šè¿‡å¸§æå–+å›¾åƒåˆ†æï¼‰ã€‚
    ç¤ºä¾‹ä»£ç ä½¿ç”¨OpenAI APIæ¼”ç¤ºï¼ŒGeminiç­‰ä»·å®ç°å‚è€ƒ02-gpt4-vision.pyé¡¶éƒ¨è¯´æ˜ã€‚
"""

import os
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šè§†é¢‘ç†è§£æ¦‚è¿° ====================


def introduction():
    """è§†é¢‘ç†è§£æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šè§†é¢‘ç†è§£æ¦‚è¿°")
    print("=" * 60)

    print("""
    ğŸ“Œ è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. æ•°æ®é‡å¤§ï¼š1åˆ†é’Ÿè§†é¢‘ = 1800å¸§ (30fps)               â”‚
    â”‚ 2. æ—¶åºä¾èµ–ï¼šéœ€è¦ç†è§£å¸§ä¹‹é—´çš„å…³ç³»                      â”‚
    â”‚ 3. è®¡ç®—æˆæœ¬ï¼šå¤„ç†å¤§é‡å¸§æ¶ˆè€—èµ„æº                        â”‚
    â”‚ 4. ä¸Šä¸‹æ–‡é•¿åº¦ï¼šToken é™åˆ¶                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ å¸¸ç”¨æ–¹æ³•ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å…³é”®å¸§é‡‡æ ·      â”‚ å‡åŒ€æˆ–æ™ºèƒ½é€‰å–ä»£è¡¨æ€§å¸§              â”‚
    â”‚ åœºæ™¯åˆ†å‰²       â”‚ æŒ‰åœºæ™¯å˜åŒ–åˆ†æ®µå¤„ç†                   â”‚
    â”‚ è§†é¢‘æ‘˜è¦       â”‚ ç”Ÿæˆè§†é¢‘å†…å®¹æ¦‚è¿°                     â”‚
    â”‚ è§†é¢‘é—®ç­”       â”‚ åŸºäºè§†é¢‘å›ç­”é—®é¢˜                     â”‚
    â”‚ åŸç”Ÿè§†é¢‘æ¨¡å‹   â”‚ Gemini 2.0 ç­‰æ”¯æŒç›´æ¥è¾“å…¥è§†é¢‘       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æ”¯æŒè§†é¢‘çš„æ¨¡å‹ï¼š
    - Gemini 2.0ï¼šåŸç”Ÿè§†é¢‘ç†è§£
    - Qwen2-VLï¼šæ”¯æŒè§†é¢‘è¾“å…¥
    - GPT-4oï¼šé€šè¿‡å¸§é‡‡æ ·åˆ†æ
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå¸§é‡‡æ · ====================


def frame_sampling():
    """å¸§é‡‡æ ·"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šè§†é¢‘å¸§é‡‡æ ·")
    print("=" * 60)

    code = '''
import cv2
import base64
from PIL import Image
import io

def extract_frames(video_path: str, num_frames: int = 10) -> list:
    """ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·å¸§"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps

    print(f"è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps}fps, {duration:.1f}ç§’")

    # å‡åŒ€é‡‡æ ·
    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]

    frames = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            # BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append({
                "frame_idx": idx,
                "timestamp": idx / fps,
                "image": Image.fromarray(frame_rgb)
            })

    cap.release()
    return frames

def frames_to_base64(frames: list) -> list:
    """å°†å¸§è½¬æ¢ä¸º base64"""
    base64_frames = []
    for frame in frames:
        buffer = io.BytesIO()
        frame["image"].save(buffer, format="JPEG", quality=85)
        b64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        base64_frames.append({
            "timestamp": frame["timestamp"],
            "base64": b64
        })
    return base64_frames
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†é¢‘åˆ†æ ====================


def video_analysis():
    """è§†é¢‘åˆ†æ"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†é¢‘å†…å®¹åˆ†æ")
    print("=" * 60)

    code = '''
from openai import OpenAI

client = OpenAI()

def analyze_video(video_path: str, num_frames: int = 8) -> str:
    """åˆ†æè§†é¢‘å†…å®¹"""
    # æå–å¸§
    frames = extract_frames(video_path, num_frames)
    base64_frames = frames_to_base64(frames)

    # æ„å»ºè¯·æ±‚
    content = [{
        "type": "text",
        "text": """è¿™æ˜¯ä¸€ä¸ªè§†é¢‘çš„å…³é”®å¸§åºåˆ—ï¼Œè¯·åˆ†æè§†é¢‘å†…å®¹ï¼š

1. è§†é¢‘ä¸»é¢˜ï¼šè¿™ä¸ªè§†é¢‘åœ¨è®²ä»€ä¹ˆï¼Ÿ
2. åœºæ™¯æè¿°ï¼šä¸»è¦åœºæ™¯å’Œç¯å¢ƒ
3. å…³é”®äº‹ä»¶ï¼šæŒ‰æ—¶é—´é¡ºåºåˆ—å‡ºå‘ç”Ÿçš„äº‹æƒ…
4. äººç‰©/ç‰©ä½“ï¼šä¸»è¦å‡ºç°çš„äººç‰©æˆ–ç‰©ä½“
5. æ•´ä½“æ‘˜è¦ï¼šç”¨2-3å¥è¯æ€»ç»“è§†é¢‘

å¸§åºåˆ—ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ï¼š"""
    }]

    for i, frame in enumerate(base64_frames):
        content.append({"type": "text", "text": f"\\nå¸§ {i+1} (æ—¶é—´: {frame['timestamp']:.1f}ç§’):"})
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{frame['base64']}"
            }
        })

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=1500
    )

    return response.choices[0].message.content
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šè§†é¢‘é—®ç­” ====================


def video_qa():
    """è§†é¢‘é—®ç­”"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šè§†é¢‘é—®ç­”")
    print("=" * 60)

    code = '''
def video_question_answer(
    video_path: str,
    question: str,
    num_frames: int = 8
) -> str:
    """åŸºäºè§†é¢‘å›ç­”é—®é¢˜"""
    frames = extract_frames(video_path, num_frames)
    base64_frames = frames_to_base64(frames)

    content = [{
        "type": "text",
        "text": f"""è¿™æ˜¯ä¸€ä¸ªè§†é¢‘çš„å…³é”®å¸§åºåˆ—ã€‚è¯·æ ¹æ®è§†é¢‘å†…å®¹å›ç­”é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

å¸§åºåˆ—ï¼š"""
    }]

    for i, frame in enumerate(base64_frames):
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{frame['base64']}"
            }
        })

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=800
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
# answer = video_question_answer("demo.mp4", "è§†é¢‘ä¸­æœ‰å¤šå°‘äººï¼Ÿ")
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šä½¿ç”¨ Gemini ====================


def gemini_video():
    """ä½¿ç”¨ Gemini åŸç”Ÿè§†é¢‘ç†è§£"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šGemini åŸç”Ÿè§†é¢‘ç†è§£")
    print("=" * 60)

    code = '''
import google.generativeai as genai
import time

# é…ç½® API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def analyze_video_gemini(video_path: str, prompt: str) -> str:
    """ä½¿ç”¨ Gemini åˆ†æè§†é¢‘ï¼ˆåŸç”Ÿæ”¯æŒï¼‰"""
    # ä¸Šä¼ è§†é¢‘æ–‡ä»¶
    video_file = genai.upload_file(path=video_path)

    # ç­‰å¾…å¤„ç†å®Œæˆ
    while video_file.state.name == "PROCESSING":
        print("å¤„ç†ä¸­...")
        time.sleep(5)
        video_file = genai.get_file(video_file.name)

    if video_file.state.name == "FAILED":
        raise ValueError("è§†é¢‘å¤„ç†å¤±è´¥")

    # è°ƒç”¨æ¨¡å‹
    model = genai.GenerativeModel("gemini-2.0-flash")
    response = model.generate_content(
        [video_file, prompt],
        generation_config={"max_output_tokens": 2000}
    )

    return response.text

# ä½¿ç”¨ç¤ºä¾‹
# result = analyze_video_gemini("video.mp4", "æ€»ç»“è¿™ä¸ªè§†é¢‘çš„å†…å®¹")
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°æ™ºèƒ½å¸§é‡‡æ ·ï¼ˆåŸºäºåœºæ™¯å˜åŒ–ï¼‰

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import cv2
        import numpy as np
        from PIL import Image
        
        class SmartFrameSampler:
            '''æ™ºèƒ½å¸§é‡‡æ ·å™¨ - åŸºäºåœºæ™¯å˜åŒ–'''
            
            def __init__(self, threshold: float = 30.0):
                '''
                threshold: åœºæ™¯å˜åŒ–é˜ˆå€¼ï¼Œè¶Šå°è¶Šæ•æ„Ÿ
                '''
                self.threshold = threshold
            
            def calculate_frame_diff(self, frame1, frame2) -> float:
                '''è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„å·®å¼‚'''
                # è½¬æ¢ä¸ºç°åº¦å›¾
                gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
                gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
                
                # è®¡ç®—ç›´æ–¹å›¾
                hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])
                hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])
                
                # å½’ä¸€åŒ–
                cv2.normalize(hist1, hist1)
                cv2.normalize(hist2, hist2)
                
                # è®¡ç®—ç›¸å…³æ€§ï¼ˆ1 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ-1 è¡¨ç¤ºå®Œå…¨ä¸åŒï¼‰
                correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
                
                # è½¬æ¢ä¸ºå·®å¼‚åˆ†æ•°ï¼ˆ0-100ï¼‰
                diff_score = (1 - correlation) * 100
                return diff_score
            
            def extract_keyframes(
                self, 
                video_path: str,
                min_frames: int = 5,
                max_frames: int = 20
            ) -> list:
                '''æå–å…³é”®å¸§'''
                cap = cv2.VideoCapture(video_path)
                fps = cap.get(cv2.CAP_PROP_FPS)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                
                keyframes = []
                prev_frame = None
                frame_idx = 0
                
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    if prev_frame is None:
                        # ç¬¬ä¸€å¸§æ°¸è¿œæ˜¯å…³é”®å¸§
                        keyframes.append({
                            'frame_idx': frame_idx,
                            'timestamp': frame_idx / fps,
                            'image': Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)),
                            'reason': 'first_frame'
                        })
                    else:
                        diff = self.calculate_frame_diff(prev_frame, frame)
                        if diff > self.threshold:
                            keyframes.append({
                                'frame_idx': frame_idx,
                                'timestamp': frame_idx / fps,
                                'image': Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)),
                                'reason': f'scene_change (diff={diff:.1f})'
                            })
                    
                    prev_frame = frame
                    frame_idx += 1
                
                cap.release()
                
                # ç¡®ä¿å¸§æ•°åœ¨èŒƒå›´å†…
                if len(keyframes) < min_frames:
                    # è¡¥å……å‡åŒ€é‡‡æ ·
                    return self.uniform_sample(video_path, min_frames)
                elif len(keyframes) > max_frames:
                    # ä¿ç•™æœ€é‡è¦çš„å¸§
                    step = len(keyframes) // max_frames
                    keyframes = keyframes[::step][:max_frames]
                
                return keyframes
        
        # ä½¿ç”¨ç¤ºä¾‹
        # sampler = SmartFrameSampler(threshold=25.0)
        # frames = sampler.extract_keyframes("video.mp4", min_frames=8)
        ```
    
    ç»ƒä¹  2ï¼šæ„å»ºè§†é¢‘å†…å®¹å®¡æ ¸ç³»ç»Ÿ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import google.generativeai as genai
        
        class VideoContentModerator:
            '''è§†é¢‘å†…å®¹å®¡æ ¸ç³»ç»Ÿ'''
            
            def __init__(self, api_key: str):
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                self.sampler = SmartFrameSampler(threshold=20.0)
            
            def moderate_video(self, video_path: str) -> Dict:
                '''å®¡æ ¸è§†é¢‘å†…å®¹'''
                # 1. æå–å…³é”®å¸§
                keyframes = self.sampler.extract_keyframes(
                    video_path, 
                    min_frames=10, 
                    max_frames=20
                )
                
                # 2. æ„å»ºå®¡æ ¸è¯·æ±‚
                content = ['''å®¡æ ¸è¿™ä¸ªè§†é¢‘çš„å†…å®¹å®‰å…¨æ€§ã€‚

åˆ†ææ¯ä¸€å¸§å¹¶è¿”å›ç»¼åˆè¯„ä¼° JSONï¼š
{
    "overall_safe": true/false,
    "risk_level": "low/medium/high/critical",
    "categories": {
        "violence": {"detected": bool, "frames": [å¸§ç¼–å·], "severity": "..."},
        "adult": {"detected": bool, "frames": [å¸§ç¼–å·], "severity": "..."},
        "dangerous": {"detected": bool, "frames": [å¸§ç¼–å·], "severity": "..."},
        "hate_speech": {"detected": bool, "frames": [å¸§ç¼–å·], "severity": "..."}
    },
    "flagged_moments": [
        {"timestamp": ç§’, "reason": "åŸå› "}
    ],
    "recommendation": "approve/manual_review/reject",
    "summary": "è§†é¢‘å†…å®¹æ¦‚è¿°"
}''']
                
                for i, frame in enumerate(keyframes):
                    content.append(f"å¸§ {i+1} (æ—¶é—´: {frame['timestamp']:.1f}ç§’):")
                    content.append(frame['image'])
                
                response = self.model.generate_content(content)
                
                import json
                return json.loads(response.text)
            
            def batch_moderate(self, video_paths: list) -> list:
                '''æ‰¹é‡å®¡æ ¸'''
                results = []
                for path in video_paths:
                    result = self.moderate_video(path)
                    result['path'] = path
                    results.append(result)
                return results
        
        # ä½¿ç”¨ç¤ºä¾‹
        # moderator = VideoContentModerator(os.getenv("GOOGLE_API_KEY"))
        # result = moderator.moderate_video("uploaded_video.mp4")
        # if result['recommendation'] == 'reject':
        #     print(f"è§†é¢‘è¢«æ‹’ç»: {result['summary']}")
        ```

    æ€è€ƒé¢˜ï¼šå¸§é‡‡æ ·æ•°é‡å¦‚ä½•é€‰æ‹©ï¼Ÿ

        âœ… ç­”ï¼š
        1. çŸ­è§†é¢‘ï¼ˆ<1åˆ†é’Ÿï¼‰ï¼š6-10 å¸§ï¼Œå‡åŒ€é‡‡æ ·å³å¯
        2. ä¸­ç­‰è§†é¢‘ï¼ˆ1-5åˆ†é’Ÿï¼‰ï¼š10-16 å¸§ï¼Œç»“åˆåœºæ™¯å˜åŒ–
        3. é•¿è§†é¢‘ï¼ˆ>5åˆ†é’Ÿï¼‰ï¼šæŒ‰åœºæ™¯åˆ†æ®µï¼Œæ¯æ®µé‡‡æ ·
        4. å¿«é€Ÿå˜åŒ–åœºæ™¯ï¼ˆåŠ¨ä½œç‰‡ã€ä½“è‚²ï¼‰ï¼šå¢åŠ é‡‡æ ·å¯†åº¦
        5. é™æ€åœºæ™¯ï¼ˆæ¼”è®²ã€æ•™ç¨‹ï¼‰ï¼šå¯å‡å°‘é‡‡æ ·
        6. æˆæœ¬è€ƒè™‘ï¼šæ¯å¸§æ¶ˆè€— tokenï¼Œéœ€å¹³è¡¡è´¨é‡ä¸æˆæœ¬
    """)


def main():
    introduction()
    frame_sampling()
    video_analysis()
    video_qa()
    gemini_video()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š07-speech-to-text.py")


if __name__ == "__main__":
    main()
