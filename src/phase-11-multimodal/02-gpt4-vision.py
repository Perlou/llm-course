"""
Gemini Vision (é‰´äº GPT-4V)
==========

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡ Gemini Vision çš„ API ä½¿ç”¨
    2. äº†è§£å›¾åƒè¾“å…¥çš„æ ¼å¼å’Œé™åˆ¶
    3. å®ç°å¤šå›¾å¯¹è¯å’Œé«˜çº§ç”¨æ³•

æ ¸å¿ƒæ¦‚å¿µï¼š
    - base64 ç¼–ç ï¼šå›¾åƒä¼ è¾“æ ¼å¼
    - å›¾åƒ URLï¼šè¿œç¨‹å›¾åƒå¼•ç”¨
    - å¤šå›¾è¾“å…¥ï¼šåŒæ—¶åˆ†æå¤šå¼ å›¾ç‰‡

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install google-generativeai pillow
    - éœ€è¦ Google API Key

âš ï¸ æˆæœ¬æé†’ï¼šGemini è§†è§‰è°ƒç”¨æŒ‰ token è®¡è´¹

ğŸ“Œ Gemini è¿ç§»è¯´æ˜ï¼š
    æœ¬æ–‡ä»¶å±•ç¤ºè§†è§‰LLMçš„æ ¸å¿ƒæ¦‚å¿µå’Œä½¿ç”¨æ–¹æ³•ã€‚
    ç¤ºä¾‹ä»£ç ä½¿ç”¨OpenAI APIæ¼”ç¤ºï¼ŒGeminiç­‰ä»·å®ç°ï¼š

    # Gemini Vision ç¤ºä¾‹
    import google.generativeai as genai
    from PIL import Image

    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
    model = genai.GenerativeModel('gemini-1.5-flash')

    img = Image.open('image.jpg')
    response = model.generate_content(["æè¿°è¿™å¼ å›¾ç‰‡", img])
    print(response.text)
"""

import os
import base64
from typing import List
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ä½¿ç”¨ ====================


def basic_usage():
    """åŸºç¡€ä½¿ç”¨"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šGemini Vision åŸºç¡€ä½¿ç”¨")
    print("=" * 60)

    code = '''
from openai import OpenAI
import base64

client = OpenAI()

# æ–¹æ³•1ï¼šä½¿ç”¨ base64 ç¼–ç å›¾åƒ
def encode_image(image_path: str) -> str:
    """å°†å›¾åƒç¼–ç ä¸º base64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

image_base64 = encode_image("example.jpg")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            ]
        }
    ],
    max_tokens=500
)

print(response.choices[0].message.content)

# æ–¹æ³•2ï¼šä½¿ç”¨å›¾åƒ URL
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://example.com/image.jpg"
                    }
                }
            ]
        }
    ]
)
'''
    print(code)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾åƒè´¨é‡è®¾ç½® ====================


def image_quality():
    """å›¾åƒè´¨é‡è®¾ç½®"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾åƒè´¨é‡è®¾ç½®")
    print("=" * 60)

    print("""
    ğŸ“Œ detail å‚æ•°ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ low       â”‚ ä½åˆ†è¾¨ç‡ï¼Œ512Ã—512ï¼Œå›ºå®š 85 tokens     â”‚
    â”‚ high      â”‚ é«˜åˆ†è¾¨ç‡ï¼Œç»†èŠ‚æ¨¡å¼ï¼Œæ›´å¤š tokens       â”‚
    â”‚ auto      â”‚ è‡ªåŠ¨é€‰æ‹©ï¼ˆé»˜è®¤ï¼‰                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    code = """
# ä½åˆ†è¾¨ç‡æ¨¡å¼ - å¿«é€Ÿã€ä½æˆæœ¬
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¿™æ˜¯ä»€ä¹ˆï¼Ÿ"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}",
                        "detail": "low"  # ä½åˆ†è¾¨ç‡
                    }
                }
            ]
        }
    ]
)

# é«˜åˆ†è¾¨ç‡æ¨¡å¼ - é€‚åˆç»†èŠ‚åˆ†æ
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "é˜…è¯»å›¾ç‰‡ä¸­çš„æ–‡å­—"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}",
                        "detail": "high"  # é«˜åˆ†è¾¨ç‡
                    }
                }
            ]
        }
    ]
)
"""
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤šå›¾è¾“å…¥ ====================


def multiple_images():
    """å¤šå›¾è¾“å…¥"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤šå›¾è¾“å…¥")
    print("=" * 60)

    code = '''
def compare_images(image_paths: list, question: str):
    """æ¯”è¾ƒå¤šå¼ å›¾ç‰‡"""
    content = [{"type": "text", "text": question}]

    for path in image_paths:
        image_base64 = encode_image(path)
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_base64}"
            }
        })

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=1000
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
result = compare_images(
    ["before.jpg", "after.jpg"],
    "æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡çš„åŒºåˆ«"
)
print(result)
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šå®ç”¨åœºæ™¯ ====================


def practical_examples():
    """å®ç”¨åœºæ™¯"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šå®ç”¨åœºæ™¯ç¤ºä¾‹")
    print("=" * 60)

    print("""
    ğŸ“Œ åœºæ™¯1ï¼šå›¾ç‰‡ OCR
    """)

    ocr_code = '''
def image_ocr(image_path: str) -> str:
    """ä»å›¾ç‰‡ä¸­æå–æ–‡å­—"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ï¼Œä¿æŒåŸæœ‰æ ¼å¼ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}",
                            "detail": "high"
                        }
                    }
                ]
            }
        ],
        max_tokens=2000
    )

    return response.choices[0].message.content
'''
    print(ocr_code)

    print("""
    ğŸ“Œ åœºæ™¯2ï¼šå›¾è¡¨åˆ†æ
    """)

    chart_code = '''
def analyze_chart(image_path: str) -> str:
    """åˆ†æå›¾è¡¨å†…å®¹"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """åˆ†æè¿™å¼ å›¾è¡¨ï¼Œè¯·æä¾›ï¼š
1. å›¾è¡¨ç±»å‹
2. ä¸»è¦æ•°æ®ç‚¹
3. è¶‹åŠ¿åˆ†æ
4. å…³é”®æ´å¯Ÿ"""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}",
                            "detail": "high"
                        }
                    }
                ]
            }
        ],
        max_tokens=1500
    )

    return response.choices[0].message.content
'''
    print(chart_code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ ====================


def best_practices():
    """æœ€ä½³å®è·µ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
    ğŸ“Œ å›¾åƒä¼˜åŒ–ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. å‹ç¼©å¤§å›¾ä»¥å‡å°‘æˆæœ¬                                  â”‚
    â”‚ 2. ç®€å•ä»»åŠ¡ç”¨ detail="low"                             â”‚
    â”‚ 3. OCR/ç»†èŠ‚ä»»åŠ¡ç”¨ detail="high"                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ Prompt æŠ€å·§ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. æ˜ç¡®æŒ‡å‡ºè¦åˆ†æå›¾ç‰‡çš„å“ªäº›æ–¹é¢                        â”‚
    â”‚ 2. è¦æ±‚ç»“æ„åŒ–è¾“å‡ºï¼ˆJSONã€åˆ—è¡¨ç­‰ï¼‰                      â”‚
    â”‚ 3. å¤šå›¾æ—¶æ˜ç¡®æŒ‡å‡º"ç¬¬ä¸€å¼ "ã€"ç¬¬äºŒå¼ "                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æˆæœ¬æ§åˆ¶ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â€¢ é¢„å¤„ç†ï¼šå‹ç¼©å›¾ç‰‡åˆ°åˆç†åˆ†è¾¨ç‡                        â”‚
    â”‚ â€¢ æ‰¹å¤„ç†ï¼šåˆå¹¶ç›¸å…³å›¾ç‰‡è¯·æ±‚                            â”‚
    â”‚ â€¢ ç¼“å­˜ï¼šç¼“å­˜é‡å¤å›¾ç‰‡çš„åˆ†æç»“æœ                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ GPT-4o Token è®¡ç®—ï¼š
    - low: å›ºå®š 85 tokens
    - high: åŸºç¡€ 85 + æ¯ä¸ª 512Ã—512 å— 170 tokens
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªå›¾ç‰‡æè¿°å‡½æ•°

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import google.generativeai as genai
        from PIL import Image
        import base64
        from typing import Optional
        
        class ImageDescriber:
            '''å›¾ç‰‡æè¿°å™¨'''
            
            def __init__(self, api_key: str):
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
            
            def describe(
                self,
                image_path: str,
                detail_level: str = "medium",
                language: str = "zh"
            ) -> str:
                '''æè¿°å›¾ç‰‡å†…å®¹'''
                prompts = {
                    "brief": "ç”¨ä¸€å¥è¯æè¿°è¿™å¼ å›¾ç‰‡",
                    "medium": "æè¿°è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“å’Œæ´»åŠ¨",
                    "detailed": '''è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼š
                        1. åœºæ™¯å’Œç¯å¢ƒ
                        2. ä¸»è¦ç‰©ä½“å’Œäººç‰©
                        3. é¢œè‰²å’Œå…‰çº¿
                        4. æƒ…æ„Ÿå’Œæ°›å›´
                        5. ä»»ä½•æ–‡å­—æˆ–æ ‡å¿—'''
                }
                
                img = Image.open(image_path)
                lang_prompt = "è¯·ç”¨ä¸­æ–‡å›ç­”" if language == "zh" else ""
                
                response = self.model.generate_content([
                    prompts.get(detail_level, prompts["medium"]) + lang_prompt,
                    img
                ])
                
                return response.text
            
            def describe_from_url(self, image_url: str) -> str:
                '''ä» URL æè¿°å›¾ç‰‡'''
                import requests
                from io import BytesIO
                
                response = requests.get(image_url)
                img = Image.open(BytesIO(response.content))
                
                return self.model.generate_content([
                    "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹",
                    img
                ]).text
        
        # ä½¿ç”¨ç¤ºä¾‹
        # describer = ImageDescriber(os.getenv("GOOGLE_API_KEY"))
        # desc = describer.describe("photo.jpg", detail_level="detailed")
        ```
    
    ç»ƒä¹  2ï¼šå®ç°ä¸€ä¸ªå¤šå›¾æ¯”è¾ƒåˆ†æ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        class MultiImageAnalyzer:
            '''å¤šå›¾åˆ†æå™¨'''
            
            def __init__(self, api_key: str):
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
            
            def compare(
                self,
                image_paths: list,
                comparison_type: str = "general"
            ) -> str:
                '''æ¯”è¾ƒå¤šå¼ å›¾ç‰‡'''
                prompts = {
                    "general": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„ç›¸ä¼¼ç‚¹å’Œä¸åŒç‚¹",
                    "style": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„é£æ ¼å’Œè§†è§‰æ•ˆæœå·®å¼‚",
                    "content": "åˆ†æè¿™äº›å›¾ç‰‡åœ¨å†…å®¹ä¸Šçš„å…³è”å’ŒåŒºåˆ«",
                    "timeline": "å¦‚æœè¿™æ˜¯æŒ‰æ—¶é—´é¡ºåºçš„å›¾ç‰‡ï¼Œæè¿°å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–",
                    "quality": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„è´¨é‡å’Œæ¸…æ™°åº¦"
                }
                
                content = [prompts.get(comparison_type, prompts["general"])]
                
                for i, path in enumerate(image_paths):
                    img = Image.open(path)
                    content.append(f"å›¾ç‰‡ {i+1}:")
                    content.append(img)
                
                response = self.model.generate_content(content)
                return response.text
            
            def find_differences(
                self, 
                image1_path: str, 
                image2_path: str
            ) -> str:
                '''æ‰¾å‡ºä¸¤å¼ å›¾ç‰‡çš„å·®å¼‚ï¼ˆæ‰¾èŒ¬æ¸¸æˆï¼‰'''
                img1 = Image.open(image1_path)
                img2 = Image.open(image2_path)
                
                prompt = '''ä»”ç»†å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡ï¼Œæ‰¾å‡ºæ‰€æœ‰ä¸åŒä¹‹å¤„ã€‚
                åˆ—å‡ºæ¯ä¸ªå·®å¼‚çš„ä½ç½®å’Œå…·ä½“å˜åŒ–ã€‚'''
                
                response = self.model.generate_content([prompt, img1, img2])
                return response.text
        
        # ä½¿ç”¨ç¤ºä¾‹
        # analyzer = MultiImageAnalyzer(os.getenv("GOOGLE_API_KEY"))
        # result = analyzer.compare(
        #     ["before.jpg", "after.jpg"], 
        #     comparison_type="timeline"
        # )
        ```

    æ€è€ƒé¢˜ï¼šä»€ä¹ˆæ—¶å€™ç”¨ lowï¼Œä»€ä¹ˆæ—¶å€™ç”¨ highï¼Ÿ

        âœ… ç­”ï¼š
        - low é€‚ç”¨åœºæ™¯ï¼š
          1. ç®€å•åˆ†ç±»ä»»åŠ¡ï¼ˆæ˜¯å¦/ç±»åˆ«åˆ¤æ–­ï¼‰
          2. å¿«é€Ÿé¢„è§ˆæˆ–ç­›é€‰
          3. æˆæœ¬æ•æ„Ÿçš„æ‰¹é‡å¤„ç†
          4. ä¸éœ€è¦è¯»å–ç»†èŠ‚çš„åœºæ™¯
        
        - high é€‚ç”¨åœºæ™¯ï¼š
          1. OCR æ–‡å­—è¯†åˆ«
          2. å›¾è¡¨æ•°æ®æå–
          3. æ–‡æ¡£åˆ†æ
          4. ç»†èŠ‚è¯†åˆ«ï¼ˆå°ç‰©ä½“ã€è¿œå¤„å†…å®¹ï¼‰
          5. ä¸“ä¸šå›¾åƒåˆ†æï¼ˆåŒ»å­¦ã€æ³•å¾‹ï¼‰
    """)


def main():
    basic_usage()
    image_quality()
    multiple_images()
    practical_examples()
    best_practices()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š03-image-understanding.py")


if __name__ == "__main__":
    main()
