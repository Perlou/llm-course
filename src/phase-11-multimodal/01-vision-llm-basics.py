"""
è§†è§‰ LLM åŸºç¡€
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£å¤šæ¨¡æ€ LLM çš„æ¦‚å¿µå’Œå‘å±•
    2. æŒæ¡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ¶æ„
    3. äº†è§£ä¸»æµå¤šæ¨¡æ€æ¨¡å‹

æ ¸å¿ƒæ¦‚å¿µï¼š
    - æ¨¡æ€ (Modality)ï¼šä¿¡æ¯çš„ä¸åŒå½¢å¼
    - Vision Encoderï¼šè§†è§‰ç¼–ç å™¨
    - æ¨¡æ€å¯¹é½ï¼šå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç©ºé—´

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install transformers torch pillow
"""


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä»€ä¹ˆæ˜¯å¤šæ¨¡æ€ ====================


def introduction():
    """ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šä»€ä¹ˆæ˜¯å¤šæ¨¡æ€")
    print("=" * 60)

    print("""
    ğŸ“Œ æ¨¡æ€ (Modality) çš„ç±»å‹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ“ æ–‡æœ¬ (Text)      - æ–‡ç« ã€å¯¹è¯ã€ä»£ç                â”‚
    â”‚  ğŸ–¼ï¸  å›¾åƒ (Image)     - ç…§ç‰‡ã€å›¾è¡¨ã€æˆªå›¾               â”‚
    â”‚  ğŸµ éŸ³é¢‘ (Audio)     - è¯­éŸ³ã€éŸ³ä¹ã€ç¯å¢ƒéŸ³             â”‚
    â”‚  ğŸ¬ è§†é¢‘ (Video)     - å½±ç‰‡ã€åŠ¨ç”»ã€å®æ—¶æµ             â”‚
    â”‚  ğŸ“Š ç»“æ„åŒ–æ•°æ®       - è¡¨æ ¼ã€å›¾è°±ã€æ•°æ®åº“             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ å•æ¨¡æ€ vs å¤šæ¨¡æ€ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å•æ¨¡æ€ LLM:                                           â”‚
    â”‚   æ–‡æœ¬ â†’ LLM â†’ æ–‡æœ¬                                   â”‚
    â”‚                                                        â”‚
    â”‚ å¤šæ¨¡æ€ LLM:                                           â”‚
    â”‚   æ–‡æœ¬ â”€â”€â”                                            â”‚
    â”‚   å›¾åƒ â”€â”€â”¼â”€â”€â†’ MLLM â†’ æ–‡æœ¬/å›¾åƒ...                    â”‚
    â”‚   éŸ³é¢‘ â”€â”€â”˜                                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¨¡æ€ï¼Ÿ
    - ä¿¡æ¯å®Œæ•´æ€§ï¼šç›´æ¥ç†è§£å›¾åƒã€è§†é¢‘å†…å®¹
    - äº¤äº’è‡ªç„¶æ€§ï¼šæ”¯æŒè¯­éŸ³ã€å›¾ç‰‡ç­‰è‡ªç„¶äº¤äº’
    - ä»»åŠ¡è¦†ç›–ï¼šå›¾æ–‡é—®ç­”ã€è§†é¢‘ç†è§£ç­‰
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæ¶æ„ ====================


def architecture():
    """æ ¸å¿ƒæ¶æ„"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå¤šæ¨¡æ€ LLM æ ¸å¿ƒæ¶æ„")
    print("=" * 60)

    print("""
    ğŸ“Œ é€šç”¨æ¶æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  è¾“å…¥    â”‚    â”‚ æ¨¡æ€ç¼–ç  â”‚    â”‚  å¯¹é½/   â”‚    â”‚   LLM    â”‚
    â”‚  æ¨¡æ€    â”‚â”€â”€â”€â–ºâ”‚ Encoders â”‚â”€â”€â”€â–ºâ”‚  èåˆå±‚  â”‚â”€â”€â”€â–ºâ”‚ Backbone â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ è§†è§‰ç¼–ç å™¨ (Vision Encoder)ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ç¼–ç å™¨    â”‚   æ¥æº     â”‚       ç‰¹ç‚¹        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ ViT         â”‚ Google     â”‚ æ ‡å‡† Transformer  â”‚
    â”‚ CLIP-ViT    â”‚ OpenAI     â”‚ å›¾æ–‡å¯¹é½é¢„è®­ç»ƒ    â”‚
    â”‚ SigLIP      â”‚ Google     â”‚ Sigmoid æŸå¤±æ”¹è¿›  â”‚
    â”‚ DINOv2      â”‚ Meta       â”‚ è‡ªç›‘ç£å¼ºç‰¹å¾      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æ¨¡æ€å¯¹é½æ–¹æ³•ï¼š
    1. çº¿æ€§æŠ•å½± (LLaVA) - ç®€å•é«˜æ•ˆ
    2. MLP æŠ•å½±å™¨ (LLaVA-1.5) - ä¸¤å±‚ MLP
    3. Q-Former (BLIP-2) - å‹ç¼©è§†è§‰ token
    4. Cross-Attention (Flamingo) - æ·±åº¦èåˆ
    """)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»æµæ¨¡å‹ä»‹ç» ====================


def models_overview():
    """ä¸»æµæ¨¡å‹ä»‹ç»"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»æµå¤šæ¨¡æ€æ¨¡å‹")
    print("=" * 60)

    print("""
    ğŸ“Œ é—­æºæ¨¡å‹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GPT-4o      â”‚ OpenAIï¼ŒåŸç”Ÿå¤šæ¨¡æ€ï¼Œå®æ—¶è¯­éŸ³äº¤äº’     â”‚
    â”‚ Claude 3.5  â”‚ Anthropicï¼Œå¼ºæ–‡æ¡£/å›¾è¡¨ç†è§£           â”‚
    â”‚ Gemini 2.0  â”‚ Googleï¼Œå…¨æ¨¡æ€ï¼Œ100ä¸‡+ tokens        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ å¼€æºæ¨¡å‹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLaVA       â”‚ è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼Œç®€å•é«˜æ•ˆ               â”‚
    â”‚ Qwen2-VL    â”‚ é˜¿é‡Œï¼ŒåŠ¨æ€åˆ†è¾¨ç‡ï¼Œä¸­æ–‡ä¼˜åŒ–           â”‚
    â”‚ InternVL2   â”‚ ä¹¦ç”Ÿï¼Œå¼ºè§†è§‰ç¼–ç å™¨ï¼Œå¤šè§„æ¨¡           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ èƒ½åŠ›å¯¹æ¯” (2024.12)ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   æ¨¡å‹    â”‚ å›¾åƒQA â”‚  OCR  â”‚ å›¾è¡¨  â”‚ è§†é¢‘  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ GPT-4o    â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜…â˜† â”‚
    â”‚ Claude3.5 â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜†  â”‚
    â”‚ Qwen2-VL  â”‚ â˜…â˜…â˜…â˜…â˜†â”‚ â˜…â˜…â˜…â˜…â˜…â”‚ â˜…â˜…â˜…â˜…â˜†â”‚ â˜…â˜…â˜…â˜…â˜†â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä»£ç ç¤ºä¾‹ ====================


def code_example():
    """ä»£ç ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šä»£ç ç¤ºä¾‹")
    print("=" * 60)

    code = """
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import torch

# åŠ è½½æ¨¡å‹
model_name = "Qwen/Qwen2-VL-7B-Instruct"
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)

# å‡†å¤‡è¾“å…¥
image = Image.open("example.jpg")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"}
        ]
    }
]

# å¤„ç†è¾“å…¥
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
inputs = processor(
    text=[text],
    images=[image],
    return_tensors="pt"
).to(model.device)

# ç”Ÿæˆè¾“å‡º
generated_ids = model.generate(**inputs, max_new_tokens=512)
output = processor.batch_decode(
    generated_ids[:, inputs.input_ids.shape[1]:],
    skip_special_tokens=True
)[0]

print(output)
"""
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šä½¿ç”¨ Qwen2-VL æˆ– LLaVA åŠ è½½ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
        import torch
        
        class VisionLLM:
            '''è§†è§‰ LLM å°è£…ç±»'''
            
            def __init__(
                self, 
                model_name: str = "Qwen/Qwen2-VL-7B-Instruct"
            ):
                '''åˆå§‹åŒ–æ¨¡å‹'''
                self.processor = AutoProcessor.from_pretrained(model_name)
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.bfloat16,
                    device_map="auto"
                )
            
            def generate(
                self, 
                image, 
                prompt: str,
                max_tokens: int = 512
            ) -> str:
                '''ç”Ÿæˆå“åº”'''
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": image},
                            {"type": "text", "text": prompt}
                        ]
                    }
                ]
                
                text = self.processor.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
                
                inputs = self.processor(
                    text=[text],
                    images=[image],
                    return_tensors="pt"
                ).to(self.model.device)
                
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_tokens
                )
                
                output = self.processor.batch_decode(
                    generated_ids[:, inputs.input_ids.shape[1]:],
                    skip_special_tokens=True
                )[0]
                
                return output
        
        # ä½¿ç”¨ç¤ºä¾‹
        # vision_llm = VisionLLM()
        # from PIL import Image
        # img = Image.open("photo.jpg")
        # response = vision_llm.generate(img, "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ")
        ```
    
    ç»ƒä¹  2ï¼šè¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œè®©æ¨¡å‹æè¿°å›¾ç‰‡å†…å®¹

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from PIL import Image
        import google.generativeai as genai
        
        def describe_image_gemini(
            image_path: str,
            style: str = "detailed"
        ) -> str:
            '''ä½¿ç”¨ Gemini æè¿°å›¾ç‰‡'''
            genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            prompts = {
                "brief": "ç”¨ä¸€å¥è¯æè¿°è¿™å¼ å›¾ç‰‡",
                "detailed": "è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“ã€é¢œè‰²å’Œæ°›å›´",
                "creative": "ç”¨è¯—æ„çš„è¯­è¨€æè¿°è¿™å¼ å›¾ç‰‡",
                "technical": "ä»æ‘„å½±æŠ€æœ¯è§’åº¦åˆ†æè¿™å¼ å›¾ç‰‡"
            }
            
            img = Image.open(image_path)
            response = model.generate_content([
                prompts.get(style, prompts["detailed"]),
                img
            ])
            
            return response.text
        
        # ä½¿ç”¨ç¤ºä¾‹
        # description = describe_image_gemini("sunset.jpg", style="creative")
        # print(description)
        ```

    æ€è€ƒé¢˜ï¼šä¸ºä»€ä¹ˆè§†è§‰ç¼–ç å™¨é€šå¸¸æ˜¯é¢„è®­ç»ƒå†»ç»“çš„ï¼Ÿ

        âœ… ç­”ï¼š
        1. ä¿ç•™çŸ¥è¯† - è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ CLIP-ViTï¼‰å·²é€šè¿‡æµ·é‡æ•°æ®é¢„è®­ç»ƒï¼Œå…·æœ‰å¼ºå¤§çš„å›¾åƒè¡¨ç¤ºèƒ½åŠ›
        2. å‡å°‘æˆæœ¬ - å†»ç»“ç¼–ç å™¨å¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°å’Œè®¡ç®—èµ„æº
        3. é˜²æ­¢é—å¿˜ - å¾®è°ƒå¯èƒ½å¯¼è‡´å·²å­¦åˆ°çš„è§†è§‰çŸ¥è¯†è¢«è¦†ç›–
        4. å¯¹é½å±‚å­¦ä¹  - åªéœ€è®­ç»ƒè½»é‡çš„å¯¹é½å±‚å°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç©ºé—´
        5. ç¨³å®šè®­ç»ƒ - å†»ç»“å¤§å‹ç»„ä»¶æœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§
    """)


def main():
    introduction()
    architecture()
    models_overview()
    code_example()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š02-gpt4-vision.py")


if __name__ == "__main__":
    main()
