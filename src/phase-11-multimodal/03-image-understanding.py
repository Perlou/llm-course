"""
å›¾åƒç†è§£
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡å›¾åƒç†è§£çš„å¸¸è§ä»»åŠ¡ç±»å‹
    2. å®ç°å›¾åƒæè¿°ã€é—®ç­”ã€åˆ†ç±»ç­‰åŠŸèƒ½
    3. å¤„ç†å¤æ‚çš„å›¾åƒåˆ†æåœºæ™¯

æ ¸å¿ƒæ¦‚å¿µï¼š
    - å›¾åƒæè¿° (Image Captioning)
    - è§†è§‰é—®ç­” (VQA)
    - ç‰©ä½“è¯†åˆ«ä¸å®šä½

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install google-generativeai pillow

ğŸ“Œ Gemini è¿ç§»è¯´æ˜ï¼š
    æœ¬æ–‡ä»¶å±•ç¤ºå›¾åƒç†è§£ä»»åŠ¡çš„æ ¸å¿ƒæ¦‚å¿µã€‚
    ç¤ºä¾‹ä»£ç ä½¿ç”¨OpenAI APIæ¼”ç¤ºï¼ŒGeminiç­‰ä»·å®ç°å‚è€ƒ02-gpt4-vision.pyé¡¶éƒ¨è¯´æ˜ã€‚
"""

import os
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå›¾åƒç†è§£ä»»åŠ¡ç±»å‹ ====================


def task_types():
    """å›¾åƒç†è§£ä»»åŠ¡ç±»å‹"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå›¾åƒç†è§£ä»»åŠ¡ç±»å‹")
    print("=" * 60)

    print("""
    ğŸ“Œ å¸¸è§ä»»åŠ¡ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å›¾åƒæè¿°       â”‚ ç”Ÿæˆå›¾åƒçš„è‡ªç„¶è¯­è¨€æè¿°               â”‚
    â”‚ è§†è§‰é—®ç­” (VQA) â”‚ æ ¹æ®å›¾åƒå›ç­”é—®é¢˜                     â”‚
    â”‚ ç‰©ä½“è¯†åˆ«       â”‚ è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“                     â”‚
    â”‚ åœºæ™¯ç†è§£       â”‚ ç†è§£å›¾åƒçš„æ•´ä½“åœºæ™¯å’Œä¸Šä¸‹æ–‡           â”‚
    â”‚ æ–‡å­—è¯†åˆ« (OCR) â”‚ æå–å›¾åƒä¸­çš„æ–‡å­—                     â”‚
    â”‚ å›¾è¡¨åˆ†æ       â”‚ ç†è§£å’Œè§£é‡Šå›¾è¡¨æ•°æ®                   â”‚
    â”‚ ç©ºé—´å…³ç³»       â”‚ ç†è§£ç‰©ä½“é—´çš„ä½ç½®å…³ç³»                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ ä»»åŠ¡éš¾åº¦ï¼š
    ç®€å•ï¼šå›¾åƒåˆ†ç±»ã€ç®€å•æè¿°
    ä¸­ç­‰ï¼šVQAã€ç‰©ä½“è®¡æ•°ã€OCR
    å›°éš¾ï¼šå¤æ‚æ¨ç†ã€å¤šå›¾å¯¹æ¯”ã€ç©ºé—´å…³ç³»
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾åƒæè¿° ====================


def image_captioning():
    """å›¾åƒæè¿°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾åƒæè¿°")
    print("=" * 60)

    code = '''
from openai import OpenAI
import base64

client = OpenAI()

def encode_image(path: str) -> str:
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

def describe_image(image_path: str, style: str = "detailed") -> str:
    """ç”Ÿæˆå›¾åƒæè¿°"""
    image_base64 = encode_image(image_path)

    prompts = {
        "brief": "ç”¨ä¸€å¥è¯æè¿°è¿™å¼ å›¾ç‰‡ã€‚",
        "detailed": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å…ƒç´ ã€åœºæ™¯å’Œæ°›å›´ã€‚",
        "creative": "ç”¨å¯Œæœ‰æƒ³è±¡åŠ›çš„è¯­è¨€æè¿°è¿™å¼ å›¾ç‰‡ï¼Œåƒè®²æ•…äº‹ä¸€æ ·ã€‚",
        "technical": "ä»æŠ€æœ¯è§’åº¦åˆ†æè¿™å¼ å›¾ç‰‡çš„æ„å›¾ã€å…‰çº¿å’Œè‰²å½©ã€‚"
    }

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompts.get(style, prompts["detailed"])},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
# desc = describe_image("photo.jpg", style="detailed")
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†è§‰é—®ç­” ====================


def visual_qa():
    """è§†è§‰é—®ç­”"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†è§‰é—®ç­” (VQA)")
    print("=" * 60)

    code = '''
def visual_question_answer(image_path: str, question: str) -> str:
    """è§†è§‰é—®ç­”"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# å¸¸è§é—®é¢˜ç±»å‹ç¤ºä¾‹
questions = [
    "å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿ",           # è®¡æ•°
    "å›¾ç‰‡ä¸­çš„äººåœ¨åšä»€ä¹ˆï¼Ÿ",       # åŠ¨ä½œè¯†åˆ«
    "è¿™æ˜¯åœ¨å“ªé‡Œæ‹æ‘„çš„ï¼Ÿ",         # åœºæ™¯ç†è§£
    "å›¾ç‰‡ä¸­æœ€çªå‡ºçš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ", # è§†è§‰å±æ€§
    "è¿™å¼ å›¾ç‰‡ç»™äººä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ",     # æƒ…æ„Ÿåˆ†æ
]
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šç‰©ä½“è¯†åˆ«ä¸åˆ†æ ====================


def object_analysis():
    """ç‰©ä½“è¯†åˆ«ä¸åˆ†æ"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šç‰©ä½“è¯†åˆ«ä¸åˆ†æ")
    print("=" * 60)

    code = '''
def identify_objects(image_path: str) -> dict:
    """è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """åˆ†æå›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼Œè¿”å› JSON æ ¼å¼ï¼š
{
    "objects": [
        {"name": "ç‰©ä½“åç§°", "count": æ•°é‡, "position": "ä½ç½®æè¿°"}
    ],
    "main_subject": "ä¸»è¦ä¸»é¢˜",
    "scene": "åœºæ™¯ç±»å‹"
}"""
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        max_tokens=800
    )

    import json
    return json.loads(response.choices[0].message.content)

def analyze_spatial_relations(image_path: str) -> str:
    """åˆ†æç©ºé—´å…³ç³»"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "æè¿°å›¾ç‰‡ä¸­å„å…ƒç´ ä¹‹é—´çš„ç©ºé—´ä½ç½®å…³ç³»ï¼ˆå¦‚ï¼šä¸Šä¸‹ã€å·¦å³ã€å‰åç­‰ï¼‰ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ]
    )

    return response.choices[0].message.content
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šå¤šå›¾åˆ†æ ====================


def multi_image_analysis():
    """å¤šå›¾åˆ†æ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå¤šå›¾åˆ†æ")
    print("=" * 60)

    code = '''
def compare_images(images: list, aspect: str = "general") -> str:
    """æ¯”è¾ƒå¤šå¼ å›¾ç‰‡"""
    content = []

    prompts = {
        "general": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„å¼‚åŒç‚¹ã€‚",
        "style": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„é£æ ¼å·®å¼‚ã€‚",
        "content": "æè¿°è¿™äº›å›¾ç‰‡å†…å®¹ä¸Šçš„å…³è”å’ŒåŒºåˆ«ã€‚",
        "timeline": "å¦‚æœè¿™äº›å›¾ç‰‡æ˜¯æŒ‰æ—¶é—´é¡ºåºçš„ï¼Œæè¿°å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ã€‚"
    }

    content.append({"type": "text", "text": prompts.get(aspect, prompts["general"])})

    for i, img_path in enumerate(images):
        image_base64 = encode_image(img_path)
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
        })

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=1000
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹ï¼šå¯¹æ¯”äº§å“ç…§ç‰‡
# result = compare_images(["product_v1.jpg", "product_v2.jpg"], aspect="content")
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªå›¾ç‰‡å†…å®¹å®¡æ ¸å‡½æ•°
    ç»ƒä¹  2ï¼šå®ç°å•†å“å›¾ç‰‡è‡ªåŠ¨åˆ†ç±»

    æ€è€ƒé¢˜ï¼šå¤šæ¨¡æ€ LLM åœ¨å›¾åƒç†è§£ä¸Šæœ‰ä»€ä¹ˆå±€é™ï¼Ÿ
    ç­”æ¡ˆï¼š
    1. ç²¾ç¡®è®¡æ•°å›°éš¾ï¼ˆäººå¤šæ—¶å®¹æ˜“å‡ºé”™ï¼‰
    2. ç©ºé—´å®šä½ä¸å¤Ÿç²¾ç¡®ï¼ˆæ— æ³•è¾“å‡ºåæ ‡ï¼‰
    3. å°ç‰©ä½“è¯†åˆ«èƒ½åŠ›æœ‰é™
    4. å¯èƒ½äº§ç”Ÿè§†è§‰å¹»è§‰
    """)


def main():
    task_types()
    image_captioning()
    visual_qa()
    object_analysis()
    multi_image_analysis()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š04-document-ocr.py")


if __name__ == "__main__":
    main()
