"""
å›¾åƒç†è§£
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡å›¾åƒç†è§£çš„å¸¸è§ä»»åŠ¡ç±»å‹
    2. å®ç°å›¾åƒæè¿°ã€é—®ç­”ã€åˆ†ç±»ç­‰åŠŸèƒ½
    3. å¤„ç†å¤æ‚çš„å›¾åƒåˆ†æåœºæ™¯

æ ¸å¿ƒæ¦‚å¿µï¼š
    - å›¾åƒæè¿° (Image Captioning)
    - è§†è§‰é—®ç­” (VQA)
    - ç‰©ä½“è¯†åˆ«ä¸å®šä½

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install google-generativeai pillow

ğŸ“Œ Gemini è¿ç§»è¯´æ˜ï¼š
    æœ¬æ–‡ä»¶å±•ç¤ºå›¾åƒç†è§£ä»»åŠ¡çš„æ ¸å¿ƒæ¦‚å¿µã€‚
    ç¤ºä¾‹ä»£ç ä½¿ç”¨OpenAI APIæ¼”ç¤ºï¼ŒGeminiç­‰ä»·å®ç°å‚è€ƒ02-gpt4-vision.pyé¡¶éƒ¨è¯´æ˜ã€‚
"""

import os
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå›¾åƒç†è§£ä»»åŠ¡ç±»å‹ ====================


def task_types():
    """å›¾åƒç†è§£ä»»åŠ¡ç±»å‹"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå›¾åƒç†è§£ä»»åŠ¡ç±»å‹")
    print("=" * 60)

    print("""
    ğŸ“Œ å¸¸è§ä»»åŠ¡ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å›¾åƒæè¿°       â”‚ ç”Ÿæˆå›¾åƒçš„è‡ªç„¶è¯­è¨€æè¿°               â”‚
    â”‚ è§†è§‰é—®ç­” (VQA) â”‚ æ ¹æ®å›¾åƒå›ç­”é—®é¢˜                     â”‚
    â”‚ ç‰©ä½“è¯†åˆ«       â”‚ è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“                     â”‚
    â”‚ åœºæ™¯ç†è§£       â”‚ ç†è§£å›¾åƒçš„æ•´ä½“åœºæ™¯å’Œä¸Šä¸‹æ–‡           â”‚
    â”‚ æ–‡å­—è¯†åˆ« (OCR) â”‚ æå–å›¾åƒä¸­çš„æ–‡å­—                     â”‚
    â”‚ å›¾è¡¨åˆ†æ       â”‚ ç†è§£å’Œè§£é‡Šå›¾è¡¨æ•°æ®                   â”‚
    â”‚ ç©ºé—´å…³ç³»       â”‚ ç†è§£ç‰©ä½“é—´çš„ä½ç½®å…³ç³»                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ ä»»åŠ¡éš¾åº¦ï¼š
    ç®€å•ï¼šå›¾åƒåˆ†ç±»ã€ç®€å•æè¿°
    ä¸­ç­‰ï¼šVQAã€ç‰©ä½“è®¡æ•°ã€OCR
    å›°éš¾ï¼šå¤æ‚æ¨ç†ã€å¤šå›¾å¯¹æ¯”ã€ç©ºé—´å…³ç³»
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾åƒæè¿° ====================


def image_captioning():
    """å›¾åƒæè¿°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå›¾åƒæè¿°")
    print("=" * 60)

    code = '''
from openai import OpenAI
import base64

client = OpenAI()

def encode_image(path: str) -> str:
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

def describe_image(image_path: str, style: str = "detailed") -> str:
    """ç”Ÿæˆå›¾åƒæè¿°"""
    image_base64 = encode_image(image_path)

    prompts = {
        "brief": "ç”¨ä¸€å¥è¯æè¿°è¿™å¼ å›¾ç‰‡ã€‚",
        "detailed": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å…ƒç´ ã€åœºæ™¯å’Œæ°›å›´ã€‚",
        "creative": "ç”¨å¯Œæœ‰æƒ³è±¡åŠ›çš„è¯­è¨€æè¿°è¿™å¼ å›¾ç‰‡ï¼Œåƒè®²æ•…äº‹ä¸€æ ·ã€‚",
        "technical": "ä»æŠ€æœ¯è§’åº¦åˆ†æè¿™å¼ å›¾ç‰‡çš„æ„å›¾ã€å…‰çº¿å’Œè‰²å½©ã€‚"
    }

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompts.get(style, prompts["detailed"])},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
# desc = describe_image("photo.jpg", style="detailed")
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†è§‰é—®ç­” ====================


def visual_qa():
    """è§†è§‰é—®ç­”"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§†è§‰é—®ç­” (VQA)")
    print("=" * 60)

    code = '''
def visual_question_answer(image_path: str, question: str) -> str:
    """è§†è§‰é—®ç­”"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# å¸¸è§é—®é¢˜ç±»å‹ç¤ºä¾‹
questions = [
    "å›¾ç‰‡ä¸­æœ‰å¤šå°‘äººï¼Ÿ",           # è®¡æ•°
    "å›¾ç‰‡ä¸­çš„äººåœ¨åšä»€ä¹ˆï¼Ÿ",       # åŠ¨ä½œè¯†åˆ«
    "è¿™æ˜¯åœ¨å“ªé‡Œæ‹æ‘„çš„ï¼Ÿ",         # åœºæ™¯ç†è§£
    "å›¾ç‰‡ä¸­æœ€çªå‡ºçš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ", # è§†è§‰å±æ€§
    "è¿™å¼ å›¾ç‰‡ç»™äººä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ",     # æƒ…æ„Ÿåˆ†æ
]
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šç‰©ä½“è¯†åˆ«ä¸åˆ†æ ====================


def object_analysis():
    """ç‰©ä½“è¯†åˆ«ä¸åˆ†æ"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šç‰©ä½“è¯†åˆ«ä¸åˆ†æ")
    print("=" * 60)

    code = '''
def identify_objects(image_path: str) -> dict:
    """è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": """åˆ†æå›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼Œè¿”å› JSON æ ¼å¼ï¼š
{
    "objects": [
        {"name": "ç‰©ä½“åç§°", "count": æ•°é‡, "position": "ä½ç½®æè¿°"}
    ],
    "main_subject": "ä¸»è¦ä¸»é¢˜",
    "scene": "åœºæ™¯ç±»å‹"
}"""
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ],
        max_tokens=800
    )

    import json
    return json.loads(response.choices[0].message.content)

def analyze_spatial_relations(image_path: str) -> str:
    """åˆ†æç©ºé—´å…³ç³»"""
    image_base64 = encode_image(image_path)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "æè¿°å›¾ç‰‡ä¸­å„å…ƒç´ ä¹‹é—´çš„ç©ºé—´ä½ç½®å…³ç³»ï¼ˆå¦‚ï¼šä¸Šä¸‹ã€å·¦å³ã€å‰åç­‰ï¼‰ã€‚"
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                    }
                ]
            }
        ]
    )

    return response.choices[0].message.content
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šå¤šå›¾åˆ†æ ====================


def multi_image_analysis():
    """å¤šå›¾åˆ†æ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå¤šå›¾åˆ†æ")
    print("=" * 60)

    code = '''
def compare_images(images: list, aspect: str = "general") -> str:
    """æ¯”è¾ƒå¤šå¼ å›¾ç‰‡"""
    content = []

    prompts = {
        "general": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„å¼‚åŒç‚¹ã€‚",
        "style": "æ¯”è¾ƒè¿™äº›å›¾ç‰‡çš„é£æ ¼å·®å¼‚ã€‚",
        "content": "æè¿°è¿™äº›å›¾ç‰‡å†…å®¹ä¸Šçš„å…³è”å’ŒåŒºåˆ«ã€‚",
        "timeline": "å¦‚æœè¿™äº›å›¾ç‰‡æ˜¯æŒ‰æ—¶é—´é¡ºåºçš„ï¼Œæè¿°å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ã€‚"
    }

    content.append({"type": "text", "text": prompts.get(aspect, prompts["general"])})

    for i, img_path in enumerate(images):
        image_base64 = encode_image(img_path)
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
        })

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=1000
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹ï¼šå¯¹æ¯”äº§å“ç…§ç‰‡
# result = compare_images(["product_v1.jpg", "product_v2.jpg"], aspect="content")
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªå›¾ç‰‡å†…å®¹å®¡æ ¸å‡½æ•°

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import google.generativeai as genai
        from PIL import Image
        from typing import Dict
        
        class ContentModerator:
            '''å›¾ç‰‡å†…å®¹å®¡æ ¸å™¨'''
            
            def __init__(self, api_key: str):
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
            
            def moderate(self, image_path: str) -> Dict:
                '''å®¡æ ¸å›¾ç‰‡å†…å®¹'''
                img = Image.open(image_path)
                
                prompt = '''å®¡æ ¸è¿™å¼ å›¾ç‰‡çš„å†…å®¹å®‰å…¨æ€§ã€‚
                
åˆ†æä»¥ä¸‹ç»´åº¦å¹¶è¿”å› JSONï¼š
{
    "safe": true/false,
    "categories": {
        "violence": {"detected": bool, "severity": "none/mild/moderate/severe"},
        "adult": {"detected": bool, "severity": "none/mild/moderate/severe"},
        "hate_speech": {"detected": bool, "severity": "none/mild/moderate/severe"},
        "dangerous": {"detected": bool, "severity": "none/mild/moderate/severe"}
    },
    "description": "å›¾ç‰‡å†…å®¹ç®€è¿°",
    "recommendation": "pass/review/reject",
    "reason": "å®¡æ ¸ç†ç”±"
}'''
                
                response = self.model.generate_content([prompt, img])
                
                import json
                return json.loads(response.text)
            
            def batch_moderate(self, image_paths: list) -> list:
                '''æ‰¹é‡å®¡æ ¸'''
                results = []
                for path in image_paths:
                    try:
                        result = self.moderate(path)
                        result['path'] = path
                        results.append(result)
                    except Exception as e:
                        results.append({
                            'path': path,
                            'error': str(e),
                            'recommendation': 'review'
                        })
                return results
        
        # ä½¿ç”¨ç¤ºä¾‹
        # moderator = ContentModerator(os.getenv("GOOGLE_API_KEY"))
        # result = moderator.moderate("uploaded_image.jpg")
        # if result['recommendation'] == 'reject':
        #     print("å›¾ç‰‡è¢«æ‹’ç»:", result['reason'])
        ```
    
    ç»ƒä¹  2ï¼šå®ç°å•†å“å›¾ç‰‡è‡ªåŠ¨åˆ†ç±»

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        class ProductClassifier:
            '''å•†å“å›¾ç‰‡åˆ†ç±»å™¨'''
            
            def __init__(self, api_key: str, categories: list = None):
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                self.categories = categories or [
                    "æœè£…é‹å¸½", "æ•°ç ç”µå™¨", "å®¶å±…å®¶è£…", 
                    "é£Ÿå“é¥®æ–™", "ç¾å¦†ä¸ªæŠ¤", "è¿åŠ¨æˆ·å¤–",
                    "æ¯å©´ç”¨å“", "å›¾ä¹¦æ–‡å…·", "å…¶ä»–"
                ]
            
            def classify(self, image_path: str) -> Dict:
                '''åˆ†ç±»å•†å“å›¾ç‰‡'''
                img = Image.open(image_path)
                
                categories_str = ", ".join(self.categories)
                prompt = f'''åˆ†æè¿™å¼ å•†å“å›¾ç‰‡ï¼Œè¿”å› JSONï¼š

{{
    "category": "ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©: {categories_str}",
    "subcategory": "æ›´å…·ä½“çš„å­ç±»åˆ«",
    "product_name": "äº§å“åç§°çŒœæµ‹",
    "attributes": {{
        "color": "é¢œè‰²",
        "brand": "å“ç‰Œï¼ˆå¦‚å¯è¯†åˆ«ï¼‰",
        "style": "é£æ ¼ç‰¹ç‚¹"
    }},
    "confidence": 0.0-1.0,
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "..."]
}}'''
                
                response = self.model.generate_content([prompt, img])
                
                import json
                return json.loads(response.text)
            
            def classify_batch(self, image_paths: list) -> Dict:
                '''æ‰¹é‡åˆ†ç±»å¹¶ç»Ÿè®¡'''
                results = []
                category_counts = {}
                
                for path in image_paths:
                    result = self.classify(path)
                    result['path'] = path
                    results.append(result)
                    
                    cat = result.get('category', 'å…¶ä»–')
                    category_counts[cat] = category_counts.get(cat, 0) + 1
                
                return {
                    'results': results,
                    'statistics': category_counts
                }
        
        # ä½¿ç”¨ç¤ºä¾‹
        # classifier = ProductClassifier(os.getenv("GOOGLE_API_KEY"))
        # result = classifier.classify("product.jpg")
        # print(f"ç±»åˆ«: {result['category']}, ç½®ä¿¡åº¦: {result['confidence']}")
        ```

    æ€è€ƒé¢˜ï¼šå¤šæ¨¡æ€ LLM åœ¨å›¾åƒç†è§£ä¸Šæœ‰ä»€ä¹ˆå±€é™ï¼Ÿ

        âœ… ç­”ï¼š
        1. ç²¾ç¡®è®¡æ•°å›°éš¾ - äººç¾¤ã€å¯†é›†ç‰©ä½“å®¹æ˜“å‡ºé”™
        2. ç©ºé—´å®šä½ä¸å‡† - æ— æ³•è¾“å‡ºç²¾ç¡®çš„è¾¹ç•Œæ¡†åæ ‡
        3. å°ç‰©ä½“è¯†åˆ«æœ‰é™ - è¿œå¤„æˆ–å¾®å°ç‰©ä½“å®¹æ˜“é—æ¼
        4. è§†è§‰å¹»è§‰ - å¯èƒ½"çœ‹åˆ°"ä¸å­˜åœ¨çš„å†…å®¹
        5. ç»†ç²’åº¦åŒºåˆ†å›°éš¾ - ç›¸ä¼¼ç‰©ä½“ï¼ˆå¦‚ä¸åŒå“ç§çš„ç‹—ï¼‰
        6. æ•°å­—/æ–‡å­—è¯¯è¯» - å¤æ‚å­—ä½“æˆ–æ‰‹å†™å®¹æ˜“å‡ºé”™
    """)


def main():
    task_types()
    image_captioning()
    visual_qa()
    object_analysis()
    multi_image_analysis()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š04-document-ocr.py")


if __name__ == "__main__":
    main()
