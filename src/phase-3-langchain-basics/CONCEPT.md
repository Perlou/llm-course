# LangChain æ·±å…¥è§£ææŒ‡å—

## ğŸ“– ç›®å½•

1. [ä»€ä¹ˆæ˜¯LangChain](#1-ä»€ä¹ˆæ˜¯langchain)
2. [æ ¸å¿ƒæ¶æ„](#2-æ ¸å¿ƒæ¶æ„)
3. [å®‰è£…ä¸é…ç½®](#3-å®‰è£…ä¸é…ç½®)
4. [å…­å¤§æ ¸å¿ƒç»„ä»¶](#4-å…­å¤§æ ¸å¿ƒç»„ä»¶)
5. [LCELè¡¨è¾¾å¼è¯­è¨€](#5-lcelè¡¨è¾¾å¼è¯­è¨€)
6. [å®æˆ˜ç¤ºä¾‹](#6-å®æˆ˜ç¤ºä¾‹)
7. [é«˜çº§åº”ç”¨](#7-é«˜çº§åº”ç”¨)
8. [æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)

---

## 1. ä»€ä¹ˆæ˜¯LangChain

### 1.1 å®šä¹‰

LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘**å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨åº”ç”¨ç¨‹åº**çš„å¼€æºæ¡†æ¶ï¼Œç”± Harrison Chase äº 2022 å¹´åˆ›å»ºã€‚

### 1.2 æ ¸å¿ƒä»·å€¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LangChain è§£å†³çš„é—®é¢˜                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âŒ åŸå§‹LLMçš„å±€é™æ€§          â”‚   âœ… LangChainçš„è§£å†³æ–¹æ¡ˆ           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ æ— æ³•è®¿é—®å®æ—¶æ•°æ®           â”‚   â€¢ é›†æˆå¤–éƒ¨æ•°æ®æºå’ŒAPI            â”‚
â”‚  â€¢ æ— è®°å¿†èƒ½åŠ›                â”‚   â€¢ æä¾›Memoryç»„ä»¶                â”‚
â”‚  â€¢ æ— æ³•æ‰§è¡Œå¤æ‚ä»»åŠ¡           â”‚   â€¢ Chainé“¾å¼è°ƒç”¨                 â”‚
â”‚  â€¢ æ— æ³•ä½¿ç”¨å¤–éƒ¨å·¥å…·           â”‚   â€¢ Agentæ™ºèƒ½ä»£ç†                 â”‚
â”‚  â€¢ ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶             â”‚   â€¢ æ–‡æ¡£åˆ†å‰²ä¸å‘é‡å­˜å‚¨             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ç”Ÿæ€ç³»ç»Ÿ

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   LangChain ç”Ÿæ€ç³»ç»Ÿ   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ langchain   â”‚       â”‚ langchain-  â”‚       â”‚ langsmith   â”‚
â”‚   æ ¸å¿ƒåº“     â”‚       â”‚ community   â”‚       â”‚  è°ƒè¯•è¿½è¸ª    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚                      â”‚
       â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚langchain-   â”‚       â”‚ ç¬¬ä¸‰æ–¹é›†æˆ   â”‚       â”‚ langgraph   â”‚
â”‚  core       â”‚       â”‚ æ’ä»¶æ‰©å±•     â”‚       â”‚ å¤æ‚å·¥ä½œæµ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. æ ¸å¿ƒæ¶æ„

### 2.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          åº”ç”¨å±‚ (Application)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  èŠå¤©æœºå™¨äºº â”‚  â”‚  é—®ç­”ç³»ç»Ÿ  â”‚  â”‚  ä»£ç åŠ©æ‰‹  â”‚  â”‚  æ•°æ®åˆ†æ  â”‚  â”‚  æ–‡æ¡£æ‘˜è¦  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ¡†æ¶å±‚ (LangChain)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    LCEL (LangChain Expression Language)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Models â”‚ â”‚Prompts â”‚ â”‚ Chains â”‚ â”‚ Agents â”‚ â”‚ Memory â”‚ â”‚Retrievâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        é›†æˆå±‚ (Integrations)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ OpenAI  â”‚  â”‚ Pineconeâ”‚  â”‚ Redis   â”‚  â”‚ Google  â”‚  â”‚ æ›´å¤š...  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®æµå‘

```
ç”¨æˆ·è¾“å…¥
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt     â”‚ â† æ¨¡æ¿æ ¼å¼åŒ–
â”‚  Template   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Memory    â”‚ â†â”€â”€ â”‚  å†å²å¯¹è¯    â”‚
â”‚   æ³¨å…¥ä¸Šä¸‹æ–‡ â”‚     â”‚  æ£€ç´¢ç»“æœ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LLM      â”‚ â† å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨
â”‚   Model     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output    â”‚ â† è¾“å‡ºè§£æ
â”‚   Parser    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
ç»“æ„åŒ–è¾“å‡º
```

---

## 3. å®‰è£…ä¸é…ç½®

### 3.1 å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install langchain

# å®Œæ•´å®‰è£… (åŒ…å«å¸¸ç”¨ä¾èµ–)
pip install langchain langchain-openai langchain-community

# ç‰¹å®šé›†æˆ
pip install langchain-anthropic  # Anthropic Claude
pip install langchain-google-genai  # Google Gemini
pip install chromadb  # å‘é‡æ•°æ®åº“
pip install faiss-cpu  # Facebookå‘é‡æœç´¢
```

### 3.2 ç¯å¢ƒé…ç½®

```python
import os
from dotenv import load_dotenv

# æ–¹å¼1: ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "sk-your-key-here"
os.environ["ANTHROPIC_API_KEY"] = "your-key-here"

# æ–¹å¼2: ä½¿ç”¨.envæ–‡ä»¶
load_dotenv()

# .env æ–‡ä»¶å†…å®¹:
# OPENAI_API_KEY=sk-xxxxx
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=your-langsmith-key
```

---

## 4. å…­å¤§æ ¸å¿ƒç»„ä»¶

### 4.1 Models (æ¨¡å‹)

#### æ¨¡å‹ç±»å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LangChain æ¨¡å‹ç±»å‹                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚    LLMs     â”‚  â”‚ Chat Models â”‚  â”‚ Text Embedding  â”‚   â”‚
â”‚   â”‚   çº¯æ–‡æœ¬æ¨¡å‹  â”‚  â”‚   å¯¹è¯æ¨¡å‹   â”‚  â”‚    åµŒå…¥æ¨¡å‹      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                â”‚                  â”‚              â”‚
â”‚         â–¼                â–¼                  â–¼              â”‚
â”‚   "æ–‡æœ¬è¾“å…¥â†’æ–‡æœ¬è¾“å‡º" "æ¶ˆæ¯åˆ—è¡¨â†’AIæ¶ˆæ¯" "æ–‡æœ¬â†’å‘é‡è¡¨ç¤º"       â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»£ç ç¤ºä¾‹

```python
# ==================== LLMs ====================
from langchain_openai import OpenAI

llm = OpenAI(
    model="gpt-3.5-turbo-instruct",
    temperature=0.7,
    max_tokens=256
)

response = llm.invoke("è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")
print(response)


# ==================== Chat Models ====================
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

chat = ChatOpenAI(
    model="gpt-4",
    temperature=0.7
)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¼€å‘è€…"),
    HumanMessage(content="å¦‚ä½•å®ç°å•ä¾‹æ¨¡å¼?")
]

response = chat.invoke(messages)
print(response.content)


# ==================== Embeddings ====================
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# å•ä¸ªæ–‡æœ¬åµŒå…¥
vector = embeddings.embed_query("LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶")
print(f"å‘é‡ç»´åº¦: {len(vector)}")

# æ‰¹é‡åµŒå…¥
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
vectors = embeddings.embed_documents(texts)
```

### 4.2 Prompts (æç¤ºè¯)

#### Prompt ç»„ä»¶ç»“æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Prompt ç»„ä»¶                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  PromptTemplate          ç”¨äºçº¯æ–‡æœ¬LLMçš„ç®€å•æ¨¡æ¿              â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€â”€ ChatPromptTemplate     ç”¨äºChatæ¨¡å‹çš„æ¶ˆæ¯æ¨¡æ¿       â”‚
â”‚       â”‚       â”‚                                             â”‚
â”‚       â”‚       â”œâ”€â”€ SystemMessagePromptTemplate               â”‚
â”‚       â”‚       â”œâ”€â”€ HumanMessagePromptTemplate                â”‚
â”‚       â”‚       â””â”€â”€ AIMessagePromptTemplate                   â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€â”€ FewShotPromptTemplate  å°‘æ ·æœ¬å­¦ä¹ æ¨¡æ¿               â”‚
â”‚       â”‚                                                     â”‚
â”‚       â””â”€â”€ PipelinePromptTemplate ç»„åˆå¤šä¸ªæ¨¡æ¿                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»£ç ç¤ºä¾‹

```python
from langchain_core.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
    FewShotPromptTemplate,
    MessagesPlaceholder
)

# ==================== åŸºç¡€ PromptTemplate ====================
template = PromptTemplate.from_template(
    "è¯·å°†ä»¥ä¸‹{source_language}æ–‡æœ¬ç¿»è¯‘æˆ{target_language}:\n\n{text}"
)

prompt = template.format(
    source_language="ä¸­æ–‡",
    target_language="è‹±æ–‡",
    text="ä»Šå¤©å¤©æ°”çœŸå¥½"
)
print(prompt)


# ==================== ChatPromptTemplate ====================
chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role},æ“…é•¿{skill}"),
    ("human", "{user_input}"),
])

messages = chat_template.format_messages(
    role="èµ„æ·±ç¨‹åºå‘˜",
    skill="ä»£ç å®¡æŸ¥",
    user_input="è¯·å¸®æˆ‘æ£€æŸ¥è¿™æ®µä»£ç çš„é—®é¢˜"
)


# ==================== å¸¦å†å²è®°å½•çš„æ¨¡æ¿ ====================
chat_with_history = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="history"),  # åŠ¨æ€æ’å…¥å†å²æ¶ˆæ¯
    ("human", "{input}")
])


# ==================== FewShotPromptTemplate ====================
examples = [
    {"input": "å¼€å¿ƒ", "output": "ğŸ˜Š"},
    {"input": "æ‚²ä¼¤", "output": "ğŸ˜¢"},
    {"input": "æ„¤æ€’", "output": "ğŸ˜ "},
]

example_template = PromptTemplate.from_template(
    "è¾“å…¥: {input}\nè¾“å‡º: {output}"
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_template,
    prefix="å°†æƒ…æ„Ÿè¯è½¬æ¢ä¸ºå¯¹åº”çš„emojiè¡¨æƒ…:",
    suffix="è¾“å…¥: {input}\nè¾“å‡º:",
    input_variables=["input"]
)

print(few_shot_prompt.format(input="æƒŠè®¶"))
```

### 4.3 Chains (é“¾)

#### Chain å·¥ä½œåŸç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Chain å·¥ä½œæµç¨‹                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç®€å•é“¾ (Simple Chain):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input  â”‚ â”€â–¶ â”‚  LLM    â”‚ â”€â–¶ â”‚ Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é¡ºåºé“¾ (Sequential Chain):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input  â”‚ â”€â–¶ â”‚ Chain 1 â”‚ â”€â–¶ â”‚ Chain 2 â”‚ â”€â–¶ â”‚ Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è·¯ç”±é“¾ (Router Chain):
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”Œâ”€â”€â–¶ â”‚ Chain A â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  Input  â”‚ â”€â–¶ â”‚Routerâ”‚â”€â”€â”¤    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜   â”œâ”€â”€â–¶ â”‚ Chain B â”‚
                         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â””â”€â”€â–¶ â”‚ Chain C â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»£ç ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

llm = ChatOpenAI(model="gpt-4")

# ==================== åŸºç¡€é“¾ (LCELæ–¹å¼) ====================
prompt = ChatPromptTemplate.from_template("ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
chain = prompt | llm | StrOutputParser()

result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(result)


# ==================== å¤šæ­¥éª¤é“¾ ====================
# ç¬¬ä¸€æ­¥: ç”Ÿæˆæ•…äº‹å¤§çº²
outline_prompt = ChatPromptTemplate.from_template(
    "ä¸ºä¸€ä¸ª{genre}ç±»å‹çš„æ•…äº‹ç”Ÿæˆä¸‰ç‚¹å¤§çº²,ä¸»é¢˜æ˜¯: {theme}"
)

# ç¬¬äºŒæ­¥: åŸºäºå¤§çº²å†™æ•…äº‹
story_prompt = ChatPromptTemplate.from_template(
    "åŸºäºä»¥ä¸‹å¤§çº²,å†™ä¸€ä¸ª300å­—çš„çŸ­æ•…äº‹:\n\n{outline}"
)

# ç¬¬ä¸‰æ­¥: ç”Ÿæˆæ ‡é¢˜
title_prompt = ChatPromptTemplate.from_template(
    "ä¸ºä»¥ä¸‹æ•…äº‹èµ·ä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜:\n\n{story}"
)

# ç»„åˆé“¾
full_chain = (
    {"genre": RunnablePassthrough(), "theme": RunnablePassthrough()}
    | outline_prompt
    | llm
    | StrOutputParser()
    | {"outline": RunnablePassthrough()}
    | story_prompt
    | llm
    | StrOutputParser()
    | {"story": RunnablePassthrough()}
    | title_prompt
    | llm
    | StrOutputParser()
)


# ==================== å¹¶è¡Œé“¾ ====================
from langchain_core.runnables import RunnableParallel

analysis_chain = RunnableParallel(
    summary=ChatPromptTemplate.from_template("æ€»ç»“è¿™æ®µæ–‡æœ¬: {text}") | llm | StrOutputParser(),
    sentiment=ChatPromptTemplate.from_template("åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿ: {text}") | llm | StrOutputParser(),
    keywords=ChatPromptTemplate.from_template("æå–è¿™æ®µæ–‡æœ¬çš„å…³é”®è¯: {text}") | llm | StrOutputParser(),
)

result = analysis_chain.invoke({"text": "LangChainæ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ¡†æ¶..."})
# result = {"summary": "...", "sentiment": "...", "keywords": "..."}
```

### 4.4 Memory (è®°å¿†)

#### Memory ç±»å‹å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Memory ç±»å‹å¯¹æ¯”                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       ç±»å‹            â”‚                   ç‰¹ç‚¹                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ConversationBuffer   â”‚ å­˜å‚¨å®Œæ•´å¯¹è¯å†å²,ç®€å•ä½†æ¶ˆè€—token               â”‚
â”‚ Memory               â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ConversationBuffer   â”‚ åªä¿ç•™æœ€è¿‘kè½®å¯¹è¯,æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦               â”‚
â”‚ WindowMemory         â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ConversationSummary  â”‚ ç”¨LLMæ€»ç»“å†å²å¯¹è¯,èŠ‚çœtokenä½†æœ‰ä¿¡æ¯æŸå¤±        â”‚
â”‚ Memory               â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ConversationToken    â”‚ æŒ‰tokenæ•°é‡é™åˆ¶,ç²¾ç¡®æ§åˆ¶æˆæœ¬                  â”‚
â”‚ BufferMemory         â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VectorStoreRetriever â”‚ ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³å†å²,é€‚åˆé•¿å¯¹è¯          â”‚
â”‚ Memory               â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ConversationKG       â”‚ æ„å»ºçŸ¥è¯†å›¾è°±,æå–å®ä½“å…³ç³»                     â”‚
â”‚ Memory               â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»£ç ç¤ºä¾‹

```python
from langchain.memory import (
    ConversationBufferMemory,
    ConversationBufferWindowMemory,
    ConversationSummaryMemory,
    ConversationSummaryBufferMemory
)
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model="gpt-4")

# ==================== åŸºç¡€ Buffer Memory ====================
memory = ConversationBufferMemory(return_messages=True)
memory.save_context(
    {"input": "ä½ å¥½,æˆ‘å«å¼ ä¸‰"},
    {"output": "ä½ å¥½å¼ ä¸‰!å¾ˆé«˜å…´è®¤è¯†ä½ !"}
)
memory.save_context(
    {"input": "æˆ‘å–œæ¬¢ç¼–ç¨‹"},
    {"output": "ç¼–ç¨‹æ˜¯ä¸€é¡¹å¾ˆæ£’çš„æŠ€èƒ½!ä½ ä¸»è¦ä½¿ç”¨ä»€ä¹ˆç¼–ç¨‹è¯­è¨€?"}
)

print(memory.load_memory_variables({}))


# ==================== Window Memory ====================
window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)
# åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯


# ==================== Summary Memory ====================
summary_memory = ConversationSummaryMemory(llm=llm)
# è‡ªåŠ¨ä½¿ç”¨LLMæ€»ç»“å¯¹è¯


# ==================== åœ¨Chainä¸­ä½¿ç”¨Memory ====================
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# å­˜å‚¨ä¼šè¯å†å²çš„å­—å…¸
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = prompt | llm

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# ä½¿ç”¨
response1 = chain_with_history.invoke(
    {"input": "æˆ‘å«ææ˜"},
    config={"configurable": {"session_id": "user123"}}
)

response2 = chain_with_history.invoke(
    {"input": "æˆ‘å«ä»€ä¹ˆåå­—?"},
    config={"configurable": {"session_id": "user123"}}
)
# AIä¼šè®°ä½ç”¨æˆ·å«ææ˜
```

### 4.5 Indexes & Retrievers (ç´¢å¼•ä¸æ£€ç´¢)

#### RAG æ¶æ„æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®Œæ•´æµç¨‹                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

               ç´¢å¼•é˜¶æ®µ (Indexing)
               â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŸå§‹æ–‡æ¡£  â”‚ â”€â–¶ â”‚  æ–‡æ¡£åŠ è½½  â”‚ â”€â–¶ â”‚  æ–‡æ¡£åˆ†å‰²  â”‚ â”€â–¶ â”‚  å‘é‡åµŒå…¥  â”‚
â”‚ PDF/TXT/..â”‚    â”‚  Loaders  â”‚    â”‚ Splitters â”‚    â”‚ Embeddingsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                   â”‚ å‘é‡æ•°æ®åº“  â”‚
                                                   â”‚VectorStoreâ”‚
                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
               æ£€ç´¢é˜¶æ®µ (Retrieval)                        â”‚
               â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  ç”¨æˆ·é—®é¢˜  â”‚ â”€â–¶ â”‚  é—®é¢˜åµŒå…¥  â”‚ â”€â–¶ â”‚  ç›¸ä¼¼æœç´¢  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
               ç”Ÿæˆé˜¶æ®µ (Generation)   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•      â”‚  ç›¸å…³æ–‡æ¡£  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æœ€ç»ˆå›ç­”  â”‚ â—€â”€ â”‚    LLM    â”‚ â—€â”€ â”‚ Prompt + Context  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»£ç ç¤ºä¾‹

```python
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    DirectoryLoader,
    WebBaseLoader
)
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter
)
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma, FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ==================== 1. æ–‡æ¡£åŠ è½½ ====================
# åŠ è½½æ–‡æœ¬æ–‡ä»¶
loader = TextLoader("./data/document.txt", encoding="utf-8")
documents = loader.load()

# åŠ è½½PDF
pdf_loader = PyPDFLoader("./data/report.pdf")
pdf_docs = pdf_loader.load()

# åŠ è½½ç½‘é¡µ
web_loader = WebBaseLoader("https://example.com/article")
web_docs = web_loader.load()

# æ‰¹é‡åŠ è½½ç›®å½•
dir_loader = DirectoryLoader("./data/", glob="**/*.txt")
all_docs = dir_loader.load()


# ==================== 2. æ–‡æ¡£åˆ†å‰² ====================
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,           # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,         # å—ä¹‹é—´çš„é‡å 
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "!", "?", ",", " ", ""]
)

chunks = splitter.split_documents(documents)
print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")


# ==================== 3. åˆ›å»ºå‘é‡å­˜å‚¨ ====================
embeddings = OpenAIEmbeddings()

# ä½¿ç”¨ Chroma
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æŒä¹…åŒ–å­˜å‚¨
)

# æˆ–ä½¿ç”¨ FAISS
faiss_store = FAISS.from_documents(chunks, embeddings)
faiss_store.save_local("./faiss_index")  # ä¿å­˜ç´¢å¼•

# åŠ è½½å·²æœ‰ç´¢å¼•
loaded_store = FAISS.load_local("./faiss_index", embeddings)


# ==================== 4. åˆ›å»ºæ£€ç´¢å™¨ ====================
retriever = vectorstore.as_retriever(
    search_type="similarity",      # ç›¸ä¼¼åº¦æœç´¢
    search_kwargs={"k": 4}         # è¿”å›top 4ç»“æœ
)

# æˆ–è€…ä½¿ç”¨ MMR (æœ€å¤§è¾¹é™…ç›¸å…³æ€§,å¢åŠ å¤šæ ·æ€§)
mmr_retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 10}
)


# ==================== 5. æ„å»º RAG Chain ====================
llm = ChatOpenAI(model="gpt-4")

template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆ,è¯·è¯´"æˆ‘æ— æ³•æ ¹æ®æä¾›çš„ä¿¡æ¯å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

ç­”æ¡ˆ:"""

prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ä½¿ç”¨
answer = rag_chain.invoke("LangChainçš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆ?")
print(answer)
```

### 4.6 Agents (ä»£ç†)

#### Agent å·¥ä½œåŸç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Agent å†³ç­–å¾ªç¯                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    ç”¨æˆ·è¾“å…¥      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      æ€è€ƒ (Thought)       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  åˆ†æé—®é¢˜,å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨    â”‚               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                           â”‚                              â”‚
                           â–¼                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
              â”‚      è¡ŒåŠ¨ (Action)        â”‚               â”‚
              â”‚  é€‰æ‹©å¹¶æ‰§è¡ŒæŸä¸ªå·¥å…·         â”‚               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                           â”‚                              â”‚
                           â–¼                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
              â”‚      è§‚å¯Ÿ (Observation)   â”‚               â”‚
              â”‚  è·å–å·¥å…·æ‰§è¡Œç»“æœ          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     ç»§ç»­å¾ªç¯
                           â”‚
                           â”‚ ä»»åŠ¡å®Œæˆ
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      æœ€ç»ˆç­”æ¡ˆ             â”‚
              â”‚  ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç»™å‡ºå›ç­”      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


å¯ç”¨å·¥å…·ç¤ºä¾‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æœç´¢å¼•æ“  â”‚ â”‚  è®¡ç®—å™¨   â”‚ â”‚  æ•°æ®åº“   â”‚ â”‚ ä»£ç æ‰§è¡Œ  â”‚ â”‚  APIè°ƒç”¨  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»£ç ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.tools import Tool, tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools import DuckDuckGoSearchRun
import requests
import json

llm = ChatOpenAI(model="gpt-4", temperature=0)

# ==================== å®šä¹‰å·¥å…· ====================

# æ–¹å¼1: ä½¿ç”¨è£…é¥°å™¨
@tool
def calculate(expression: str) -> str:
    """æ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­¦è¡¨è¾¾å¼ã€‚"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"

@tool
def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯ã€‚"""
    # æ¨¡æ‹Ÿå¤©æ°”APIè°ƒç”¨
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©, 25Â°C",
        "ä¸Šæµ·": "å¤šäº‘, 28Â°C",
        "å¹¿å·": "å°é›¨, 30Â°C"
    }
    return weather_data.get(city, f"æœªæ‰¾åˆ°{city}çš„å¤©æ°”ä¿¡æ¯")

@tool
def search_knowledge(query: str) -> str:
    """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ã€‚"""
    # æ¨¡æ‹ŸçŸ¥è¯†åº“æœç´¢
    return f"å…³äº'{query}'çš„æœç´¢ç»“æœ: è¿™æ˜¯ä¸€äº›ç›¸å…³ä¿¡æ¯..."


# æ–¹å¼2: ä½¿ç”¨ Tool ç±»
def get_current_time():
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

time_tool = Tool(
    name="get_current_time",
    description="è·å–å½“å‰æ—¶é—´,æ— éœ€è¾“å…¥å‚æ•°",
    func=lambda x: get_current_time()
)


# ==================== åˆ›å»º Agent ====================
tools = [calculate, get_weather, search_knowledge, time_tool]

prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹,å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å®Œæˆä»»åŠ¡:
    - calculate: æ‰§è¡Œæ•°å­¦è®¡ç®—
    - get_weather: æŸ¥è¯¢å¤©æ°”
    - search_knowledge: æœç´¢çŸ¥è¯†åº“
    - get_current_time: è·å–å½“å‰æ—¶é—´

    è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜,é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥è·å–ä¿¡æ¯ã€‚"""),
    MessagesPlaceholder(variable_name="chat_history", optional=True),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

agent = create_openai_tools_agent(llm, tools, prompt)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,  # æ˜¾ç¤ºè¯¦ç»†æ‰§è¡Œè¿‡ç¨‹
    max_iterations=5,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    handle_parsing_errors=True  # å¤„ç†è§£æé”™è¯¯
)

# ä½¿ç”¨
response = agent_executor.invoke({
    "input": "åŒ—äº¬ç°åœ¨å‡ ç‚¹äº†?å¤©æ°”æ€ä¹ˆæ ·?å¦‚æœæ¸©åº¦æ˜¯25åº¦,è½¬æ¢æˆåæ°åº¦æ˜¯å¤šå°‘?"
})
print(response["output"])


# ==================== å¸¦è®°å¿†çš„ Agent ====================
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

agent_with_memory = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True
)
```

---

## 5. LCELè¡¨è¾¾å¼è¯­è¨€

### 5.1 LCEL æ¦‚è¿°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             LCEL (LangChain Expression Language)                     â”‚
â”‚                    LangChain è¡¨è¾¾å¼è¯­è¨€                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ ¸å¿ƒæ€æƒ³: ä½¿ç”¨ | (ç®¡é“ç¬¦) å°†ç»„ä»¶ä¸²è”èµ·æ¥

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt  â”‚ â”€â”€â–¶ â”‚   LLM   â”‚ â”€â”€â–¶ â”‚ Parser  â”‚ â”€â”€â–¶ â”‚ Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         é“¾å¼è°ƒç”¨
               prompt | llm | parser
```

### 5.2 LCEL æ ¸å¿ƒç»„ä»¶

```python
from langchain_core.runnables import (
    RunnablePassthrough,    # é€ä¼ è¾“å…¥
    RunnableLambda,         # è‡ªå®šä¹‰å‡½æ•°
    RunnableParallel,       # å¹¶è¡Œæ‰§è¡Œ
    RunnableBranch,         # æ¡ä»¶åˆ†æ”¯
    RunnableSequence        # é¡ºåºæ‰§è¡Œ
)

# ==================== 1. åŸºç¡€é“¾ ====================
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
llm = ChatOpenAI()
chain = prompt | llm | StrOutputParser()


# ==================== 2. RunnablePassthrough ====================
# ä¼ é€’è¾“å…¥,åŒæ—¶æ·»åŠ æ–°çš„é”®å€¼å¯¹
chain = (
    {"topic": RunnablePassthrough(), "language": lambda x: "ä¸­æ–‡"}
    | prompt
    | llm
)


# ==================== 3. RunnableLambda ====================
# ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
def transform_input(text: str) -> str:
    return text.upper()

chain = (
    RunnableLambda(transform_input)
    | prompt
    | llm
)


# ==================== 4. RunnableParallel ====================
# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
from langchain_core.runnables import RunnableParallel

parallel_chain = RunnableParallel(
    joke=ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯") | llm,
    poem=ChatPromptTemplate.from_template("å†™ä¸€é¦–å…³äº{topic}çš„è¯—") | llm,
    fact=ChatPromptTemplate.from_template("å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„äº‹å®") | llm,
)

result = parallel_chain.invoke({"topic": "æœˆäº®"})
# result = {"joke": "...", "poem": "...", "fact": "..."}


# ==================== 5. RunnableBranch ====================
# æ¡ä»¶åˆ†æ”¯
from langchain_core.runnables import RunnableBranch

def classify_input(text):
    if "ä»£ç " in text or "ç¼–ç¨‹" in text:
        return "technical"
    elif "æƒ…æ„Ÿ" in text or "æ„Ÿè§‰" in text:
        return "emotional"
    else:
        return "general"

branch_chain = RunnableBranch(
    (lambda x: classify_input(x["input"]) == "technical",
     ChatPromptTemplate.from_template("ä½œä¸ºæŠ€æœ¯ä¸“å®¶å›ç­”: {input}") | llm),
    (lambda x: classify_input(x["input"]) == "emotional",
     ChatPromptTemplate.from_template("ä½œä¸ºå¿ƒç†å’¨è¯¢å¸ˆå›ç­”: {input}") | llm),
    # é»˜è®¤åˆ†æ”¯
    ChatPromptTemplate.from_template("ä½œä¸ºé€šç”¨åŠ©æ‰‹å›ç­”: {input}") | llm
)
```

### 5.3 LCEL é«˜çº§åŠŸèƒ½

```python
# ==================== æµå¼è¾“å‡º ====================
chain = prompt | llm | StrOutputParser()

# æµå¼è¾“å‡º
for chunk in chain.stream({"topic": "äººå·¥æ™ºèƒ½"}):
    print(chunk, end="", flush=True)


# ==================== å¼‚æ­¥è°ƒç”¨ ====================
import asyncio

async def async_example():
    result = await chain.ainvoke({"topic": "äººå·¥æ™ºèƒ½"})
    return result

asyncio.run(async_example())


# ==================== æ‰¹é‡è°ƒç”¨ ====================
inputs = [
    {"topic": "çŒ«"},
    {"topic": "ç‹—"},
    {"topic": "é¸Ÿ"}
]
results = chain.batch(inputs, config={"max_concurrency": 3})


# ==================== ç»‘å®šå‚æ•° ====================
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
# ç»‘å®šå›ºå®šå‚æ•°
llm_with_stop = llm.bind(stop=["\n\n"])
llm_with_tools = llm.bind_tools([my_tool])


# ==================== é‡è¯•ä¸å›é€€ ====================
chain = prompt | llm.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
) | StrOutputParser()

# é…ç½®å›é€€æ¨¡å‹
fallback_llm = ChatOpenAI(model="gpt-3.5-turbo")
chain_with_fallback = (
    prompt
    | llm.with_fallbacks([fallback_llm])
    | StrOutputParser()
)


# ==================== é…ç½®ä¼ é€’ ====================
chain.invoke(
    {"topic": "AI"},
    config={
        "callbacks": [my_callback],
        "tags": ["production"],
        "metadata": {"user_id": "123"}
    }
)
```

---

## 6. å®æˆ˜ç¤ºä¾‹

### 6.1 å®Œæ•´çš„èŠå¤©æœºå™¨äºº

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# ä¼šè¯å­˜å‚¨
session_store = {}

def get_session_history(session_id: str):
    if session_id not in session_store:
        session_store[session_id] = ChatMessageHistory()
    return session_store[session_id]

# æ„å»º Chain
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹,åå«å°æ™ºã€‚
    - ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€å›ç­”é—®é¢˜
    - å¦‚æœä¸ç¡®å®š,å¦è¯šè¯´æ˜
    - é€‚å½“ä½¿ç”¨emojiè®©å¯¹è¯æ›´ç”ŸåŠ¨"""),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = prompt | llm | StrOutputParser()

chatbot = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# å¯¹è¯å‡½æ•°
def chat(user_input: str, session_id: str = "default"):
    response = chatbot.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": session_id}}
    )
    return response

# ä½¿ç”¨
print(chat("ä½ å¥½,æˆ‘æ˜¯å¼ ä¸‰"))
print(chat("æˆ‘å–œæ¬¢å­¦ä¹ ç¼–ç¨‹"))
print(chat("ä½ è¿˜è®°å¾—æˆ‘å«ä»€ä¹ˆåå­—å—?"))
```

### 6.2 æ–‡æ¡£é—®ç­”ç³»ç»Ÿ (RAG)

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

class DocumentQA:
    def __init__(self, persist_directory="./chroma_db"):
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.retriever = None
        self.chain = None

    def load_documents(self, file_path: str):
        """åŠ è½½å¹¶å¤„ç†æ–‡æ¡£"""
        # åŠ è½½
        loader = PyPDFLoader(file_path)
        documents = loader.load()

        # åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = splitter.split_documents(documents)

        # å­˜å‚¨
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )

        # åˆ›å»ºæ£€ç´¢å™¨
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )

        # æ„å»º Chain
        self._build_chain()

        return len(chunks)

    def _build_chain(self):
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·æ˜ç¡®è¯´æ˜"æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”:"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n---\n\n".join(
                f"[æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}, é¡µç : {doc.metadata.get('page', 'æœªçŸ¥')}]\n{doc.page_content}"
                for doc in docs
            )

        self.chain = (
            {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

    def query(self, question: str) -> str:
        """æŸ¥è¯¢é—®é¢˜"""
        if not self.chain:
            return "è¯·å…ˆåŠ è½½æ–‡æ¡£"
        return self.chain.invoke(question)

    def query_with_sources(self, question: str):
        """æŸ¥è¯¢é—®é¢˜å¹¶è¿”å›æ¥æº"""
        if not self.retriever:
            return {"answer": "è¯·å…ˆåŠ è½½æ–‡æ¡£", "sources": []}

        docs = self.retriever.invoke(question)
        answer = self.chain.invoke(question)

        sources = [
            {
                "content": doc.page_content[:200] + "...",
                "source": doc.metadata.get('source'),
                "page": doc.metadata.get('page')
            }
            for doc in docs
        ]

        return {"answer": answer, "sources": sources}


# ä½¿ç”¨ç¤ºä¾‹
qa_system = DocumentQA()
num_chunks = qa_system.load_documents("./documents/technical_manual.pdf")
print(f"æ–‡æ¡£å·²å¤„ç†,å…± {num_chunks} ä¸ªæ–‡æœ¬å—")

result = qa_system.query_with_sources("è¿™ä¸ªäº§å“çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆ?")
print(f"å›ç­”: {result['answer']}")
print(f"æ¥æº: {result['sources']}")
```

### 6.3 æ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import pandas as pd
import matplotlib.pyplot as plt
import io
import base64

# å…¨å±€æ•°æ®å­˜å‚¨
current_dataframe = None

@tool
def load_csv(file_path: str) -> str:
    """åŠ è½½CSVæ–‡ä»¶å¹¶æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯ã€‚è¾“å…¥æ–‡ä»¶è·¯å¾„ã€‚"""
    global current_dataframe
    try:
        current_dataframe = pd.read_csv(file_path)
        info = f"""
æ•°æ®åŠ è½½æˆåŠŸ!
- è¡Œæ•°: {len(current_dataframe)}
- åˆ—æ•°: {len(current_dataframe.columns)}
- åˆ—å: {list(current_dataframe.columns)}
- æ•°æ®ç±»å‹:
{current_dataframe.dtypes.to_string()}

å‰5è¡Œæ•°æ®:
{current_dataframe.head().to_string()}
"""
        return info
    except Exception as e:
        return f"åŠ è½½å¤±è´¥: {str(e)}"

@tool
def analyze_column(column_name: str) -> str:
    """åˆ†ææŒ‡å®šåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ã€‚è¾“å…¥åˆ—åã€‚"""
    global current_dataframe
    if current_dataframe is None:
        return "è¯·å…ˆåŠ è½½æ•°æ®"

    if column_name not in current_dataframe.columns:
        return f"åˆ— '{column_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨åˆ—: {list(current_dataframe.columns)}"

    col = current_dataframe[column_name]

    if pd.api.types.is_numeric_dtype(col):
        stats = f"""
åˆ— '{column_name}' çš„ç»Ÿè®¡ä¿¡æ¯:
- è®¡æ•°: {col.count()}
- å‡å€¼: {col.mean():.2f}
- æ ‡å‡†å·®: {col.std():.2f}
- æœ€å°å€¼: {col.min()}
- 25%åˆ†ä½: {col.quantile(0.25):.2f}
- ä¸­ä½æ•°: {col.median():.2f}
- 75%åˆ†ä½: {col.quantile(0.75):.2f}
- æœ€å¤§å€¼: {col.max()}
- ç¼ºå¤±å€¼: {col.isna().sum()}
"""
    else:
        stats = f"""
åˆ— '{column_name}' çš„ç»Ÿè®¡ä¿¡æ¯:
- å”¯ä¸€å€¼æ•°é‡: {col.nunique()}
- æœ€å¸¸è§å€¼: {col.mode().iloc[0] if len(col.mode()) > 0 else 'N/A'}
- ç¼ºå¤±å€¼: {col.isna().sum()}
- å€¼åˆ†å¸ƒ:
{col.value_counts().head(10).to_string()}
"""
    return stats

@tool
def filter_data(condition: str) -> str:
    """æ ¹æ®æ¡ä»¶ç­›é€‰æ•°æ®ã€‚è¾“å…¥Pythonè¡¨è¾¾å¼,å¦‚ 'age > 30' æˆ– 'city == "åŒ—äº¬"'"""
    global current_dataframe
    if current_dataframe is None:
        return "è¯·å…ˆåŠ è½½æ•°æ®"

    try:
        filtered = current_dataframe.query(condition)
        return f"ç­›é€‰åæ•°æ®: {len(filtered)} è¡Œ\n\n{filtered.head(10).to_string()}"
    except Exception as e:
        return f"ç­›é€‰å¤±è´¥: {str(e)}"

@tool
def calculate_correlation(col1: str, col2: str) -> str:
    """è®¡ç®—ä¸¤åˆ—ä¹‹é—´çš„ç›¸å…³æ€§ã€‚è¾“å…¥ä¸¤ä¸ªåˆ—å,ç”¨é€—å·åˆ†éš”ã€‚"""
    global current_dataframe
    if current_dataframe is None:
        return "è¯·å…ˆåŠ è½½æ•°æ®"

    try:
        corr = current_dataframe[col1].corr(current_dataframe[col2])
        return f"'{col1}' å’Œ '{col2}' çš„ç›¸å…³ç³»æ•°: {corr:.4f}"
    except Exception as e:
        return f"è®¡ç®—å¤±è´¥: {str(e)}"

# åˆ›å»ºAgent
llm = ChatOpenAI(model="gpt-4", temperature=0)

tools = [load_csv, analyze_column, filter_data, calculate_correlation]

prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·:
1. åŠ è½½å’ŒæŸ¥çœ‹CSVæ•°æ®
2. åˆ†ææ•°æ®åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
3. æ ¹æ®æ¡ä»¶ç­›é€‰æ•°æ®
4. è®¡ç®—å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·,å¹¶ç”¨æ¸…æ™°çš„è¯­è¨€è§£é‡Šåˆ†æç»“æœã€‚"""),
    MessagesPlaceholder(variable_name="chat_history", optional=True),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

agent = create_openai_tools_agent(llm, tools, prompt)
data_analyst = AgentExecutor(agent=agent, tools=tools, verbose=True)

# ä½¿ç”¨
response = data_analyst.invoke({
    "input": "åŠ è½½ sales_data.csv æ–‡ä»¶,åˆ†æé”€å”®é¢åˆ—,æ‰¾å‡ºé”€å”®é¢å¤§äº10000çš„è®°å½•"
})
```

---

## 7. é«˜çº§åº”ç”¨

### 7.1 å¤šæ¨¡æ€åº”ç”¨

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import base64

def encode_image(image_path: str) -> str:
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

# ä½¿ç”¨GPT-4 Vision
llm = ChatOpenAI(model="gpt-4-vision-preview", max_tokens=1024)

def analyze_image(image_path: str, question: str):
    base64_image = encode_image(image_path)

    message = HumanMessage(
        content=[
            {"type": "text", "text": question},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}",
                    "detail": "high"
                }
            }
        ]
    )

    response = llm.invoke([message])
    return response.content

# ä½¿ç”¨
result = analyze_image("./chart.png", "è¯·åˆ†æè¿™å¼ å›¾è¡¨æ˜¾ç¤ºçš„è¶‹åŠ¿å’Œå…³é”®æ•°æ®ç‚¹")
```

### 7.2 ç»“æ„åŒ–è¾“å‡º

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from typing import List

# å®šä¹‰è¾“å‡ºç»“æ„
class ProductReview(BaseModel):
    product_name: str = Field(description="äº§å“åç§°")
    rating: int = Field(description="è¯„åˆ†,1-5åˆ†")
    pros: List[str] = Field(description="ä¼˜ç‚¹åˆ—è¡¨")
    cons: List[str] = Field(description="ç¼ºç‚¹åˆ—è¡¨")
    summary: str = Field(description="æ€»ç»“è¯„ä»·")

# åˆ›å»ºè§£æå™¨
parser = JsonOutputParser(pydantic_object=ProductReview)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªäº§å“è¯„æµ‹ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æä¾›çš„äº§å“è¯„è®º,å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚"),
    ("human", "{format_instructions}\n\nè¯·åˆ†æä»¥ä¸‹è¯„è®º:\n{review}")
])

llm = ChatOpenAI(model="gpt-4", temperature=0)

chain = prompt | llm | parser

# ä½¿ç”¨
review_text = """
è¿™æ¬¾è€³æœºéŸ³è´¨éå¸¸æ£’,ä½éŸ³æµ‘åš,é«˜éŸ³æ¸…äº®ã€‚ç»­èˆªèƒ½åŠ›ä¹Ÿå¾ˆå¼º,èƒ½ç”¨ä¸€æ•´å¤©ã€‚
ä¸è¿‡é™å™ªæ•ˆæœä¸€èˆ¬,åœ¨å˜ˆæ‚ç¯å¢ƒä¸‹è¿˜æ˜¯èƒ½å¬åˆ°å¤–ç•Œå£°éŸ³ã€‚å¦å¤–ä»·æ ¼åè´µã€‚
æ€»ä½“æ¥è¯´æ˜¯ä¸€æ¬¾å€¼å¾—è´­ä¹°çš„äº§å“,é€‚åˆå¯¹éŸ³è´¨æœ‰è¦æ±‚çš„ç”¨æˆ·ã€‚
"""

result = chain.invoke({
    "review": review_text,
    "format_instructions": parser.get_format_instructions()
})

print(result)
# è¾“å‡ºç»“æ„åŒ–çš„ ProductReview å¯¹è±¡
```

### 7.3 LangGraph å·¥ä½œæµ

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
import operator

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    next_step: str

# åˆ›å»ºèŠ‚ç‚¹å‡½æ•°
llm = ChatOpenAI(model="gpt-4")

def classifier(state: AgentState):
    """åˆ†ç±»ç”¨æˆ·è¯·æ±‚"""
    last_message = state["messages"][-1].content

    prompt = f"å°†ä»¥ä¸‹è¯·æ±‚åˆ†ç±»ä¸º 'technical' æˆ– 'general': {last_message}"
    response = llm.invoke([HumanMessage(content=prompt)])

    if "technical" in response.content.lower():
        return {"next_step": "technical_handler"}
    return {"next_step": "general_handler"}

def technical_handler(state: AgentState):
    """å¤„ç†æŠ€æœ¯é—®é¢˜"""
    last_message = state["messages"][-1].content
    response = llm.invoke([
        HumanMessage(content=f"ä½œä¸ºæŠ€æœ¯ä¸“å®¶å›ç­”: {last_message}")
    ])
    return {"messages": [AIMessage(content=response.content)]}

def general_handler(state: AgentState):
    """å¤„ç†ä¸€èˆ¬é—®é¢˜"""
    last_message = state["messages"][-1].content
    response = llm.invoke([
        HumanMessage(content=f"ä½œä¸ºå‹å¥½åŠ©æ‰‹å›ç­”: {last_message}")
    ])
    return {"messages": [AIMessage(content=response.content)]}

def router(state: AgentState):
    """è·¯ç”±å†³ç­–"""
    return state["next_step"]

# æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("classifier", classifier)
workflow.add_node("technical_handler", technical_handler)
workflow.add_node("general_handler", general_handler)

# æ·»åŠ è¾¹
workflow.set_entry_point("classifier")
workflow.add_conditional_edges(
    "classifier",
    router,
    {
        "technical_handler": "technical_handler",
        "general_handler": "general_handler"
    }
)
workflow.add_edge("technical_handler", END)
workflow.add_edge("general_handler", END)

# ç¼–è¯‘
app = workflow.compile()

# ä½¿ç”¨
result = app.invoke({
    "messages": [HumanMessage(content="å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½?")],
    "next_step": ""
})
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 æ€§èƒ½ä¼˜åŒ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ€§èƒ½ä¼˜åŒ–ç­–ç•¥                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. ç¼“å­˜ç­–ç•¥                                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ from langchain.cache import InMemoryCache              â”‚    â”‚
â”‚     â”‚ from langchain.globals import set_llm_cache             â”‚    â”‚
â”‚     â”‚ set_llm_cache(InMemoryCache())                          â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  2. æ‰¹é‡å¤„ç†                                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ results = chain.batch(inputs, config={"max_concurrency": 5})â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  3. æµå¼è¾“å‡º (å‡å°‘é¦–å­—å»¶è¿Ÿ)                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚ for chunk in chain.stream(input):                       â”‚    â”‚
â”‚     â”‚     print(chunk, end="")                                â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  4. æ¨¡å‹é€‰æ‹©                                                        â”‚
â”‚     â€¢ ç®€å•ä»»åŠ¡ç”¨ gpt-3.5-turbo (å¿«é€Ÿã€ä¾¿å®œ)                          â”‚
â”‚     â€¢ å¤æ‚æ¨ç†ç”¨ gpt-4 (è´¨é‡é«˜)                                      â”‚
â”‚     â€¢ åµŒå…¥ç”¨ text-embedding-3-small (æ€§ä»·æ¯”é«˜)                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 å®‰å…¨å®è·µ

```python
# ==================== 1. è¾“å…¥éªŒè¯ ====================
from langchain_core.runnables import RunnableLambda

def validate_input(text: str) -> str:
    # æ£€æŸ¥è¾“å…¥é•¿åº¦
    if len(text) > 10000:
        raise ValueError("è¾“å…¥è¿‡é•¿")
    # è¿‡æ»¤æ•æ„Ÿå†…å®¹
    sensitive_words = ["å¯†ç ", "ä¿¡ç”¨å¡"]
    for word in sensitive_words:
        if word in text:
            raise ValueError("åŒ…å«æ•æ„Ÿä¿¡æ¯")
    return text

safe_chain = RunnableLambda(validate_input) | prompt | llm


# ==================== 2. è¾“å‡ºè¿‡æ»¤ ====================
def filter_output(response: str) -> str:
    # ç§»é™¤å¯èƒ½çš„æœ‰å®³å†…å®¹
    # æ·»åŠ å…è´£å£°æ˜ç­‰
    return response

chain = prompt | llm | StrOutputParser() | RunnableLambda(filter_output)


# ==================== 3. é€Ÿç‡é™åˆ¶ ====================
from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
    requests_per_second=1,
    check_every_n_seconds=0.1,
    max_bucket_size=10
)

llm = ChatOpenAI(rate_limiter=rate_limiter)


# ==================== 4. APIå¯†é’¥ç®¡ç† ====================
# ä½¿ç”¨ç¯å¢ƒå˜é‡,ä¸è¦ç¡¬ç¼–ç 
import os
from dotenv import load_dotenv

load_dotenv()  # ä» .env æ–‡ä»¶åŠ è½½
api_key = os.getenv("OPENAI_API_KEY")
```

### 8.3 è°ƒè¯•ä¸ç›‘æ§

```python
# ==================== ä½¿ç”¨ LangSmith ====================
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-project"

# æ‰€æœ‰è°ƒç”¨éƒ½ä¼šè‡ªåŠ¨è®°å½•åˆ° LangSmith


# ==================== è‡ªå®šä¹‰å›è°ƒ ====================
from langchain_core.callbacks import BaseCallbackHandler

class CustomHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"ğŸš€ LLM å¼€å§‹è°ƒç”¨")
        print(f"   Prompt: {prompts[0][:100]}...")

    def on_llm_end(self, response, **kwargs):
        print(f"âœ… LLM è°ƒç”¨å®Œæˆ")
        print(f"   Token ä½¿ç”¨: {response.llm_output.get('token_usage', 'N/A')}")

    def on_llm_error(self, error, **kwargs):
        print(f"âŒ LLM é”™è¯¯: {error}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"â›“ï¸ Chain å¼€å§‹: {serialized.get('name', 'unnamed')}")

    def on_chain_end(self, outputs, **kwargs):
        print(f"â›“ï¸ Chain ç»“æŸ")

# ä½¿ç”¨
handler = CustomHandler()
chain.invoke(input, config={"callbacks": [handler]})


# ==================== æˆæœ¬è¿½è¸ª ====================
from langchain_community.callbacks import get_openai_callback

with get_openai_callback() as cb:
    result = chain.invoke({"input": "ä½ å¥½"})
    print(f"æ€» Token: {cb.total_tokens}")
    print(f"Prompt Token: {cb.prompt_tokens}")
    print(f"Completion Token: {cb.completion_tokens}")
    print(f"æ€»æˆæœ¬: ${cb.total_cost:.4f}")
```

### 8.4 é¡¹ç›®ç»“æ„å»ºè®®

```
my_langchain_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ qa_chain.py          # QAé“¾
â”‚   â”‚   â””â”€â”€ summarize_chain.py   # æ‘˜è¦é“¾
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ assistant_agent.py   # åŠ©æ‰‹Agent
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search_tool.py       # æœç´¢å·¥å…·
â”‚   â”‚   â””â”€â”€ calculator_tool.py   # è®¡ç®—å·¥å…·
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ templates.py         # Promptæ¨¡æ¿
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ session_memory.py    # ä¼šè¯è®°å¿†
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ callbacks.py         # å›è°ƒå¤„ç†
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/               # æ–‡æ¡£å­˜å‚¨
â”‚   â””â”€â”€ vectorstore/             # å‘é‡ç´¢å¼•
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_chains.py
â”œâ”€â”€ .env                         # ç¯å¢ƒå˜é‡
â”œâ”€â”€ .env.example                 # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“š æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangChain å­¦ä¹ è·¯çº¿å›¾                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Level 1: åŸºç¡€å…¥é—¨
â”œâ”€â”€ âœ… äº†è§£ LangChain æ˜¯ä»€ä¹ˆ
â”œâ”€â”€ âœ… å®‰è£…é…ç½®ç¯å¢ƒ
â”œâ”€â”€ âœ… ä½¿ç”¨ Models (LLM/Chat/Embeddings)
â””â”€â”€ âœ… ä½¿ç”¨ PromptTemplate

Level 2: æ ¸å¿ƒæŒæ¡
â”œâ”€â”€ âœ… ç†è§£ LCEL è¡¨è¾¾å¼
â”œâ”€â”€ âœ… æ„å»º Chains
â”œâ”€â”€ âœ… å®ç° Memory
â””â”€â”€ âœ… æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²

Level 3: è¿›é˜¶åº”ç”¨
â”œâ”€â”€ âœ… æ„å»º RAG åº”ç”¨
â”œâ”€â”€ âœ… ä½¿ç”¨ VectorStore
â”œâ”€â”€ âœ… åˆ›å»ºè‡ªå®šä¹‰ Tools
â””â”€â”€ âœ… å®ç° Agents

Level 4: é«˜çº§è¿›é˜¶
â”œâ”€â”€ â¬œ LangGraph å·¥ä½œæµ
â”œâ”€â”€ â¬œ å¤šæ¨¡æ€åº”ç”¨
â”œâ”€â”€ â¬œ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
â””â”€â”€ â¬œ LangSmith ç›‘æ§è°ƒä¼˜

Level 5: ä¸“å®¶æ°´å¹³
â”œâ”€â”€ â¬œ è‡ªå®šä¹‰ç»„ä»¶å¼€å‘
â”œâ”€â”€ â¬œ æ€§èƒ½è°ƒä¼˜
â”œâ”€â”€ â¬œ ä¼ä¸šçº§æ¶æ„è®¾è®¡
â””â”€â”€ â¬œ å¼€æºè´¡çŒ®
```

---

## ğŸ”— ç›¸å…³èµ„æº

| èµ„æº         | é“¾æ¥                                      |
| ------------ | ----------------------------------------- |
| å®˜æ–¹æ–‡æ¡£     | https://python.langchain.com/docs         |
| GitHub       | https://github.com/langchain-ai/langchain |
| LangSmith    | https://smith.langchain.com               |
| LangGraph    | https://langchain-ai.github.io/langgraph  |
| ç¤¾åŒº Discord | https://discord.gg/langchain              |

---

> ğŸ’¡ **æç¤º**: LangChain æ›´æ–°è¿­ä»£è¾ƒå¿«,å»ºè®®ç»å¸¸æŸ¥é˜…å®˜æ–¹æ–‡æ¡£è·å–æœ€æ–°ä¿¡æ¯ã€‚
