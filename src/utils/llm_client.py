"""
ç»Ÿä¸€ LLM å®¢æˆ·ç«¯
================

æä¾›ç»Ÿä¸€çš„æ¥å£æ¥è°ƒç”¨ä¸åŒçš„ LLM APIã€‚
é»˜è®¤ä½¿ç”¨ Google Geminiï¼ˆå…è´¹é¢åº¦è¾ƒå¤§ï¼‰ï¼Œå¯åˆ‡æ¢åˆ° OpenAI æˆ– Ollamaã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    from utils.llm_client import get_client, chat

    # ç®€å•è°ƒç”¨
    response = chat("ä½ å¥½")
    print(response)

    # æµå¼è°ƒç”¨
    for chunk in chat("ä½ å¥½", stream=True):
        print(chunk, end="", flush=True)
"""

import os
from typing import Iterator, Optional, List, Dict, Any
from dotenv import load_dotenv

load_dotenv()


class LLMClient:
    """ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯"""

    def __init__(self, provider: str = "auto"):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            provider: "gemini", "openai", "ollama", æˆ– "auto"ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        self.provider = self._detect_provider() if provider == "auto" else provider
        self._client = None
        self._model = None
        self._setup_client()

    def _detect_provider(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ API æä¾›å•†"""
        if os.getenv("GOOGLE_API_KEY"):
            return "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            return "openai"
        elif os.getenv("OLLAMA_HOST") or self._check_ollama():
            return "ollama"
        else:
            raise ValueError(
                "æœªæ‰¾åˆ°å¯ç”¨çš„ API Keyï¼Œè¯·é…ç½® GOOGLE_API_KEY æˆ– OPENAI_API_KEY"
            )

    def _check_ollama(self) -> bool:
        """æ£€æŸ¥ Ollama æ˜¯å¦å¯ç”¨"""
        try:
            import requests

            host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
            response = requests.get(f"{host}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False

    def _setup_client(self):
        """è®¾ç½®å®¢æˆ·ç«¯"""
        if self.provider == "gemini":
            import google.generativeai as genai

            genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
            self._model = genai.GenerativeModel("gemini-2.0-flash")

        elif self.provider == "openai":
            from openai import OpenAI

            self._client = OpenAI()
            self._model = "gpt-3.5-turbo"

        elif self.provider == "ollama":
            from openai import OpenAI

            host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
            self._client = OpenAI(base_url=f"{host}/v1", api_key="ollama")
            self._model = "llama3"

    def chat(
        self,
        message: str,
        system: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        stream: bool = False,
        **kwargs,
    ) -> str | Iterator[str]:
        """
        å‘é€èŠå¤©æ¶ˆæ¯

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚ temperature, max_tokensï¼‰

        Returns:
            å›å¤æ–‡æœ¬ï¼Œæˆ–æµå¼è¾“å‡ºçš„è¿­ä»£å™¨
        """
        if self.provider == "gemini":
            return self._chat_gemini(message, system, history, stream, **kwargs)
        else:
            return self._chat_openai(message, system, history, stream, **kwargs)

    def _chat_gemini(
        self,
        message: str,
        system: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        stream: bool = False,
        **kwargs,
    ) -> str | Iterator[str]:
        """Gemini èŠå¤©å®ç°"""
        import google.generativeai as genai

        # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ï¼Œé‡æ–°åˆ›å»ºæ¨¡å‹
        if system:
            model = genai.GenerativeModel("gemini-2.0-flash", system_instruction=system)
        else:
            model = self._model

        # æ„å»ºå¯¹è¯å†å²
        if history:
            chat = model.start_chat(
                history=[{"role": h["role"], "parts": [h["content"]]} for h in history]
            )
            response = chat.send_message(message, stream=stream)
        else:
            response = model.generate_content(message, stream=stream)

        if stream:

            def stream_generator():
                for chunk in response:
                    if chunk.text:
                        yield chunk.text

            return stream_generator()
        else:
            return response.text

    def _chat_openai(
        self,
        message: str,
        system: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        stream: bool = False,
        **kwargs,
    ) -> str | Iterator[str]:
        """OpenAI/Ollama èŠå¤©å®ç°"""
        messages = []

        if system:
            messages.append({"role": "system", "content": system})

        if history:
            messages.extend(history)

        messages.append({"role": "user", "content": message})

        response = self._client.chat.completions.create(
            model=self._model, messages=messages, stream=stream, **kwargs
        )

        if stream:

            def stream_generator():
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content

            return stream_generator()
        else:
            return response.choices[0].message.content

    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æä¾›å•†ä¿¡æ¯"""
        return {
            "provider": self.provider,
            "model": self._model
            if isinstance(self._model, str)
            else "gemini-2.0-flash",
        }


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_default_client: Optional[LLMClient] = None


def get_client(provider: str = "auto") -> LLMClient:
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹"""
    global _default_client
    if _default_client is None or _default_client.provider != provider:
        _default_client = LLMClient(provider)
    return _default_client


def chat(
    message: str,
    system: Optional[str] = None,
    history: Optional[List[Dict[str, str]]] = None,
    stream: bool = False,
    **kwargs,
) -> str | Iterator[str]:
    """
    å¿«æ·èŠå¤©å‡½æ•°

    ç¤ºä¾‹ï¼š
        # ç®€å•è°ƒç”¨
        response = chat("ä½ å¥½")

        # å¸¦ç³»ç»Ÿæç¤ºè¯
        response = chat("ä»Šå¤©å¤©æ°”å¾ˆå¥½", system="ä½ æ˜¯ç¿»è¯‘å®˜ï¼Œå°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡")

        # æµå¼è¾“å‡º
        for chunk in chat("å†™ä¸€é¦–è¯—", stream=True):
            print(chunk, end="", flush=True)
    """
    client = get_client()
    return client.chat(message, system, history, stream, **kwargs)


def check_api_status() -> Dict[str, bool]:
    """æ£€æŸ¥å„ API çš„å¯ç”¨çŠ¶æ€"""
    status = {
        "gemini": bool(os.getenv("GOOGLE_API_KEY")),
        "openai": bool(os.getenv("OPENAI_API_KEY")),
        "ollama": False,
    }

    # æ£€æŸ¥ Ollama
    try:
        import requests

        host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
        response = requests.get(f"{host}/api/tags", timeout=2)
        status["ollama"] = response.status_code == 200
    except:
        pass

    return status


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ” æ£€æŸ¥ API çŠ¶æ€...")
    status = check_api_status()
    for provider, available in status.items():
        icon = "âœ…" if available else "âŒ"
        print(f"  {icon} {provider}")

    print("\nğŸ“¤ æµ‹è¯•èŠå¤©...")
    try:
        client = get_client()
        info = client.get_provider_info()
        print(f"  ä½¿ç”¨: {info['provider']} ({info['model']})")

        response = chat("ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
        print(f"  å›å¤: {response}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")
