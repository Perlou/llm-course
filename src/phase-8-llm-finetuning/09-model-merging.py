"""
æ¨¡å‹åˆå¹¶
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£æ¨¡å‹åˆå¹¶çš„æ¦‚å¿µ
    2. æŒæ¡ LoRA æƒé‡åˆå¹¶
    3. äº†è§£å¤šæ¨¡å‹åˆå¹¶æŠ€æœ¯

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Merge and Unload
    - æ¨¡å‹èåˆ
    - TIES/DARE Merge

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install peft transformers
"""

import os
from dotenv import load_dotenv

load_dotenv()


def merge_overview():
    """åˆå¹¶æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹åˆå¹¶æ¦‚è¿°")
    print("=" * 60)

    print("""
    ä¸ºä»€ä¹ˆéœ€è¦æ¨¡å‹åˆå¹¶ï¼Ÿ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. æ¨ç†æ•ˆç‡
       - åˆå¹¶ååªéœ€åŠ è½½ä¸€ä¸ªæ¨¡å‹
       - æ— é¢å¤–è®¡ç®—å¼€é”€
    
    2. èƒ½åŠ›ç»„åˆ
       - åˆå¹¶å¤šä¸ªä¸“ä¸šæ¨¡å‹
       - è·å¾—ç»¼åˆèƒ½åŠ›
    
    3. éƒ¨ç½²ç®€åŒ–
       - æ— éœ€ç®¡ç†å¤šä¸ª adapter
       - ç»Ÿä¸€æ¨¡å‹æ ¼å¼
    
    
    åˆå¹¶ç±»å‹
    â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    æ¨¡å‹åˆå¹¶ç±»å‹                          â”‚
    â”‚                                                         â”‚
    â”‚   1. LoRA åˆå¹¶                                          â”‚
    â”‚      Base + LoRA â†’ Merged Model                         â”‚
    â”‚                                                         â”‚
    â”‚   2. å¤š Adapter åˆå¹¶                                    â”‚
    â”‚      Base + LoRA1 + LoRA2 â†’ Combined Model              â”‚
    â”‚                                                         â”‚
    â”‚   3. å¤šæ¨¡å‹èåˆ                                         â”‚
    â”‚      Model A + Model B â†’ Hybrid Model                   â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def lora_merge():
    """LoRA åˆå¹¶"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šLoRA æƒé‡åˆå¹¶")
    print("=" * 60)

    print("""
    LoRA åˆå¹¶åŸç†
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    W' = W + BA
    
    åˆå¹¶åå¾—åˆ°ä¸€ä¸ªæ ‡å‡†æ¨¡å‹ï¼Œæ²¡æœ‰é¢å¤–çš„ adapter æƒé‡ã€‚
    """)

    code_example = """
    from peft import PeftModel
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-2-7b-hf",
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # 2. åŠ è½½ PEFT æ¨¡å‹
    peft_model = PeftModel.from_pretrained(
        base_model,
        "./lora_adapter",
    )

    # 3. åˆå¹¶æƒé‡
    merged_model = peft_model.merge_and_unload()

    # 4. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    merged_model.save_pretrained("./merged_model")

    # åŒæ—¶ä¿å­˜ tokenizer
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
    tokenizer.save_pretrained("./merged_model")

    # 5. éªŒè¯åˆå¹¶ç»“æœ
    # åŠ è½½åˆå¹¶æ¨¡å‹
    loaded_model = AutoModelForCausalLM.from_pretrained("./merged_model")
    """

    print(code_example)


def multi_adapter_merge():
    """å¤š Adapter åˆå¹¶"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤š Adapter åˆå¹¶")
    print("=" * 60)

    print("""
    åŠ æƒåˆå¹¶å¤šä¸ª Adapter
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    åœºæ™¯ï¼šç»„åˆå¤šä¸ªä»»åŠ¡ç‰¹å®šçš„ adapter
    """)

    code_example = """
    from peft import PeftModel

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("base_model")

    # åŠ è½½ç¬¬ä¸€ä¸ª adapter
    model = PeftModel.from_pretrained(model, "./adapter_translate")

    # åŠ è½½ç¬¬äºŒä¸ª adapter
    model.load_adapter("./adapter_summarize", adapter_name="summarize")

    # æ–¹æ³• 1: åˆ‡æ¢ä½¿ç”¨
    model.set_adapter("default")  # translate
    output1 = model.generate(...)

    model.set_adapter("summarize")
    output2 = model.generate(...)

    # æ–¹æ³• 2: åŠ æƒåˆå¹¶
    model.add_weighted_adapter(
        adapters=["default", "summarize"],
        weights=[0.6, 0.4],
        adapter_name="combined",
        combination_type="linear",  # æˆ– "svd", "ties"
    )
    model.set_adapter("combined")

    # æ–¹æ³• 3: åˆå¹¶å¹¶å¸è½½
    merged = model.merge_and_unload()
    """

    print(code_example)


def advanced_merge():
    """é«˜çº§åˆå¹¶æŠ€æœ¯"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šé«˜çº§åˆå¹¶æŠ€æœ¯")
    print("=" * 60)

    print("""
    æ¨¡å‹èåˆæŠ€æœ¯
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. çº¿æ€§æ’å€¼ (Linear Interpolation)
       W_merged = Î± Ã— W_A + (1-Î±) Ã— W_B
    
    2. SLERP (Spherical Linear Interpolation)
       å¯¹æƒé‡å‘é‡è¿›è¡Œçƒé¢æ’å€¼
    
    3. TIES Merge
       - ä¿ç•™æœ€é‡è¦çš„å‚æ•°å˜åŒ–
       - è§£å†³ç¬¦å·å†²çª
       - æ•ˆæœé€šå¸¸æ›´å¥½
    
    4. DARE (Drop And REscale)
       - éšæœºä¸¢å¼ƒéƒ¨åˆ†å˜åŒ–
       - é‡æ–°ç¼©æ”¾å‰©ä½™å˜åŒ–
    """)

    merge_code = '''
    # ä½¿ç”¨ mergekit è¿›è¡Œé«˜çº§åˆå¹¶
    # pip install mergekit

    # YAML é…ç½®æ–‡ä»¶ (merge_config.yaml):
    """
    models:
      - model: ./model_a
        parameters:
          weight: 0.6
      - model: ./model_b
        parameters:
          weight: 0.4
    merge_method: ties  # æˆ– linear, slerp, dare_ties
    base_model: meta-llama/Llama-2-7b-hf
    parameters:
      density: 0.5
      normalize: true
    dtype: float16
    """

    # å‘½ä»¤è¡Œæ‰§è¡Œ
    # mergekit-yaml merge_config.yaml ./merged_output
    '''

    print(merge_code)


def merge_tips():
    """åˆå¹¶æŠ€å·§"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šåˆå¹¶æœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
    æœ€ä½³å®è·µ
    â”€â”€â”€â”€â”€â”€â”€
    
    1. åˆå¹¶å‰
       - ç¡®ä¿åŸºç¡€æ¨¡å‹å®Œå…¨ç›¸åŒ
       - æ£€æŸ¥ adapter å…¼å®¹æ€§
       - å¤‡ä»½åŸå§‹æ¨¡å‹
    
    2. æƒé‡é€‰æ‹©
       - æ ¹æ®ä»»åŠ¡é‡è¦æ€§è°ƒæ•´æƒé‡
       - å¯ä»¥è¿›è¡Œæ¶ˆèå®éªŒ
    
    3. éªŒè¯åˆå¹¶
       - åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæµ‹è¯•
       - æ£€æŸ¥æ˜¯å¦æœ‰èƒ½åŠ›ä¸¢å¤±
    
    4. ç²¾åº¦æ³¨æ„
       - åˆå¹¶æ—¶ä¿æŒé«˜ç²¾åº¦
       - é‡åŒ–åœ¨åˆå¹¶åè¿›è¡Œ
    
    
    å¸¸è§é—®é¢˜
    â”€â”€â”€â”€â”€â”€â”€
    
    Q: åˆå¹¶åæ¨¡å‹å˜å¤§äº†ï¼Ÿ
    A: æ­£å¸¸ï¼Œå› ä¸º adapter æƒé‡è¢«èå…¥ã€‚
    
    Q: èƒ½åŠ›å‘ç”Ÿå†²çªæ€ä¹ˆåŠï¼Ÿ
    A: è°ƒæ•´æƒé‡æˆ–ä½¿ç”¨ TIES åˆå¹¶ã€‚
    
    Q: QLoRA æ¨¡å‹èƒ½åˆå¹¶å—ï¼Ÿ
    A: éœ€è¦å…ˆåé‡åŒ–åŸºç¡€æ¨¡å‹ã€‚
    """)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šLoRA åˆå¹¶
        è®­ç»ƒä¸€ä¸ª LoRA å¹¶åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from peft import PeftModel, LoraConfig, get_peft_model
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        # 1. è®­ç»ƒ LoRA (å‡è®¾å·²å®Œæˆ)
        # ...
        
        # 2. åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2-0.5B",
            torch_dtype=torch.float16,
        )
        
        # 3. åŠ è½½ LoRA adapter
        peft_model = PeftModel.from_pretrained(base_model, "./lora_adapter")
        
        # 4. åˆå¹¶æƒé‡
        merged_model = peft_model.merge_and_unload()
        
        # 5. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        merged_model.save_pretrained("./merged_model")
        
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
        tokenizer.save_pretrained("./merged_model")
        
        # 6. éªŒè¯
        loaded = AutoModelForCausalLM.from_pretrained("./merged_model")
        print("åˆå¹¶æˆåŠŸï¼")
        ```
    
    ç»ƒä¹  2ï¼šå¤š Adapter å®éªŒ
        æ¯”è¾ƒä¸åŒæƒé‡çš„åˆå¹¶æ•ˆæœ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        def experiment_merge_weights(model, adapter_names, weight_pairs):
            results = []
            
            for weights in weight_pairs:
                # åˆ›å»ºåˆå¹¶çš„ adapter
                model.add_weighted_adapter(
                    adapters=adapter_names,
                    weights=list(weights),
                    adapter_name=f"merged_{weights}",
                )
                model.set_adapter(f"merged_{weights}")
                
                # è¯„ä¼°
                task1_score = evaluate_task1(model)
                task2_score = evaluate_task2(model)
                
                results.append({
                    "weights": weights,
                    "task1": task1_score,
                    "task2": task2_score,
                    "avg": (task1_score + task2_score) / 2,
                })
            
            # æ‰¾åˆ°æœ€ä½³æƒé‡
            best = max(results, key=lambda x: x["avg"])
            return results, best
        
        # æµ‹è¯•
        weight_pairs = [(0.3, 0.7), (0.5, 0.5), (0.7, 0.3)]
        results, best = experiment_merge_weights(
            model, ["adapter_a", "adapter_b"], weight_pairs
        )
        ```
    
    ç»ƒä¹  3ï¼šTIES åˆå¹¶
        ä½¿ç”¨ mergekit è¿›è¡Œé«˜çº§åˆå¹¶

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```yaml
        # merge_config.yaml
        models:
          - model: ./model_chat
            parameters:
              weight: 0.5
              density: 0.5
          - model: ./model_code
            parameters:
              weight: 0.5
              density: 0.5
        merge_method: ties
        base_model: Qwen/Qwen2-0.5B
        parameters:
          normalize: true
          int8_mask: true
        dtype: float16
        ```
        
        ```bash
        # æ‰§è¡Œåˆå¹¶
        pip install mergekit
        mergekit-yaml merge_config.yaml ./merged_output --cuda
        ```
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. åˆå¹¶åèƒ½å¦æ¢å¤åŸå§‹ adapterï¼Ÿ

       âœ… ç­”ï¼š
       ä¸èƒ½ã€‚åˆå¹¶æ˜¯ä¸å¯é€†æ“ä½œã€‚
       å»ºè®®ï¼š
       - åˆå¹¶å‰ä¿å­˜ adapter å‰¯æœ¬
       - ä¿å­˜åˆå¹¶é…ç½®ç”¨äºå¤ç°
       - ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç®¡ç† adapter

    2. å¦‚ä½•è‡ªåŠ¨é€‰æ‹©æœ€ä½³åˆå¹¶æƒé‡ï¼Ÿ

       âœ… ç­”ï¼š
       - ç½‘æ ¼æœç´¢ï¼šéå†æƒé‡ç»„åˆ
       - è´å¶æ–¯ä¼˜åŒ–ï¼šæ™ºèƒ½æœç´¢
       - éªŒè¯é›†é©±åŠ¨ï¼šä½¿ç”¨éªŒè¯é›†åˆ†æ•°æŒ‡å¯¼
       
       ```python
       from sklearn.model_selection import ParameterGrid
       
       param_grid = {"w1": [0.3, 0.5, 0.7], "w2": [0.3, 0.5, 0.7]}
       best_score = 0
       best_weights = None
       
       for params in ParameterGrid(param_grid):
           if params["w1"] + params["w2"] != 1.0:
               continue
           score = evaluate_merged(params["w1"], params["w2"])
           if score > best_score:
               best_score = score
               best_weights = params
       ```
    """)


def main():
    print("ğŸ”— æ¨¡å‹åˆå¹¶")
    print("=" * 60)
    merge_overview()
    lora_merge()
    multi_adapter_merge()
    advanced_merge()
    merge_tips()
    exercises()
    print("\nâœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š10-finetuning-evaluation.py")


if __name__ == "__main__":
    main()
