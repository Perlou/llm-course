"""
å¾®è°ƒæ•ˆæœè¯„ä¼°
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡å¾®è°ƒè¯„ä¼°æŒ‡æ ‡
    2. å­¦ä¼šä½¿ç”¨è¯„ä¼°å·¥å…·
    3. ç†è§£ benchmark æµ‹è¯•

æ ¸å¿ƒæ¦‚å¿µï¼š
    - æŸå¤±å’Œå›°æƒ‘åº¦
    - ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡
    - LLM è¯„ä¼° benchmark

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install lm-eval transformers
"""

import os
from dotenv import load_dotenv

load_dotenv()


def evaluation_overview():
    """è¯„ä¼°æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šè¯„ä¼°æ¦‚è¿°")
    print("=" * 60)

    print("""
    è¯„ä¼°ç»´åº¦
    â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               å¾®è°ƒè¯„ä¼°ç»´åº¦                               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚   â”‚  è®­ç»ƒæŒ‡æ ‡   â”‚  â”‚  ä»»åŠ¡æ€§èƒ½   â”‚  â”‚  ç»¼åˆèƒ½åŠ›   â”‚    â”‚
    â”‚   â”‚             â”‚  â”‚             â”‚  â”‚             â”‚    â”‚
    â”‚   â”‚ â€¢ Loss      â”‚  â”‚ â€¢ å‡†ç¡®ç‡    â”‚  â”‚ â€¢ MMLU      â”‚    â”‚
    â”‚   â”‚ â€¢ PPL       â”‚  â”‚ â€¢ F1        â”‚  â”‚ â€¢ HumanEval â”‚    â”‚
    â”‚   â”‚ â€¢ æ”¶æ•›æ€§    â”‚  â”‚ â€¢ BLEU      â”‚  â”‚ â€¢ TruthfulQAâ”‚    â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    è¯„ä¼°ç­–ç•¥
    â”€â”€â”€â”€â”€â”€â”€
    
    1. è®­ç»ƒè¿‡ç¨‹ç›‘æ§
       - è®­ç»ƒ/éªŒè¯ loss
       - æ¢¯åº¦èŒƒæ•°
       - å­¦ä¹ ç‡å˜åŒ–
    
    2. ä»»åŠ¡æ•ˆæœè¯„ä¼°
       - åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„æ€§èƒ½
       - ä¸åŸºç¡€æ¨¡å‹å¯¹æ¯”
    
    3. é€šç”¨èƒ½åŠ›æ£€æŸ¥
       - ç¡®ä¿æ²¡æœ‰ç¾éš¾æ€§é—å¿˜
       - ç»¼åˆ benchmark æµ‹è¯•
    """)


def training_metrics():
    """è®­ç»ƒæŒ‡æ ‡"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šè®­ç»ƒæŒ‡æ ‡")
    print("=" * 60)

    print("""
    æ ¸å¿ƒè®­ç»ƒæŒ‡æ ‡
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. Loss (æŸå¤±)
       - äº¤å‰ç†µæŸå¤±
       - è¶Šä½è¶Šå¥½
       - ç›‘æ§æ˜¯å¦è¿‡æ‹Ÿåˆ
    
    2. Perplexity (å›°æƒ‘åº¦)
       PPL = exp(Loss)
       
       - è¡¡é‡æ¨¡å‹å¯¹æ•°æ®çš„"å›°æƒ‘"ç¨‹åº¦
       - è¶Šä½è¡¨ç¤ºæ¨¡å‹è¶Šç¡®ä¿¡
    
    3. å­¦ä¹ æ›²çº¿
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Loss                                â”‚
       â”‚  â”‚                                  â”‚
       â”‚  â”‚\\                                 â”‚
       â”‚  â”‚ \\____                           â”‚
       â”‚  â”‚      \\____  <- è®­ç»ƒ loss        â”‚
       â”‚  â”‚            \\___                 â”‚
       â”‚  â”‚                 \\               â”‚
       â”‚  â”‚   éªŒè¯ loss ->   \\____          â”‚
       â”‚  â”‚                       \\___      â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
       â”‚                           Epochs    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    code_example = """
    from transformers import Trainer

    # è®­ç»ƒæ—¶è‡ªåŠ¨è®°å½•
    trainer.train()

    # è·å–è®­ç»ƒå†å²
    train_loss = trainer.state.log_history

    # è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        # è®¡ç®—å‡†ç¡®ç‡ã€F1 ç­‰
        return {"accuracy": accuracy}

    trainer = Trainer(
        model=model,
        args=training_args,
        compute_metrics=compute_metrics,
    )
    """
    print(code_example)


def task_evaluation():
    """ä»»åŠ¡è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»»åŠ¡ç‰¹å®šè¯„ä¼°")
    print("=" * 60)

    print("""
    å¸¸ç”¨ä»»åŠ¡æŒ‡æ ‡
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ä»»åŠ¡       â”‚            è¯„ä¼°æŒ‡æ ‡                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ æ–‡æœ¬åˆ†ç±»       â”‚ Accuracy, F1, Precision, Recall   â”‚
    â”‚ æƒ…æ„Ÿåˆ†æ       â”‚ Accuracy, Macro-F1                â”‚
    â”‚ é—®ç­”           â”‚ Exact Match, F1                   â”‚
    â”‚ æ‘˜è¦           â”‚ ROUGE-1/2/L, BERTScore            â”‚
    â”‚ ç¿»è¯‘           â”‚ BLEU, chrF                        â”‚
    â”‚ ä»£ç ç”Ÿæˆ       â”‚ pass@k, ç¼–è¯‘é€šè¿‡ç‡                 â”‚
    â”‚ æŒ‡ä»¤éµå¾ª       â”‚ äººå·¥è¯„ä¼°, GPT-4 è¯„åˆ†              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    eval_code = '''
    # ä½¿ç”¨ evaluate åº“
    import evaluate

    # åŠ è½½æŒ‡æ ‡
    rouge = evaluate.load("rouge")
    bleu = evaluate.load("bleu")

    # è®¡ç®— ROUGE
    results = rouge.compute(
        predictions=predictions,
        references=references,
    )
    print(f"ROUGE-L: {results['rougeL']}")

    # è®¡ç®— BLEU
    results = bleu.compute(
        predictions=predictions,
        references=references,
    )
    print(f"BLEU: {results['bleu']}")

    # ä½¿ç”¨ GPT-4 è¯„ä¼°æŒ‡ä»¤éµå¾ª
    def gpt4_evaluate(instruction: str, response: str) -> int:
        prompt = f"""è¯„ä¼°ä»¥ä¸‹å›ç­”æ˜¯å¦æ­£ç¡®éµå¾ªäº†æŒ‡ä»¤ã€‚
        
æŒ‡ä»¤: {instruction}
å›ç­”: {response}

è¯„åˆ† (1-5):"""
        
        # è°ƒç”¨ GPT-4 è¯„åˆ†
        score = call_gpt4(prompt)
        return int(score)
    '''
    print(eval_code)


def benchmark_eval():
    """Benchmark è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šBenchmark è¯„ä¼°")
    print("=" * 60)

    print("""
    å¸¸ç”¨ Benchmark
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Benchmark    â”‚            è¯„ä¼°å†…å®¹                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ MMLU           â”‚ çŸ¥è¯†å’Œæ¨ç† (57 å­¦ç§‘)               â”‚
    â”‚ HellaSwag      â”‚ å¸¸è¯†æ¨ç†                          â”‚
    â”‚ ARC            â”‚ ç§‘å­¦æ¨ç†                          â”‚
    â”‚ TruthfulQA     â”‚ çœŸå®æ€§å’Œäº‹å®å‡†ç¡®æ€§                â”‚
    â”‚ HumanEval      â”‚ ä»£ç ç”Ÿæˆ                          â”‚
    â”‚ GSM8K          â”‚ æ•°å­¦æ¨ç†                          â”‚
    â”‚ MT-Bench       â”‚ å¤šè½®å¯¹è¯èƒ½åŠ›                      â”‚
    â”‚ AlpacaEval     â”‚ æŒ‡ä»¤éµå¾ª (GPT-4 è¯„åˆ¤)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    lm_eval_code = '''
    # ä½¿ç”¨ lm-evaluation-harness
    # pip install lm-eval

    # å‘½ä»¤è¡Œè¯„ä¼°
    """
    lm_eval --model hf \\
        --model_args pretrained=./merged_model \\
        --tasks mmlu,hellaswag,arc_easy,arc_challenge \\
        --batch_size 16 \\
        --output_path ./eval_results
    """

    # Python API
    from lm_eval import evaluator

    results = evaluator.simple_evaluate(
        model="hf",
        model_args="pretrained=./merged_model",
        tasks=["mmlu", "hellaswag"],
        batch_size=16,
    )

    print(results["results"])
    '''
    print(lm_eval_code)


def evaluation_workflow():
    """è¯„ä¼°æµç¨‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå®Œæ•´è¯„ä¼°æµç¨‹")
    print("=" * 60)

    print("""
    æ¨èè¯„ä¼°æµç¨‹
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    å¾®è°ƒè¯„ä¼°æµç¨‹                          â”‚
    â”‚                                                         â”‚
    â”‚   Step 1: è®­ç»ƒç›‘æ§                                      â”‚
    â”‚   â””â”€â”€â–¶ ç›‘æ§ loss æ›²çº¿ï¼Œç¡®ä¿æ”¶æ•›                          â”‚
    â”‚                                                         â”‚
    â”‚   Step 2: å®šæ€§æµ‹è¯•                                      â”‚
    â”‚   â””â”€â”€â–¶ äººå·¥æµ‹è¯•å‡ ä¸ªå…¸å‹æ¡ˆä¾‹                              â”‚
    â”‚                                                         â”‚
    â”‚   Step 3: ä»»åŠ¡è¯„ä¼°                                      â”‚
    â”‚   â””â”€â”€â–¶ åœ¨ç›®æ ‡ä»»åŠ¡æµ‹è¯•é›†ä¸Šè¯„ä¼°                            â”‚
    â”‚                                                         â”‚
    â”‚   Step 4: å¯¹æ¯”è¯„ä¼°                                      â”‚
    â”‚   â””â”€â”€â–¶ ä¸åŸºç¡€æ¨¡å‹ã€å‰ç‰ˆæœ¬å¯¹æ¯”                            â”‚
    â”‚                                                         â”‚
    â”‚   Step 5: é€šç”¨èƒ½åŠ›æ£€æŸ¥                                  â”‚
    â”‚   â””â”€â”€â–¶ Benchmark æµ‹è¯•ï¼Œç¡®ä¿æ— é—å¿˜                        â”‚
    â”‚                                                         â”‚
    â”‚   Step 6: ä¸Šçº¿å‰æµ‹è¯•                                    â”‚
    â”‚   â””â”€â”€â–¶ A/B æµ‹è¯•æˆ–äººå·¥è¯„ä¼°                               â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    è¯„ä¼°æŠ¥å‘Šæ¨¡æ¿
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. è®­ç»ƒä¿¡æ¯
       - æ•°æ®é‡ã€è½®æ•°ã€æ—¶é—´
       - æœ€ç»ˆ loss/PPL
    
    2. ä»»åŠ¡æ€§èƒ½
       - å„æŒ‡æ ‡åˆ†æ•°
       - ä¸ baseline å¯¹æ¯”
    
    3. æ ·æœ¬å±•ç¤º
       - å¥½çš„æ¡ˆä¾‹
       - å¤±è´¥æ¡ˆä¾‹åˆ†æ
    
    4. ç»“è®ºå»ºè®®
       - æ˜¯å¦è¾¾åˆ°é¢„æœŸ
       - æ”¹è¿›æ–¹å‘
    """)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šè¯„ä¼°è„šæœ¬
        ç¼–å†™è‡ªåŠ¨åŒ–è¯„ä¼°è„šæœ¬

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import json
        from datetime import datetime
        
        class ModelEvaluator:
            def __init__(self, model, tokenizer, test_data):
                self.model = model
                self.tokenizer = tokenizer
                self.test_data = test_data
            
            def evaluate_all(self):
                results = {
                    "timestamp": datetime.now().isoformat(),
                    "metrics": {}
                }
                
                # 1. å›°æƒ‘åº¦
                results["metrics"]["perplexity"] = self.calculate_ppl()
                
                # 2. å‡†ç¡®ç‡ (å¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡)
                results["metrics"]["accuracy"] = self.calculate_accuracy()
                
                # 3. ç”Ÿæˆè´¨é‡
                results["metrics"]["generation"] = self.evaluate_generation()
                
                return results
            
            def calculate_ppl(self):
                total_loss = 0
                for sample in self.test_data:
                    inputs = self.tokenizer(sample["text"], return_tensors="pt")
                    outputs = self.model(**inputs, labels=inputs["input_ids"])
                    total_loss += outputs.loss.item()
                return math.exp(total_loss / len(self.test_data))
            
            def save_report(self, results, path):
                with open(path, "w") as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
        ```
    
    ç»ƒä¹  2ï¼šBenchmark æµ‹è¯•
        ä½¿ç”¨ lm-eval æµ‹è¯•æ¨¡å‹

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```bash
        # å®‰è£…
        pip install lm-eval
        
        # è¿è¡Œè¯„ä¼°
        lm_eval --model hf \\
            --model_args pretrained=./merged_model \\
            --tasks mmlu,hellaswag,arc_easy \\
            --batch_size 8 \\
            --output_path ./eval_results
        ```
        
        ```python
        # Python API
        from lm_eval import evaluator
        
        results = evaluator.simple_evaluate(
            model="hf",
            model_args="pretrained=./merged_model",
            tasks=["mmlu", "hellaswag", "arc_easy"],
            batch_size=8,
        )
        
        print("MMLU:", results["results"]["mmlu"]["acc"])
        print("HellaSwag:", results["results"]["hellaswag"]["acc_norm"])
        print("ARC-Easy:", results["results"]["arc_easy"]["acc"])
        ```
    
    ç»ƒä¹  3ï¼šè¯„ä¼°æŠ¥å‘Š
        ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        def generate_eval_report(model_name, train_info, eval_results):
            report = f'''
# å¾®è°ƒè¯„ä¼°æŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- æ¨¡å‹åç§°: {model_name}
- è®­ç»ƒæ•°æ®é‡: {train_info["num_samples"]}
- è®­ç»ƒè½®æ•°: {train_info["epochs"]}
- æœ€ç»ˆ Loss: {train_info["final_loss"]:.4f}

## è¯„ä¼°ç»“æœ

### ä»»åŠ¡æŒ‡æ ‡
| æŒ‡æ ‡ | åˆ†æ•° |
|------|------|
| å‡†ç¡®ç‡ | {eval_results["accuracy"]:.2%} |
| F1 åˆ†æ•° | {eval_results["f1"]:.2%} |
| å›°æƒ‘åº¦ | {eval_results["ppl"]:.2f} |

### Benchmark ç»“æœ
| Benchmark | åˆ†æ•° |
|-----------|------|
| MMLU | {eval_results.get("mmlu", "N/A")} |
| HellaSwag | {eval_results.get("hellaswag", "N/A")} |

## ç»“è®º
{generate_conclusion(eval_results)}
'''
            return report
        ```
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. å¦‚ä½•å¹³è¡¡ä»»åŠ¡æ€§èƒ½å’Œé€šç”¨èƒ½åŠ›ï¼Ÿ

       âœ… ç­”ï¼š
       - è®­ç»ƒæ•°æ®æ··åˆ: åŠ å…¥å°‘é‡é€šç”¨æ•°æ® (5-10%)
       - ç›‘æ§å¤šä¸ªæŒ‡æ ‡: åŒæ—¶è·Ÿè¸ªä»»åŠ¡å’Œé€šç”¨ benchmark
       - æ—©åœ: å½“é€šç”¨èƒ½åŠ›æ˜æ˜¾ä¸‹é™æ—¶åœæ­¢
       - è¾ƒä½å­¦ä¹ ç‡: å‡å°‘å¯¹åŸæœ‰çŸ¥è¯†çš„ç ´å
       - ä½¿ç”¨ LoRA: å‚æ•°é«˜æ•ˆæ–¹æ³•å½±å“æ›´å°

    2. è¯„ä¼°æŒ‡æ ‡å’Œå®é™…åº”ç”¨æ•ˆæœçš„å…³ç³»ï¼Ÿ

       âœ… ç­”ï¼š
       - æŒ‡æ ‡æ˜¯ä»£ç†æŒ‡æ ‡ï¼Œä¸æ˜¯æœ€ç»ˆç›®æ ‡
       - é«˜æŒ‡æ ‡ä¸ä¸€å®šç­‰äºå¥½çš„ç”¨æˆ·ä½“éªŒ
       - å»ºè®®:
         * ç»“åˆå®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°
         * è¿›è¡Œ A/B æµ‹è¯•æˆ–ç”¨æˆ·ç ”ç©¶
         * å…³æ³¨è¾¹ç¼˜æ¡ˆä¾‹å’Œå¤±è´¥æ¨¡å¼
         * å®šæœŸæ”¶é›†çœŸå®ç”¨æˆ·åé¦ˆ
    """)


def main():
    print("ğŸ“Š å¾®è°ƒæ•ˆæœè¯„ä¼°")
    print("=" * 60)
    evaluation_overview()
    training_metrics()
    task_evaluation()
    benchmark_eval()
    evaluation_workflow()
    exercises()
    print("\n" + "=" * 60)
    print("ğŸ‰ Phase 8 è¯¾ç¨‹å…¨éƒ¨å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
