"""
LoRA åŸç†ä¸å®ç°
===============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ LoRA çš„æ ¸å¿ƒåŸç†
    2. æŒæ¡ LoRA çš„å…³é”®å‚æ•°
    3. å­¦ä¼šä½¿ç”¨ LoRA å¾®è°ƒ

æ ¸å¿ƒæ¦‚å¿µï¼š
    - ä½ç§©åˆ†è§£
    - ç§© (rank)
    - Alpha ç¼©æ”¾å› å­

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install peft transformers torch
"""

import os
from dotenv import load_dotenv

load_dotenv()


def lora_principle():
    """LoRA åŸç†"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šLoRA æ ¸å¿ƒåŸç†")
    print("=" * 60)

    print("""
    LoRA: Low-Rank Adaptation
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    æ ¸å¿ƒæ€æƒ³ï¼šä¸æ›´æ–°åŸå§‹æƒé‡ï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ªä½ç§©çš„å¢é‡çŸ©é˜µ
    
    
    ä¼ ç»Ÿå¾®è°ƒ vs LoRA
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    ä¼ ç»Ÿå…¨é‡å¾®è°ƒ:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                         â”‚
    â”‚   W' = W + Î”W                                          â”‚
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
    â”‚   â”‚        Î”W         â”‚  â† æ›´æ–°æ•´ä¸ªçŸ©é˜µ                 â”‚
    â”‚   â”‚   (d Ã— d å‚æ•°)     â”‚     å¦‚ 4096 Ã— 4096 = 16M å‚æ•°  â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    LoRA å¾®è°ƒ:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                         â”‚
    â”‚   W' = W + BA                                          â”‚
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
    â”‚   â”‚  B  â”‚ Ã— â”‚        A        â”‚  â† ä½ç§©åˆ†è§£             â”‚
    â”‚   â”‚(dÃ—r)â”‚   â”‚      (rÃ—d)      â”‚                        â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
    â”‚                                                         â”‚
    â”‚   å‚æ•°é‡: dÃ—r + rÃ—d = 2Ã—dÃ—r                            â”‚
    â”‚   å¦‚ r=8: 2 Ã— 4096 Ã— 8 = 65K å‚æ•° (ä»… 0.4%)             â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    æ•°å­¦è¡¨ç¤º
    â”€â”€â”€â”€â”€â”€â”€â”€
    
    åŸå§‹å‰å‘ä¼ æ’­:  h = Wx
    LoRA å‰å‘ä¼ æ’­: h = Wx + BAx = (W + BA)x
    
    å…¶ä¸­:
    - W: åŸå§‹æƒé‡çŸ©é˜µ (å†»ç»“)
    - B: ä½ç§©çŸ©é˜µ (d Ã— r), åˆå§‹åŒ–ä¸º 0
    - A: ä½ç§©çŸ©é˜µ (r Ã— d), éšæœºåˆå§‹åŒ–
    - r: ç§© (rank), é€šå¸¸ 4-64
    """)


def lora_parameters():
    """LoRA å‚æ•°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šLoRA å…³é”®å‚æ•°")
    print("=" * 60)

    print("""
    å…³é”®å‚æ•°
    â”€â”€â”€â”€â”€â”€â”€
    
    1. rank (r) - ç§©
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  r = 4   â”‚  å‚æ•°æå°‘ï¼Œæ•ˆæœå¯èƒ½ä¸è¶³                   â”‚
       â”‚  r = 8   â”‚  å¸¸ç”¨é»˜è®¤å€¼ï¼Œå¹³è¡¡æ•ˆæœå’Œæ•ˆç‡               â”‚
       â”‚  r = 16  â”‚  è¾ƒå¥½æ•ˆæœ                                â”‚
       â”‚  r = 32  â”‚  æ¥è¿‘å…¨é‡å¾®è°ƒæ•ˆæœ                        â”‚
       â”‚  r = 64+ â”‚  å‚æ•°è¾ƒå¤šï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    2. lora_alpha (Î±) - ç¼©æ”¾å› å­
       å®é™…ç¼©æ”¾ = Î± / r
       
       å¸¸ç”¨è®¾ç½®:
       - alpha = rank (ç¼©æ”¾ = 1)
       - alpha = 2 * rank (ç¼©æ”¾ = 2)
    
    3. target_modules - ç›®æ ‡æ¨¡å—
       åº”ç”¨ LoRA çš„å±‚ï¼Œå¸¸è§é€‰æ‹©:
       - q_proj, v_proj (æ³¨æ„åŠ›å±‚)
       - q_proj, k_proj, v_proj, o_proj (å…¨éƒ¨æ³¨æ„åŠ›)
       - æ‰€æœ‰çº¿æ€§å±‚ (æ•ˆæœæœ€å¥½ï¼Œå‚æ•°æœ€å¤š)
    
    4. lora_dropout - Dropout æ¯”ä¾‹
       é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸ 0.05-0.1
    
    
    å‚æ•°é‡è®¡ç®—
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    å‡è®¾æ¨¡å‹æœ‰ L å±‚ï¼Œæ¯å±‚æœ‰ N ä¸ªç›®æ ‡æ¨¡å—ï¼Œç»´åº¦ä¸º d:
    
    LoRA å‚æ•°é‡ = L Ã— N Ã— 2 Ã— d Ã— r
    
    ç¤ºä¾‹ (7B æ¨¡å‹, 32 å±‚, 4 ä¸ªæ¨¡å—, d=4096, r=8):
    å‚æ•°é‡ = 32 Ã— 4 Ã— 2 Ã— 4096 Ã— 8 = 8.4M (ä»… 0.12%)
    """)


def lora_code_example():
    """LoRA ä»£ç ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šLoRA ä»£ç å®ç°")
    print("=" * 60)

    print("""
    ä½¿ç”¨ PEFT åº“å®ç° LoRA
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)

    code_example = """
    from peft import LoraConfig, get_peft_model, TaskType
    from transformers import AutoModelForCausalLM, AutoTokenizer

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    model_name = "meta-llama/Llama-2-7b-hf"
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # 2. é…ç½® LoRA
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,                          # ç§©
        lora_alpha=16,                # ç¼©æ”¾å› å­
        lora_dropout=0.05,            # Dropout
        target_modules=[              # ç›®æ ‡æ¨¡å—
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
        ],
        bias="none",                  # æ˜¯å¦è®­ç»ƒ bias
    )

    # 3. åˆ›å»º PEFT æ¨¡å‹
    peft_model = get_peft_model(model, lora_config)

    # 4. æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
    peft_model.print_trainable_parameters()
    # è¾“å‡º: trainable params: 4,194,304 || all params: 6,742,609,920 || 0.06%

    # 5. è®­ç»ƒ (ä½¿ç”¨ Trainer æˆ–æ‰‹åŠ¨è®­ç»ƒ)
    from transformers import TrainingArguments, Trainer

    training_args = TrainingArguments(
        output_dir="./lora_output",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        learning_rate=1e-4,
        fp16=True,
        logging_steps=10,
        save_steps=100,
    )

    trainer = Trainer(
        model=peft_model,
        args=training_args,
        train_dataset=train_dataset,
    )

    trainer.train()

    # 6. ä¿å­˜ LoRA æƒé‡
    peft_model.save_pretrained("./lora_weights")

    # 7. åŠ è½½ LoRA æƒé‡
    from peft import PeftModel

    base_model = AutoModelForCausalLM.from_pretrained(model_name)
    peft_model = PeftModel.from_pretrained(base_model, "./lora_weights")

    # 8. åˆå¹¶æƒé‡ (å¯é€‰)
    merged_model = peft_model.merge_and_unload()
    merged_model.save_pretrained("./merged_model")
    """

    print(code_example)


def lora_tips():
    """LoRA ä½¿ç”¨æŠ€å·§"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šLoRA ä½¿ç”¨æŠ€å·§")
    print("=" * 60)

    print("""
    æœ€ä½³å®è·µ
    â”€â”€â”€â”€â”€â”€â”€
    
    1. Rank é€‰æ‹©
       - ç®€å•ä»»åŠ¡: r=4-8
       - å¤æ‚ä»»åŠ¡: r=16-32
       - ä¸ç¡®å®šæ—¶: ä» r=8 å¼€å§‹å°è¯•
    
    2. ç›®æ ‡æ¨¡å—é€‰æ‹©
       - æœ€å°: q_proj, v_proj
       - æ¨è: q_proj, k_proj, v_proj, o_proj
       - å…¨é¢: æ‰€æœ‰çº¿æ€§å±‚
    
    3. å­¦ä¹ ç‡
       - é€šå¸¸æ¯”å…¨é‡å¾®è°ƒé«˜: 1e-4 åˆ° 3e-4
       - å¯ä»¥ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    4. è®­ç»ƒè½®æ•°
       - é€šå¸¸ 1-3 è½®å³å¯
       - ç›‘æ§éªŒè¯é›† lossï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    
    
    å¸¸è§é—®é¢˜
    â”€â”€â”€â”€â”€â”€â”€
    
    Q: LoRA æ•ˆæœä¸å¦‚å…¨é‡å¾®è°ƒæ€ä¹ˆåŠï¼Ÿ
    A: - å¢åŠ  rank
       - å¢åŠ ç›®æ ‡æ¨¡å—
       - å¢åŠ è®­ç»ƒæ•°æ®
       - è°ƒæ•´å­¦ä¹ ç‡
    
    Q: è®­ç»ƒä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ
    A: - é™ä½å­¦ä¹ ç‡
       - å¢åŠ  warmup
       - ä½¿ç”¨æ¢¯åº¦è£å‰ª
    
    Q: å¤šä¸ª LoRA å¦‚ä½•ç»„åˆï¼Ÿ
    A: - åˆ†åˆ«è®­ç»ƒï¼Œæ¨ç†æ—¶åˆ‡æ¢
       - ä½¿ç”¨ LoRA åˆå¹¶æŠ€æœ¯
    """)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå‚æ•°è®¡ç®—
        è®¡ç®— 13B æ¨¡å‹ä½¿ç”¨ LoRA (r=16) çš„å‚æ•°é‡
    
    ç»ƒä¹  2ï¼šé…ç½®ä¼˜åŒ–
        è®¾è®¡ä¸€ä¸ª LoRA é…ç½®ï¼Œæƒè¡¡æ•ˆæœå’Œè®­ç»ƒé€Ÿåº¦
    
    ç»ƒä¹  3ï¼šå®éªŒå¯¹æ¯”
        æ¯”è¾ƒä¸åŒ rank å€¼çš„è®­ç»ƒæ•ˆæœ
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. ä¸ºä»€ä¹ˆ LoRA é€‰æ‹©åœ¨æ³¨æ„åŠ›å±‚åº”ç”¨ï¼Ÿ
    2. LoRA çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ
    """)


def main():
    print("ğŸ”§ LoRA åŸç†ä¸å®ç°")
    print("=" * 60)
    lora_principle()
    lora_parameters()
    lora_code_example()
    lora_tips()
    exercises()
    print("\nâœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š05-qlora.py")


if __name__ == "__main__":
    main()
