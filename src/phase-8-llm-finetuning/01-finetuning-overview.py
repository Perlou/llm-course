"""
å¾®è°ƒæ–¹æ³•æ¦‚è¿°
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ä»€ä¹ˆæ˜¯ LLM å¾®è°ƒ
    2. æŒæ¡å¾®è°ƒçš„ç±»å‹å’Œæ–¹æ³•
    3. äº†è§£å¾®è°ƒçš„åº”ç”¨åœºæ™¯

æ ¸å¿ƒæ¦‚å¿µï¼š
    - é¢„è®­ç»ƒ vs å¾®è°ƒ
    - å…¨é‡å¾®è°ƒ vs å‚æ•°é«˜æ•ˆå¾®è°ƒ
    - æŒ‡ä»¤å¾®è°ƒä¸å¯¹é½

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install transformers torch
"""

import os
from dotenv import load_dotenv

load_dotenv()


def finetuning_overview():
    """å¾®è°ƒæ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šä»€ä¹ˆæ˜¯ LLM å¾®è°ƒ")
    print("=" * 60)

    print("""
    LLM è®­ç»ƒæµç¨‹
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   é¢„è®­ç»ƒ    â”‚â”€â”€â”€â–¶â”‚    å¾®è°ƒ     â”‚â”€â”€â”€â–¶â”‚    å¯¹é½    â”‚
    â”‚Pre-trainingâ”‚    â”‚Fine-tuning â”‚    â”‚ Alignment  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                 â”‚
         â–¼                 â–¼                 â–¼
    æµ·é‡æ— æ ‡æ³¨æ•°æ®     é¢†åŸŸ/ä»»åŠ¡æ•°æ®     äººç±»åå¥½æ•°æ®
    (TBçº§åˆ«)          (GBçº§åˆ«)          (MBçº§åˆ«)
    
    
    ä»€ä¹ˆæ˜¯å¾®è°ƒï¼Ÿ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    å¾®è°ƒ (Fine-tuning) æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡çš„æ•°æ®
    ç»§ç»­è®­ç»ƒï¼Œä½¿æ¨¡å‹é€‚åº”ç›®æ ‡ä»»åŠ¡çš„è¿‡ç¨‹ã€‚
    
    é¢„è®­ç»ƒï¼šå­¦ä¹ è¯­è¨€çš„é€šç”¨çŸ¥è¯†
    å¾®è°ƒï¼šå­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¸“ä¸šçŸ¥è¯†
    
    
    ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒï¼Ÿ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    âœ… é¢†åŸŸé€‚åº”ï¼šè®©æ¨¡å‹ç†è§£ä¸“ä¸šæœ¯è¯­ï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰
    âœ… ä»»åŠ¡ä¼˜åŒ–ï¼šæå‡ç‰¹å®šä»»åŠ¡æ€§èƒ½ï¼ˆé—®ç­”ã€æ‘˜è¦ã€ä»£ç ï¼‰
    âœ… é£æ ¼æ§åˆ¶ï¼šè°ƒæ•´è¾“å‡ºé£æ ¼ï¼ˆæ­£å¼ã€å¹½é»˜ã€ç®€æ´ï¼‰
    âœ… æˆæœ¬ä¼˜åŒ–ï¼šå°æ¨¡å‹+å¾®è°ƒ å¯èƒ½ä¼˜äº å¤§æ¨¡å‹+æç¤ºå·¥ç¨‹
    âœ… éšç§ä¿æŠ¤ï¼šå¯åœ¨æœ¬åœ°æ•°æ®ä¸Šè®­ç»ƒï¼Œä¸æ³„éœ²æ•æ„Ÿä¿¡æ¯
    """)


def finetuning_types():
    """å¾®è°ƒç±»å‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå¾®è°ƒç±»å‹")
    print("=" * 60)

    print("""
    å¾®è°ƒæ–¹æ³•åˆ†ç±»
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    å¾®è°ƒæ–¹æ³•                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚   å…¨é‡å¾®è°ƒ       â”‚    â”‚    å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT)   â”‚   â”‚
    â”‚   â”‚ Full Fine-tuningâ”‚    â”‚                         â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”â”‚   â”‚
    â”‚          â”‚               â”‚  â”‚ LoRAâ”‚ â”‚Prefixâ”‚ â”‚IAÂ³ â”‚â”‚   â”‚
    â”‚          â–¼               â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜â”‚   â”‚
    â”‚   æ›´æ–°æ‰€æœ‰å‚æ•°           â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
    â”‚   èµ„æºæ¶ˆè€—å¤§             â”‚  â”‚QLoRAâ”‚ â”‚ Adapters  â”‚ â”‚   â”‚
    â”‚   æ•ˆæœæœ€å¥½               â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
    â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                     â”‚                   â”‚
    â”‚                                     â–¼                   â”‚
    â”‚                            åªæ›´æ–°å°‘é‡å‚æ•°               â”‚
    â”‚                            èµ„æºæ¶ˆè€—å°                   â”‚
    â”‚                            æ•ˆæœæ¥è¿‘å…¨é‡                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    1. å…¨é‡å¾®è°ƒ (Full Fine-tuning)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       - æ›´æ–°æ¨¡å‹æ‰€æœ‰å‚æ•°
       - éœ€è¦å¤§é‡ GPU æ˜¾å­˜
       - æ•ˆæœæœ€å¥½
       - é€‚ç”¨ï¼šèµ„æºå……è¶³ï¼Œè¿½æ±‚æœ€ä½³æ•ˆæœ
    
    2. å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       - åªæ›´æ–°å°‘é‡å‚æ•°ï¼ˆ0.1%-10%ï¼‰
       - æ˜¾å­˜éœ€æ±‚å¤§å¹…é™ä½
       - æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒ
       - é€‚ç”¨ï¼šèµ„æºæœ‰é™ï¼Œå¿«é€Ÿè¿­ä»£
    
    3. æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       - ä½¿ç”¨æŒ‡ä»¤æ ¼å¼çš„æ•°æ®
       - æå‡æ¨¡å‹éµå¾ªæŒ‡ä»¤èƒ½åŠ›
       - é€‚ç”¨ï¼šæ„å»ºå¯¹è¯/åŠ©æ‰‹æ¨¡å‹
    
    4. å¯¹é½è®­ç»ƒ (Alignment)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       - RLHFï¼šåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ 
       - DPOï¼šç›´æ¥åå¥½ä¼˜åŒ–
       - é€‚ç”¨ï¼šè®©æ¨¡å‹è¾“å‡ºæ›´ç¬¦åˆäººç±»æœŸæœ›
    """)


def finetuning_comparison():
    """å¾®è°ƒæ–¹æ³•å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¾®è°ƒæ–¹æ³•å¯¹æ¯”")
    print("=" * 60)

    print("""
    æ–¹æ³•å¯¹æ¯”
    â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    æ–¹æ³•     â”‚  å‚æ•°é‡     â”‚   æ˜¾å­˜éœ€æ±‚   â”‚    æ•ˆæœ     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å…¨é‡å¾®è°ƒ    â”‚   100%      â”‚   éå¸¸é«˜    â”‚    æœ€å¥½     â”‚
    â”‚ LoRA        â”‚   0.1-1%    â”‚   ä½        â”‚    å¾ˆå¥½     â”‚
    â”‚ QLoRA       â”‚   0.1-1%    â”‚   æœ€ä½      â”‚    è¾ƒå¥½     â”‚
    â”‚ Prefix      â”‚   0.1%      â”‚   ä½        â”‚    ä¸€èˆ¬     â”‚
    â”‚ Prompt      â”‚   <0.1%     â”‚   æœ€ä½      â”‚    ä¸€èˆ¬     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    é€‰æ‹©å»ºè®®
    â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     é€‰æ‹©å†³ç­–æ ‘                          â”‚
    â”‚                                                         â”‚
    â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
    â”‚             â”‚ æœ‰è¶³å¤Ÿ GPU èµ„æºï¼Ÿ â”‚                        â”‚
    â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
    â”‚                 â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                             â”‚
    â”‚                æ˜¯         å¦                             â”‚
    â”‚                 â”‚          â”‚                             â”‚
    â”‚                 â–¼          â–¼                             â”‚
    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚          â”‚è¿½æ±‚æœ€ä½³æ•ˆæœâ”‚ â”‚ å•å¡è®­ç»ƒï¼Ÿâ”‚                      â”‚
    â”‚          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚            â”Œâ”€â”€â”´â”€â”€â”     â”Œâ”€â”€â”€â”´â”€â”€â”€â”                        â”‚
    â”‚           æ˜¯    å¦    æ˜¯      å¦                         â”‚
    â”‚            â”‚     â”‚     â”‚       â”‚                        â”‚
    â”‚            â–¼     â–¼     â–¼       â–¼                        â”‚
    â”‚        å…¨é‡å¾®è°ƒ LoRA  QLoRA  DeepSpeed                   â”‚
    â”‚                                +LoRA                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def application_scenarios():
    """åº”ç”¨åœºæ™¯"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šå¾®è°ƒåº”ç”¨åœºæ™¯")
    print("=" * 60)

    print("""
    å¸¸è§åº”ç”¨åœºæ™¯
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. é¢†åŸŸä¸“å®¶æ¨¡å‹
       - åŒ»ç–—åŠ©æ‰‹ï¼šç†è§£åŒ»å­¦æœ¯è¯­ï¼Œæä¾›å¥åº·å»ºè®®
       - æ³•å¾‹é¡¾é—®ï¼šè§£è¯»æ³•å¾‹æ¡æ¬¾ï¼Œæä¾›æ³•å¾‹æ„è§
       - é‡‘èåˆ†æï¼šåˆ†æè´¢æŠ¥ï¼Œé¢„æµ‹å¸‚åœºè¶‹åŠ¿
    
    2. ä¼ä¸šç§æœ‰åŠ©æ‰‹
       - å®¢æœæœºå™¨äººï¼šåŸºäºä¼ä¸šçŸ¥è¯†åº“å›ç­”é—®é¢˜
       - å†…éƒ¨é—®ç­”ï¼šç†è§£ä¼ä¸šæ–‡æ¡£å’Œæµç¨‹
       - ä»£ç åŠ©æ‰‹ï¼šç†è§£ä¼ä¸šä»£ç è§„èŒƒ
    
    3. ç‰¹å®šä»»åŠ¡ä¼˜åŒ–
       - æ–‡æœ¬åˆ†ç±»ï¼šæƒ…æ„Ÿåˆ†æã€æ„å›¾è¯†åˆ«
       - ä¿¡æ¯æŠ½å–ï¼šå‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–
       - æ–‡æœ¬ç”Ÿæˆï¼šæ‘˜è¦ã€ç¿»è¯‘ã€åˆ›ä½œ
    
    4. é£æ ¼/äººè®¾å®šåˆ¶
       - è§’è‰²æ‰®æ¼”ï¼šç‰¹å®šäººç‰©çš„è¯´è¯é£æ ¼
       - å“ç‰Œå£°éŸ³ï¼šç¬¦åˆå“ç‰Œè°ƒæ€§çš„å›å¤
       - æ•™å­¦é£æ ¼ï¼šé€‚åˆç‰¹å®šå—ä¼—çš„è®²è§£
    
    
    ä½•æ—¶é€‰æ‹©å¾®è°ƒ vs æç¤ºå·¥ç¨‹ï¼Ÿ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    é€‰æ‹©æç¤ºå·¥ç¨‹ï¼š
    - éœ€æ±‚å˜åŒ–å¿«
    - æ•°æ®é‡å°ï¼ˆ< 100 æ¡ï¼‰
    - èµ„æºé™åˆ¶å¤§
    
    é€‰æ‹©å¾®è°ƒï¼š
    - éœ€æ±‚ç›¸å¯¹ç¨³å®š
    - æœ‰è¶³å¤Ÿæ•°æ®ï¼ˆ> 1000 æ¡ï¼‰
    - éœ€è¦æ›´å¥½çš„æ€§èƒ½
    - éœ€è¦é™ä½æ¨ç†æˆæœ¬
    """)


def simple_example():
    """ç®€å•ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå¾®è°ƒä»£ç ç¤ºä¾‹")
    print("=" * 60)

    print("""
    ä½¿ç”¨ Transformers å¾®è°ƒçš„åŸºæœ¬æµç¨‹
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)

    code_example = """
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        TrainingArguments,
        Trainer
    )
    from datasets import load_dataset

    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "meta-llama/Llama-2-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # 2. å‡†å¤‡æ•°æ®é›†
    dataset = load_dataset("your_dataset")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=512,
            padding="max_length"
        )

    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    # 3. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        learning_rate=2e-5,
        save_steps=500,
        logging_steps=100,
    )

    # 4. åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation"],
    )

    # 5. å¼€å§‹è®­ç»ƒ
    trainer.train()

    # 6. ä¿å­˜æ¨¡å‹
    trainer.save_model("./finetuned_model")
    """

    print(code_example)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šåˆ†æåœºæ™¯
        ç»™å®šä»¥ä¸‹åœºæ™¯ï¼Œåˆ¤æ–­åº”è¯¥ä½¿ç”¨ä»€ä¹ˆæ–¹æ³•ï¼š
        - æ„å»ºæ³•å¾‹å’¨è¯¢åŠ©æ‰‹
        - ä¸´æ—¶æ·»åŠ æ–°äº§å“çŸ¥è¯†
        - åœ¨æ¶ˆè´¹çº§ GPU ä¸Šè®­ç»ƒ
    
    ç»ƒä¹  2ï¼šèµ„æºä¼°ç®—
        å¯¹äº 7B å‚æ•°æ¨¡å‹ï¼š
        - å…¨é‡å¾®è°ƒéœ€è¦å¤šå°‘æ˜¾å­˜ï¼Ÿ
        - LoRA å¾®è°ƒéœ€è¦å¤šå°‘æ˜¾å­˜ï¼Ÿ
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. å¾®è°ƒæ˜¯å¦æ€»æ˜¯æ¯”æç¤ºå·¥ç¨‹æ•ˆæœå¥½ï¼Ÿ
       ç­”ï¼šä¸ä¸€å®šã€‚æ•°æ®è´¨é‡ã€æ•°æ®é‡ã€ä»»åŠ¡å¤æ‚åº¦éƒ½ä¼šå½±å“ç»“æœã€‚
    
    2. å¾®è°ƒåçš„æ¨¡å‹ä¼š"å¿˜è®°"åŸæœ‰çŸ¥è¯†å—ï¼Ÿ
       ç­”ï¼šå¯èƒ½ä¼šå‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œéœ€è¦é€šè¿‡æ­£åˆ™åŒ–æˆ–æ··åˆæ•°æ®ç¼“è§£ã€‚
    """)


def main():
    print("ğŸ“š å¾®è°ƒæ–¹æ³•æ¦‚è¿°")
    print("=" * 60)
    finetuning_overview()
    finetuning_types()
    finetuning_comparison()
    application_scenarios()
    simple_example()
    exercises()
    print("\nâœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š02-dataset-preparation.py")


if __name__ == "__main__":
    main()
