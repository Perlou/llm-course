"""
ç›‘ç£å¾®è°ƒ (SFT)
==============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ SFT çš„åŸç†
    2. ä½¿ç”¨ TRL è¿›è¡Œ SFT
    3. æŒæ¡ SFT æœ€ä½³å®è·µ

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Supervised Fine-Tuning
    - SFTTrainer
    - Chat Template

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install trl transformers peft
"""

import os
from dotenv import load_dotenv

load_dotenv()


def sft_overview():
    """SFT æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šSFT æ¦‚è¿°")
    print("=" * 60)

    print("""
    Supervised Fine-Tuning (SFT)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    ä½¿ç”¨æ ‡æ³¨çš„æŒ‡ä»¤-å›ç­”å¯¹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œæ˜¯è®­ç»ƒå¯¹è¯æ¨¡å‹çš„æ ¸å¿ƒæ­¥éª¤ã€‚
    
    
    è®­ç»ƒæµç¨‹
    â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    SFT è®­ç»ƒæµç¨‹                          â”‚
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
    â”‚   â”‚  é¢„è®­ç»ƒ  â”‚â”€â”€â”€â–¶â”‚   SFT   â”‚â”€â”€â”€â–¶â”‚  å¯¹é½   â”‚            â”‚
    â”‚   â”‚  æ¨¡å‹   â”‚    â”‚  è®­ç»ƒ   â”‚    â”‚ (å¯é€‰)  â”‚            â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
    â”‚                       â”‚                                 â”‚
    â”‚                       â–¼                                 â”‚
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
    â”‚              â”‚  Instruction    â”‚                        â”‚
    â”‚              â”‚    Dataset      â”‚                        â”‚
    â”‚              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                        â”‚
    â”‚              â”‚ â”‚ æŒ‡ä»¤ + å›ç­”  â”‚ â”‚                        â”‚
    â”‚              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                        â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    SFT çš„ç›®æ ‡
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    æœ€å¤§åŒ– P(å›ç­” | æŒ‡ä»¤)
    
    å³ï¼šè®©æ¨¡å‹å­¦ä¼šæ ¹æ®æŒ‡ä»¤ç”Ÿæˆç¬¦åˆæœŸæœ›çš„å›ç­”
    """)


def chat_template():
    """Chat Template"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šChat Template")
    print("=" * 60)

    print("""
    èŠå¤©æ¨¡æ¿
    â”€â”€â”€â”€â”€â”€â”€
    
    ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„å¯¹è¯æ ¼å¼ï¼Œéœ€è¦æ­£ç¡®åº”ç”¨æ¨¡æ¿ã€‚
    
    
    å¸¸è§æ ¼å¼
    â”€â”€â”€â”€â”€â”€â”€
    
    1. Llama 2 æ ¼å¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ <s>[INST] <<SYS>>                                       â”‚
    â”‚ ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚                                    â”‚
    â”‚ <</SYS>>                                                 â”‚
    â”‚                                                         â”‚
    â”‚ ä½ å¥½ï¼ [/INST] ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ </s>           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    2. ChatML æ ¼å¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ <|im_start|>system                                      â”‚
    â”‚ ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚<|im_end|>                          â”‚
    â”‚ <|im_start|>user                                        â”‚
    â”‚ ä½ å¥½ï¼<|im_end|>                                         â”‚
    â”‚ <|im_start|>assistant                                   â”‚
    â”‚ ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ<|im_end|>                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    3. Alpaca æ ¼å¼:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ### Instruction:                                        â”‚
    â”‚ {instruction}                                           â”‚
    â”‚                                                         â”‚
    â”‚ ### Input:                                              â”‚
    â”‚ {input}                                                 â”‚
    â”‚                                                         â”‚
    â”‚ ### Response:                                           â”‚
    â”‚ {output}                                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def sft_with_trl():
    """ä½¿ç”¨ TRL è¿›è¡Œ SFT"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ TRL è¿›è¡Œ SFT")
    print("=" * 60)

    print("""
    TRL (Transformer Reinforcement Learning)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    HuggingFace çš„è®­ç»ƒåº“ï¼Œç®€åŒ– SFT å’Œå¯¹é½è®­ç»ƒã€‚
    """)

    code_example = '''
    from trl import SFTTrainer, SFTConfig
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset
    from peft import LoraConfig

    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "meta-llama/Llama-2-7b-hf"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. åŠ è½½æ•°æ®é›†
    dataset = load_dataset("tatsu-lab/alpaca", split="train")

    # 3. æ•°æ®æ ¼å¼åŒ–å‡½æ•°
    def formatting_prompts_func(examples):
        output_texts = []
        for i in range(len(examples["instruction"])):
            instruction = examples["instruction"][i]
            input_text = examples["input"][i]
            output = examples["output"][i]
            
            if input_text:
                text = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
            else:
                text = f"""### Instruction:
{instruction}

### Response:
{output}"""
            
            output_texts.append(text)
        return output_texts

    # 4. LoRA é…ç½® (å¯é€‰)
    peft_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],
    )

    # 5. è®­ç»ƒé…ç½®
    training_args = SFTConfig(
        output_dir="./sft_output",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        save_steps=100,
        max_seq_length=512,
    )

    # 6. åˆ›å»º SFTTrainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        formatting_func=formatting_prompts_func,
        tokenizer=tokenizer,
        peft_config=peft_config,
    )

    # 7. å¼€å§‹è®­ç»ƒ
    trainer.train()

    # 8. ä¿å­˜æ¨¡å‹
    trainer.save_model("./sft_model")
    '''

    print(code_example)


def sft_tips():
    """SFT æŠ€å·§"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šSFT æœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
    æœ€ä½³å®è·µ
    â”€â”€â”€â”€â”€â”€â”€
    
    1. æ•°æ®å‡†å¤‡
       - ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
       - æ³¨æ„ EOS token çš„æ·»åŠ 
       - åªå¯¹å›ç­”éƒ¨åˆ†è®¡ç®— loss
    
    2. è®­ç»ƒå‚æ•°
       - å­¦ä¹ ç‡: 1e-4 åˆ° 3e-4
       - è½®æ•°: é€šå¸¸ 1-3 è½®
       - æ‰¹æ¬¡å¤§å°: æ ¹æ®æ˜¾å­˜è°ƒæ•´
    
    3. æ¨¡æ¿é€‰æ‹©
       - ä½¿ç”¨æ¨¡å‹åŸç”Ÿæ¨¡æ¿
       - æˆ–ä½¿ç”¨é€šç”¨æ¨¡æ¿å¦‚ ChatML
    
    4. æ•°æ®è´¨é‡
       - è´¨é‡ > æ•°é‡
       - å¤šæ ·æ€§å¾ˆé‡è¦
       - æ¸…æ´—ä½è´¨é‡æ ·æœ¬
    
    
    å¸¸è§é—®é¢˜
    â”€â”€â”€â”€â”€â”€â”€
    
    Q: æ¨¡å‹ä¸éµå¾ªæŒ‡ä»¤æ€ä¹ˆåŠï¼Ÿ
    A: - å¢åŠ æ•°æ®é‡å’Œå¤šæ ·æ€§
       - æ£€æŸ¥æ¨¡æ¿æ ¼å¼
       - å¢åŠ è®­ç»ƒè½®æ•°
    
    Q: æ¨¡å‹å›å¤å¤ªé•¿/å¤ªçŸ­ï¼Ÿ
    A: - è°ƒæ•´æ•°æ®ä¸­çš„å›å¤é•¿åº¦åˆ†å¸ƒ
       - ä½¿ç”¨é•¿åº¦æ§åˆ¶token
    
    Q: æ¨¡å‹å¿˜è®°é¢„è®­ç»ƒçŸ¥è¯†ï¼Ÿ
    A: - é™ä½å­¦ä¹ ç‡
       - å‡å°‘è®­ç»ƒè½®æ•°
       - æ··å…¥é¢„è®­ç»ƒæ•°æ®
    """)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç° SFT
        ä½¿ç”¨ TRL è®­ç»ƒä¸€ä¸ªç®€å•çš„é—®ç­”æ¨¡å‹

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from trl import SFTTrainer, SFTConfig
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from datasets import load_dataset
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B")
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B")
        tokenizer.pad_token = tokenizer.eos_token
        
        # å‡†å¤‡æ•°æ®
        dataset = load_dataset("json", data_files="qa_data.jsonl")
        
        def format_prompt(example):
            return f"é—®é¢˜: {example['question']}; å›ç­”: {example['answer']}"
        
        # é…ç½®è®­ç»ƒ
        config = SFTConfig(
            output_dir="./sft_qa_model",
            max_seq_length=256,
            num_train_epochs=3,
            learning_rate=2e-4,
        )
        
        # è®­ç»ƒ
        trainer = SFTTrainer(
            model=model,
            args=config,
            train_dataset=dataset["train"],
        )
        trainer.train()
        ```
    
    ç»ƒä¹  2ï¼šæ¨¡æ¿è®¾è®¡
        è®¾è®¡ä¸€ä¸ªé€‚åˆä½ ä»»åŠ¡çš„å¯¹è¯æ¨¡æ¿

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        # å®¢æœåœºæ™¯æ¨¡æ¿
        def format_customer_service(sample):
            return f'''<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œè¯·ç¤¼è²Œã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚<|im_end|>
<|im_start|>user
{sample["question"]}<|im_end|>
<|im_start|>assistant
{sample["answer"]}<|im_end|>'''
        
        # ä»£ç åŠ©æ‰‹æ¨¡æ¿
        def format_code_assistant(sample):
            return f'''### Instruction:
{sample["instruction"]}

### Response:
{sample["code"]}'''
        ```
    
    ç»ƒä¹  3ï¼šæ•°æ®å¢å¼º
        æ‰©å±•è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        def augment_sft_data(dataset, llm):
            augmented = []
            for sample in dataset:
                augmented.append(sample)  # åŸå§‹æ•°æ®
                
                # æ”¹å†™æŒ‡ä»¤
                new_instruction = llm.invoke(
                    f"æ”¹å†™è¿™ä¸ªæŒ‡ä»¤ï¼Œä¿æŒæ„æ€ä¸å˜: {sample['instruction']}"
                ).content
                augmented.append({
                    "instruction": new_instruction,
                    "output": sample["output"]
                })
                
                # æ”¹å†™å›ç­”é£æ ¼
                formal_output = llm.invoke(
                    f"ç”¨æ›´æ­£å¼çš„è¯­æ°”æ”¹å†™: {sample['output']}"
                ).content
                augmented.append({
                    "instruction": sample["instruction"],
                    "output": formal_output
                })
            
            return augmented
        ```
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. å¦‚ä½•è¯„ä¼° SFT çš„æ•ˆæœï¼Ÿ

       âœ… ç­”ï¼š
       - è®­ç»ƒæŒ‡æ ‡: Loss/PPL è¶‹åŠ¿
       - ä»»åŠ¡æŒ‡æ ‡: å‡†ç¡®ç‡ã€ROUGEã€BLEU ç­‰
       - äººå·¥è¯„ä¼°: æŒ‡ä»¤éµå¾ªåº¦ã€å›ç­”è´¨é‡
       - GPT-4 è¯„ä¼°: ä½¿ç”¨ GPT-4 ä½œä¸ºè¯„åˆ¤
       - A/B æµ‹è¯•: ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”

    2. SFT å’Œ RLHF çš„å…³ç³»ï¼Ÿ

       âœ… ç­”ï¼š
       - SFT æ˜¯ RLHF çš„å‰ç½®æ­¥éª¤
       - SFT: å­¦ä¹ å¦‚ä½•å›ç­” (æ¨¡ä»¿å­¦ä¹ )
       - RLHF: å­¦ä¹ å¦‚ä½•æ›´å¥½åœ°å›ç­” (å¼ºåŒ–å­¦ä¹ )
       - æµç¨‹: é¢„è®­ç»ƒ -> SFT -> RLHF/DPO
       - SFT æ•ˆæœå¥½æ—¶ï¼ŒRLHF æ”¶ç›Šé€’å‡
    """)


def main():
    print("ğŸ“š ç›‘ç£å¾®è°ƒ (SFT)")
    print("=" * 60)
    sft_overview()
    chat_template()
    sft_with_trl()
    sft_tips()
    exercises()
    print("\nâœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š08-dpo-training.py")


if __name__ == "__main__":
    main()
