"""
PEFT åº“ä½¿ç”¨
===========

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡ PEFT åº“çš„ä½¿ç”¨
    2. äº†è§£å¤šç§ PEFT æ–¹æ³•
    3. å­¦ä¼šä¿å­˜å’ŒåŠ è½½ PEFT æ¨¡å‹

æ ¸å¿ƒæ¦‚å¿µï¼š
    - PEFT (Parameter-Efficient Fine-Tuning)
    - Adapters
    - Prefix Tuning

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install peft transformers
"""

import os
from dotenv import load_dotenv

load_dotenv()


def peft_overview():
    """PEFT æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šPEFT åº“æ¦‚è¿°")
    print("=" * 60)

    print("""
    PEFT: Parameter-Efficient Fine-Tuning
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    HuggingFace å¼€å‘çš„ç»Ÿä¸€ PEFT æ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ–¹æ³•ã€‚
    
    
    æ”¯æŒçš„æ–¹æ³•
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    PEFT æ–¹æ³•                             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚  LoRA   â”‚  â”‚ AdaLoRA â”‚  â”‚  IAÂ³    â”‚  â”‚Prefix   â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚Tuning   â”‚   â”‚
    â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚ Prompt  â”‚  â”‚  P-     â”‚  â”‚ (IA)Â³   â”‚  â”‚LoHa/    â”‚   â”‚
    â”‚   â”‚ Tuning  â”‚  â”‚ Tuning  â”‚  â”‚         â”‚  â”‚LoKR     â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    æ–¹æ³•å¯¹æ¯”
    â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    æ–¹æ³•     â”‚  å‚æ•°æ•ˆç‡   â”‚    æ•ˆæœ    â”‚   å¤æ‚åº¦   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   LoRA      â”‚   é«˜       â”‚    å¥½      â”‚    ä½      â”‚
    â”‚   AdaLoRA   â”‚   æ›´é«˜     â”‚    å¥½      â”‚    ä¸­      â”‚
    â”‚   IAÂ³       â”‚   æœ€é«˜     â”‚    ä¸€èˆ¬    â”‚    ä½      â”‚
    â”‚   Prefix    â”‚   é«˜       â”‚    ä¸€èˆ¬    â”‚    ä¸­      â”‚
    â”‚   Prompt    â”‚   æœ€é«˜     â”‚    ä¸€èˆ¬    â”‚    ä½      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def lora_adapter():
    """LoRA Adapter"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šLoRA Adapter")
    print("=" * 60)

    code_example = """
    from peft import LoraConfig, get_peft_model, TaskType
    from transformers import AutoModelForCausalLM

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("model_name")

    # LoRA é…ç½®
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"],
    )

    # åº”ç”¨ LoRA
    peft_model = get_peft_model(model, config)
    """

    print("ğŸ“Œ LoRA é…ç½®ç¤ºä¾‹ï¼š")
    print(code_example)


def other_methods():
    """å…¶ä»– PEFT æ–¹æ³•"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå…¶ä»– PEFT æ–¹æ³•")
    print("=" * 60)

    print("""
    1. Prefix Tuning
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    åœ¨æ¯å±‚æ·»åŠ å¯è®­ç»ƒçš„å‰ç¼€å‘é‡
    """)

    prefix_code = """
    from peft import PrefixTuningConfig

    config = PrefixTuningConfig(
        task_type="CAUSAL_LM",
        num_virtual_tokens=20,  # å‰ç¼€é•¿åº¦
        prefix_projection=True,
    )
    """
    print(prefix_code)

    print("""
    2. Prompt Tuning
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    åªåœ¨è¾“å…¥åµŒå…¥å±‚æ·»åŠ å¯è®­ç»ƒçš„è½¯æç¤º
    """)

    prompt_code = """
    from peft import PromptTuningConfig

    config = PromptTuningConfig(
        task_type="CAUSAL_LM",
        num_virtual_tokens=8,
        prompt_tuning_init="TEXT",      # åˆå§‹åŒ–æ–¹å¼
        prompt_tuning_init_text="åˆ†ç±»è¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿï¼š",
    )
    """
    print(prompt_code)

    print("""
    3. IAÂ³ (Infused Adapter by Inhibiting and Amplifying)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    é€šè¿‡å­¦ä¹ ç¼©æ”¾å‘é‡è°ƒæ•´æ¿€æ´»å€¼
    """)

    ia3_code = """
    from peft import IA3Config

    config = IA3Config(
        task_type="CAUSAL_LM",
        target_modules=["k_proj", "v_proj", "down_proj"],
        feedforward_modules=["down_proj"],
    )
    """
    print(ia3_code)


def model_management():
    """æ¨¡å‹ç®¡ç†"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹ä¿å­˜ä¸åŠ è½½")
    print("=" * 60)

    print("""
    ä¿å­˜å’ŒåŠ è½½ PEFT æ¨¡å‹
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)

    code_example = """
    from peft import PeftModel, PeftConfig
    from transformers import AutoModelForCausalLM

    # ============ ä¿å­˜ ============

    # 1. åªä¿å­˜ adapter æƒé‡ (æ¨è)
    peft_model.save_pretrained("./adapter_weights")
    # ä¿å­˜çš„æ–‡ä»¶:
    # - adapter_config.json
    # - adapter_model.safetensors

    # 2. åˆå¹¶åä¿å­˜å®Œæ•´æ¨¡å‹
    merged_model = peft_model.merge_and_unload()
    merged_model.save_pretrained("./merged_model")


    # ============ åŠ è½½ ============

    # 1. åŠ è½½ adapter
    base_model = AutoModelForCausalLM.from_pretrained("base_model_name")
    peft_model = PeftModel.from_pretrained(base_model, "./adapter_weights")

    # 2. æŸ¥çœ‹ adapter é…ç½®
    config = PeftConfig.from_pretrained("./adapter_weights")
    print(config)

    # 3. ç¦ç”¨ adapter (ä½¿ç”¨åŸå§‹æ¨¡å‹)
    with peft_model.disable_adapter():
        output = peft_model.generate(...)

    # 4. å¤š adapter ç®¡ç†
    peft_model.load_adapter("./adapter2", adapter_name="task2")
    peft_model.set_adapter("task2")  # åˆ‡æ¢ adapter
    """

    print(code_example)


def multi_adapter():
    """å¤š Adapter"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå¤š Adapter ç®¡ç†")
    print("=" * 60)

    print("""
    å¤šä»»åŠ¡ Adapter
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   Base Model (å†»ç»“)                      â”‚
    â”‚                                                         â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚   â”‚Adapter Aâ”‚   â”‚Adapter Bâ”‚   â”‚Adapter Câ”‚              â”‚
    â”‚   â”‚ ç¿»è¯‘    â”‚   â”‚ æ‘˜è¦    â”‚   â”‚ é—®ç­”    â”‚              â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚        â”‚             â”‚             â”‚                    â”‚
    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
    â”‚                      â”‚                                  â”‚
    â”‚              æ¨ç†æ—¶åŠ¨æ€åˆ‡æ¢                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    multi_code = """
    # è®­ç»ƒå¤šä¸ª adapter
    model.add_adapter(lora_config, adapter_name="translate")
    model.add_adapter(lora_config, adapter_name="summarize")

    # åˆ‡æ¢ adapter
    model.set_adapter("translate")
    output1 = model.generate(...)

    model.set_adapter("summarize")
    output2 = model.generate(...)

    # åˆå¹¶å¤šä¸ª adapter
    model.add_weighted_adapter(
        adapters=["translate", "summarize"],
        weights=[0.5, 0.5],
        adapter_name="combined",
    )
    """
    print(multi_code)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå¯¹æ¯”å®éªŒ
        æ¯”è¾ƒ LoRA, Prefix Tuning, IAÂ³ æ•ˆæœ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from peft import LoraConfig, PrefixTuningConfig, IA3Config
        
        configs = {
            "lora": LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"]),
            "prefix": PrefixTuningConfig(num_virtual_tokens=20, prefix_projection=True),
            "ia3": IA3Config(target_modules=["k_proj", "v_proj", "down_proj"]),
        }
        
        results = {}
        for name, config in configs.items():
            model = get_peft_model(base_model, config)
            trainer = Trainer(model=model, ...)
            trainer.train()
            results[name] = {
                "params": model.num_parameters(only_trainable=True),
                "eval_loss": trainer.evaluate()["eval_loss"],
            }
        
        # å…¸å‹å¯¹æ¯”ç»“æœ:
        # LoRA:   params=4.2M,  loss=1.8,  é€‚åˆå¤§å¤šæ•°ä»»åŠ¡
        # Prefix: params=0.8M,  loss=2.1,  å‚æ•°æœ€å°‘
        # IAÂ³:    params=0.2M,  loss=2.3,  æç®€å‚æ•°
        ```
    
    ç»ƒä¹  2ï¼šå¤š Adapter
        ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒå¤šä¸ª adapter

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from peft import PeftModel, LoraConfig
        
        # è®­ç»ƒä»»åŠ¡ A çš„ adapter
        model.add_adapter(LoraConfig(...), adapter_name="translate")
        # è®­ç»ƒ...
        model.save_pretrained("./adapter_translate")
        
        # è®­ç»ƒä»»åŠ¡ B çš„ adapter (é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹)
        model.add_adapter(LoraConfig(...), adapter_name="summarize")
        # è®­ç»ƒ...
        model.save_pretrained("./adapter_summarize")
        
        # æ¨ç†æ—¶åˆ‡æ¢
        model = PeftModel.from_pretrained(base_model, "./adapter_translate")
        model.load_adapter("./adapter_summarize", adapter_name="summarize")
        
        # ç¿»è¯‘ä»»åŠ¡
        model.set_adapter("default")
        translate_output = model.generate(...)
        
        # æ‘˜è¦ä»»åŠ¡
        model.set_adapter("summarize")
        summary_output = model.generate(...)
        ```
    
    ç»ƒä¹  3ï¼šAdapter åˆå¹¶
        å®éªŒä¸åŒæƒé‡çš„ adapter åˆå¹¶

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        # æµ‹è¯•ä¸åŒåˆå¹¶æƒé‡
        weight_combinations = [
            (0.8, 0.2),
            (0.6, 0.4),
            (0.5, 0.5),
            (0.4, 0.6),
        ]
        
        results = {}
        for w1, w2 in weight_combinations:
            model.add_weighted_adapter(
                adapters=["translate", "summarize"],
                weights=[w1, w2],
                adapter_name=f"combined_{w1}_{w2}",
                combination_type="linear",
            )
            model.set_adapter(f"combined_{w1}_{w2}")
            
            # è¯„ä¼°ä¸¤ä¸ªä»»åŠ¡
            translate_score = evaluate_translate(model)
            summary_score = evaluate_summary(model)
            
            results[f"{w1}:{w2}"] = {
                "translate": translate_score,
                "summary": summary_score,
            }
        
        # å…¸å‹ç»“æœ: 0.5:0.5 é€šå¸¸è¡¨ç°æœ€å¹³è¡¡
        ```
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. å¦‚ä½•é€‰æ‹©é€‚åˆä»»åŠ¡çš„ PEFT æ–¹æ³•ï¼Ÿ

       âœ… ç­”ï¼š
       - LoRAï¼šé€šç”¨é¦–é€‰ï¼Œæ•ˆæœå¥½ï¼Œæˆç†Ÿç¨³å®š
       - Prefix Tuningï¼šé€‚åˆç”Ÿæˆä»»åŠ¡ï¼Œå‚æ•°è¾ƒå°‘
       - IAÂ³ï¼šå‚æ•°æœ€å°‘ï¼Œé€‚åˆèµ„æºæå—é™åœºæ™¯
       - é€‰æ‹©æ ‡å‡†ï¼šæ•ˆæœ vs å‚æ•°é‡ vs è®­ç»ƒé€Ÿåº¦

    2. å¤š Adapter çš„åº”ç”¨åœºæ™¯ï¼Ÿ

       âœ… ç­”ï¼š
       - å¤šè¯­è¨€æ”¯æŒï¼šæ¯ç§è¯­è¨€ä¸€ä¸ª adapter
       - å¤šä»»åŠ¡æ¨¡å‹ï¼šä¸åŒä»»åŠ¡åˆ‡æ¢ä½¿ç”¨
       - A/B æµ‹è¯•ï¼šä¸åŒç‰ˆæœ¬çš„ adapter
       - ä¸ªæ€§åŒ–å®šåˆ¶ï¼šç”¨æˆ·çº§åˆ«çš„ adapter
       - é¢†åŸŸé€‚åº”ï¼šä¸åŒé¢†åŸŸä¸“ç”¨ adapter
    """)


def main():
    print("ğŸ“¦ PEFT åº“ä½¿ç”¨")
    print("=" * 60)
    peft_overview()
    lora_adapter()
    other_methods()
    model_management()
    multi_adapter()
    exercises()
    print("\nâœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š07-supervised-finetuning.py")


if __name__ == "__main__":
    main()
