# LLM 微调技术深度解析

> 从零开始理解大语言模型微调的核心原理与方法论

---

## 目录

1. [微调概述](#1-微调概述)
2. [预训练与微调的关系](#2-预训练与微调的关系)
3. [微调方法分类体系](#3-微调方法分类体系)
4. [全参数微调](#4-全参数微调)
5. [参数高效微调（PEFT）](#5-参数高效微调peft)
6. [指令微调与对齐](#6-指令微调与对齐)
7. [微调数据工程](#7-微调数据工程)
8. [微调中的关键技术](#8-微调中的关键技术)
9. [微调评估方法](#9-微调评估方法)
10. [常见问题与解决方案](#10-常见问题与解决方案)
11. [总结与展望](#11-总结与展望)

---

## 1. 微调概述

### 1.1 什么是微调

**微调（Fine-tuning）** 是指在预训练模型的基础上，使用特定领域或任务的数据，对模型参数进行进一步训练的过程。

```
┌─────────────────────────────────────────────────────────────┐
│                      LLM 训练流程                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   ┌──────────────┐    ┌──────────────┐    ┌──────────────┐ │
│   │   预训练      │───▶│    微调      │───▶│   部署应用   │ │
│   │ Pre-training │    │ Fine-tuning  │    │  Deployment  │ │
│   └──────────────┘    └──────────────┘    └──────────────┘ │
│         │                    │                    │         │
│         ▼                    ▼                    ▼         │
│   大规模通用语料         特定任务数据          实际业务场景    │
│   学习语言知识           适配具体任务          提供服务       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 为什么需要微调

| 需求         | 预训练模型的局限         | 微调的解决方案    |
| ------------ | ------------------------ | ----------------- |
| **领域适配** | 通用知识，缺乏专业深度   | 注入领域知识      |
| **任务优化** | 泛化能力强但任务精度不足 | 针对性优化        |
| **风格控制** | 输出风格固定             | 定制输出格式/风格 |
| **成本控制** | API调用成本高            | 本地部署小模型    |
| **数据隐私** | 数据需上传云端           | 本地私有化部署    |

### 1.3 微调的核心思想

```
预训练阶段：学习「语言是什么」—— 语法、语义、世界知识
    ↓
微调阶段：学习「如何完成特定任务」—— 任务模式、输出格式、领域知识
```

---

## 2. 预训练与微调的关系

### 2.1 知识迁移视角

```
┌────────────────────────────────────────────────────────────────┐
│                        知识层次模型                             │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  预训练获得：                    微调获得：                      │
│  ┌─────────────────────┐       ┌─────────────────────┐        │
│  │ 语言学知识           │       │ 任务特定知识         │        │
│  │ · 词法、句法         │       │ · 输出格式           │        │
│  │ · 语义理解          │        │ · 领域术语           │        │
│  │ · 篇章结构          │        │ · 推理模式           │        │
│  ├─────────────────────┤       ├─────────────────────┤        │
│  │ 世界知识            │        │ 对齐知识             │        │
│  │ · 事实性知识        │        │ · 人类偏好           │        │
│  │ · 常识推理          │        │ · 安全边界           │        │
│  │ · 因果关系          │        │ · 价值观             │        │
│  └─────────────────────┘       └─────────────────────┘        │
│           │                              │                     │
│           └──────────────┬───────────────┘                     │
│                          ▼                                     │
│                   ┌─────────────┐                              │
│                   │  最终模型   │                              │
│                   └─────────────┘                              │
└────────────────────────────────────────────────────────────────┘
```

### 2.2 参数空间视角

预训练将模型参数移动到一个"好的"初始化位置，微调则是在这个位置的邻域内进行精细调整：

```
参数空间示意：

                    ★ 任务最优点
                   /
                  /
    ●────────────●
    ↑            ↑
 随机初始化   预训练后位置
 (远离最优)   (接近最优，微调起点)
```

### 2.3 数据规模对比

| 阶段   | 数据规模  | 数据类型       | 计算成本      |
| ------ | --------- | -------------- | ------------- |
| 预训练 | TB级别    | 通用网页、书籍 | 数百万美元    |
| 微调   | MB~GB级别 | 任务相关数据   | 数百~数千美元 |

---

## 3. 微调方法分类体系

### 3.1 按参数更新范围分类

```
┌─────────────────────────────────────────────────────────────┐
│                      微调方法分类                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │               全参数微调 (Full Fine-tuning)          │   │
│  │                  更新所有模型参数                     │   │
│  └─────────────────────────────────────────────────────┘   │
│                          │                                  │
│  ┌───────────────────────┴───────────────────────┐         │
│  │         参数高效微调 (PEFT)                    │         │
│  │           只更新少量参数                       │         │
│  ├───────────────────────────────────────────────┤         │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐      │         │
│  │  │ 增加参数 │ │ 选择参数 │ │ 重参数化 │       │         │
│  │  │ Additive │ │Selective │ │Reparameter│      │         │
│  │  └────┬─────┘ └────┬─────┘ └────┬─────┘      │         │
│  │       │            │            │            │         │
│  │   Adapter       BitFit        LoRA          │         │
│  │   Prefix       Layer选择      QLoRA          │         │
│  │   Prompt                     DoRA           │         │
│  └───────────────────────────────────────────────┘         │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 按训练目标分类

```
┌────────────────────────────────────────────────────────────┐
│  监督微调（SFT）          强化学习微调（RLHF）              │
│  ┌──────────────────┐    ┌──────────────────┐             │
│  │ 输入-输出配对     │    │ 人类反馈信号      │             │
│  │ 直接学习映射      │    │ 奖励模型引导      │             │
│  └──────────────────┘    └──────────────────┘             │
│           │                       │                        │
│           └───────────┬───────────┘                        │
│                       ▼                                    │
│              ┌────────────────┐                            │
│              │   对齐的模型    │                            │
│              └────────────────┘                            │
└────────────────────────────────────────────────────────────┘
```

### 3.3 方法演进时间线

```
2018 ────── 2019 ────── 2020 ────── 2021 ────── 2022 ────── 2023 ────── 2024
  │          │          │          │          │          │          │
  ▼          ▼          ▼          ▼          ▼          ▼          ▼
全参数微调  Adapter   Prefix    LoRA      QLoRA     DoRA      各种
(BERT)    Tuning    Tuning              LongLoRA   GaLore   组合优化
                    Prompt              AdaLoRA
                    Tuning
```

---

## 4. 全参数微调

### 4.1 基本原理

全参数微调（Full Fine-tuning）更新模型的所有参数：

```
前向传播：
    Input → [Embedding] → [Layer1] → ... → [LayerN] → Output
                ↓            ↓               ↓
              更新         更新            更新

损失函数：L = CrossEntropy(Output, Target)

反向传播：更新所有 θ ∈ {θ_embed, θ_layer1, ..., θ_layerN}
```

### 4.2 数学表达

$$\theta^* = \arg\min_\theta \sum_{(x,y) \in D} \mathcal{L}(f_\theta(x), y)$$

其中：

- $\theta$ 是模型的全部参数
- $D$ 是微调数据集
- $\mathcal{L}$ 是损失函数（通常是交叉熵）
- $f_\theta$ 是模型的前向计算

### 4.3 优缺点分析

| 优点                  | 缺点                            |
| --------------------- | ------------------------------- |
| ✅ 效果最好，上限最高 | ❌ 显存需求巨大                 |
| ✅ 实现简单           | ❌ 容易过拟合                   |
| ✅ 无额外推理开销     | ❌ 灾难性遗忘风险               |
| ✅ 完全利用模型容量   | ❌ 存储成本高（每任务一个模型） |

### 4.4 显存需求估算

对于一个参数量为 $P$ 的模型，全参数微调的显存需求：

```
┌─────────────────────────────────────────────────────────────┐
│  显存组成（以 FP16 混合精度训练为例）                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  模型参数：         2P bytes  (FP16)                        │
│  梯度：            2P bytes  (FP16)                         │
│  优化器状态(Adam)：                                          │
│    - 动量 m：      4P bytes  (FP32)                         │
│    - 方差 v：      4P bytes  (FP32)                         │
│    - 主参数副本：   4P bytes  (FP32)                         │
│  激活值：          依赖batch_size和序列长度                   │
│                                                             │
│  总计 ≈ 16P bytes + 激活值                                  │
│                                                             │
│  示例：7B 模型 ≈ 7×16 = 112GB + 激活值 ≈ 130-150GB          │
└─────────────────────────────────────────────────────────────┘
```

---

## 5. 参数高效微调（PEFT）

### 5.1 PEFT 核心思想

```
核心洞察：预训练模型已经学到了丰富的知识，
         微调只需要"微调"，不需要大改。

         ┌─────────────────────────────────┐
         │  冻结大部分参数，只训练少量参数   │
         │  达到接近全参数微调的效果        │
         └─────────────────────────────────┘
```

### 5.2 LoRA（Low-Rank Adaptation）

#### 5.2.1 核心原理

LoRA 的核心假设：**模型适配过程中的权重更新矩阵是低秩的**

```
原始权重更新：
    W' = W + ΔW

    其中 W ∈ R^{d×k}, ΔW ∈ R^{d×k}

LoRA 低秩分解：
    ΔW = B × A

    其中 B ∈ R^{d×r}, A ∈ R^{r×k}, r << min(d, k)
```

#### 5.2.2 结构图示

```
┌─────────────────────────────────────────────────────────────┐
│                        LoRA 结构                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│                        ┌───────────┐                        │
│                        │  Output   │                        │
│                        └─────┬─────┘                        │
│                              │                              │
│                        ┌─────┴─────┐                        │
│                        │    Add    │                        │
│                        └─────┬─────┘                        │
│                    ┌─────────┴─────────┐                    │
│                    │                   │                    │
│              ┌─────┴─────┐       ┌─────┴─────┐             │
│              │  W (冻结) │       │   B × A   │              │
│              │  d × k    │       │ (可训练)  │              │
│              └─────┬─────┘       └─────┬─────┘             │
│                    │                   │                    │
│                    │             ┌─────┴─────┐             │
│                    │             │     B     │             │
│                    │             │   d × r   │             │
│                    │             └─────┬─────┘             │
│                    │                   │                    │
│                    │             ┌─────┴─────┐             │
│                    │             │     A     │             │
│                    │             │   r × k   │             │
│                    │             └─────┬─────┘             │
│                    │                   │                    │
│                    └─────────┬─────────┘                    │
│                        ┌─────┴─────┐                        │
│                        │   Input   │                        │
│                        │     x     │                        │
│                        └───────────┘                        │
│                                                             │
│  前向传播: h = Wx + BAx = Wx + ΔWx                          │
│  可训练参数: (d×r) + (r×k) = r(d+k) << d×k                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 5.2.3 参数量对比

| 设置         | 参数量       | 相对原始 |
| ------------ | ------------ | -------- |
| 原始权重 $W$ | $d \times k$ | 100%     |
| LoRA (r=8)   | $8(d+k)$     | ~0.1-1%  |
| LoRA (r=16)  | $16(d+k)$    | ~0.2-2%  |

**以 LLaMA-7B 的一个注意力层为例：**

- $d = k = 4096$
- 原始：$4096 \times 4096 = 16.7M$
- LoRA(r=8)：$8 \times (4096 + 4096) = 65.5K$
- 压缩比：约 **255倍**

#### 5.2.4 关键超参数

```
┌────────────────────────────────────────────────────────────┐
│                    LoRA 关键超参数                          │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  1. Rank (r)                                               │
│     ├── 常用值：4, 8, 16, 32, 64                           │
│     ├── 越大表达能力越强，但参数越多                         │
│     └── 推荐从 8 开始尝试                                   │
│                                                            │
│  2. Alpha (α)                                              │
│     ├── 缩放因子：ΔW = (α/r) × BA                          │
│     ├── 常用值：16, 32 或 α = 2r                           │
│     └── 控制 LoRA 更新的强度                                │
│                                                            │
│  3. Target Modules                                         │
│     ├── 常见选择：q_proj, v_proj (Query, Value)            │
│     ├── 更激进：q, k, v, o, gate, up, down                 │
│     └── 更多模块 = 更强能力 = 更多参数                      │
│                                                            │
│  4. Dropout                                                │
│     ├── LoRA 层的 dropout 率                               │
│     └── 常用值：0.05 - 0.1                                 │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

#### 5.2.5 初始化策略

```python
# A 矩阵：使用高斯/均匀分布初始化
A ~ N(0, σ²) 或 Uniform(-a, a)

# B 矩阵：初始化为零
B = 0

# 这保证了训练开始时 ΔW = BA = 0
# 即初始状态模型行为与预训练模型完全一致
```

### 5.3 QLoRA（Quantized LoRA）

#### 5.3.1 核心创新

QLoRA 在 LoRA 基础上引入量化技术，进一步降低显存需求：

```
┌─────────────────────────────────────────────────────────────┐
│                       QLoRA 三大创新                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 4-bit NormalFloat (NF4)                                │
│     └── 专为正态分布权重设计的 4-bit 数据类型                 │
│                                                             │
│  2. 双重量化 (Double Quantization)                          │
│     └── 对量化常数再次量化，进一步节省显存                    │
│                                                             │
│  3. 分页优化器 (Paged Optimizers)                           │
│     └── 利用 CPU 内存处理显存峰值                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 5.3.2 显存对比

| 方法              | 7B模型显存需求 | 65B模型显存需求 |
| ----------------- | -------------- | --------------- |
| 全参数微调 (FP16) | ~130GB         | ~780GB          |
| LoRA (FP16)       | ~28GB          | ~160GB          |
| QLoRA (NF4)       | ~6GB           | ~48GB           |

#### 5.3.3 NF4 量化原理

```
传统 INT4：均匀分布的量化级别
    [-8, -7, -6, ..., 6, 7]

NF4：针对正态分布优化的量化级别
    基于预训练权重通常服从正态分布的观察
    量化级别在均值附近更密集

    ┌─────────────────────────────────────┐
    │  NF4 量化级别（示意）                 │
    │                                     │
    │  ▁▂▃▅▆██▆▅▃▂▁                       │
    │  -3σ  -σ  0  +σ  +3σ               │
    │     密度在中心更高                   │
    └─────────────────────────────────────┘
```

### 5.4 DoRA（Weight-Decomposed LoRA）

#### 5.4.1 动机

DoRA 观察到 LoRA 和全参数微调在权重更新模式上的差异：

```
权重分解为：方向(direction) 和 幅度(magnitude)

W = m × (W / ||W||) = m × d̂

全参数微调：同时更新方向和幅度
LoRA：主要更新方向，幅度变化有限

DoRA：显式解耦方向和幅度，分别优化
```

#### 5.4.2 结构对比

```
┌─────────────────────────────────────────────────────────────┐
│                     LoRA vs DoRA                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  LoRA:                         DoRA:                        │
│  ┌────────────┐               ┌────────────┐               │
│  │   W + BA   │               │ m × ────── │               │
│  └────────────┘               │   ||W+BA|| │               │
│                               └────────────┘               │
│  整体更新                       m: 可学习幅度向量            │
│                               方向：(W+BA)/||W+BA||         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 5.5 Adapter Tuning

#### 5.5.1 基本结构

Adapter 在 Transformer 层中插入小型瓶颈层：

```
┌─────────────────────────────────────────────────────────────┐
│                    Adapter 结构                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  原始 Transformer 层：                                       │
│  ┌──────────────────────────────────────────┐              │
│  │  Input                                   │              │
│  │    ↓                                     │              │
│  │  Multi-Head Attention (冻结)             │              │
│  │    ↓                                     │              │
│  │  ┌─────────────────────┐ ← Adapter       │              │
│  │  │ Down: d → r         │                 │              │
│  │  │ Activation (ReLU)   │                 │              │
│  │  │ Up: r → d           │                 │              │
│  │  │ Residual Connection │                 │              │
│  │  └─────────────────────┘                 │              │
│  │    ↓                                     │              │
│  │  Layer Norm                              │              │
│  │    ↓                                     │              │
│  │  FFN (冻结)                              │              │
│  │    ↓                                     │              │
│  │  ┌─────────────────────┐ ← Adapter       │              │
│  │  │ Down → Act → Up     │                 │              │
│  │  └─────────────────────┘                 │              │
│  │    ↓                                     │              │
│  │  Output                                  │              │
│  └──────────────────────────────────────────┘              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 5.5.2 与 LoRA 对比

| 特性       | Adapter        | LoRA         |
| ---------- | -------------- | ------------ |
| 结构       | 串行插入       | 并行添加     |
| 推理开销   | 有（增加延迟） | 无（可合并） |
| 参数效率   | 较高           | 更高         |
| 多任务切换 | 简单           | 需重新合并   |

### 5.6 Prefix Tuning

#### 5.6.1 核心思想

在输入序列前添加可学习的"虚拟 token"：

```
┌─────────────────────────────────────────────────────────────┐
│                    Prefix Tuning                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  原始输入：[x₁, x₂, x₃, ..., xₙ]                            │
│                                                             │
│  添加前缀：[P₁, P₂, ..., Pₘ, x₁, x₂, x₃, ..., xₙ]          │
│            └────────────┘                                   │
│             可学习的前缀                                     │
│                                                             │
│  每一层都有独立的前缀向量                                     │
│                                                             │
│  Layer 1: [P¹₁, P¹₂, ..., P¹ₘ]                             │
│  Layer 2: [P²₁, P²₂, ..., P²ₘ]                             │
│  ...                                                        │
│  Layer L: [Pᴸ₁, Pᴸ₂, ..., Pᴸₘ]                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 5.6.2 参数化技巧

直接优化高维前缀向量不稳定，使用重参数化：

```
P = MLP(P_embed)

其中 P_embed 是低维的可学习嵌入
MLP 是小型网络，将低维映射到需要的高维

训练完成后，可以丢弃 MLP，只保留 P
```

### 5.7 Prompt Tuning

#### 5.7.1 与 Prefix Tuning 的区别

```
┌─────────────────────────────────────────────────────────────┐
│              Prefix Tuning vs Prompt Tuning                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Prefix Tuning:                                             │
│  └── 每一层都添加可学习向量                                  │
│  └── 参数量：num_layers × prefix_length × hidden_dim        │
│                                                             │
│  Prompt Tuning:                                             │
│  └── 只在输入嵌入层添加可学习向量                             │
│  └── 参数量：prompt_length × hidden_dim                     │
│  └── 更加轻量                                               │
│                                                             │
│  对比示意：                                                  │
│                                                             │
│       Prefix Tuning          Prompt Tuning                 │
│       ┌─────┐               ┌─────┐                        │
│  L12  │[P]FF│               │ FF  │                        │
│       ├─────┤               ├─────┤                        │
│  L11  │[P]FF│               │ FF  │                        │
│       ├─────┤               ├─────┤                        │
│  ...  │ ... │               │ ... │                        │
│       ├─────┤               ├─────┤                        │
│  L1   │[P]FF│               │ FF  │                        │
│       ├─────┤               ├─────┤                        │
│  Emb  │[P] x│               │[P] x│                        │
│       └─────┘               └─────┘                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 5.8 PEFT 方法总结对比

```
┌──────────────────────────────────────────────────────────────────────┐
│                         PEFT 方法对比                                 │
├───────────────┬─────────────┬───────────────┬───────────┬────────────┤
│     方法      │  参数量比例  │   推理开销    │   性能    │   适用场景  │
├───────────────┼─────────────┼───────────────┼───────────┼────────────┤
│    LoRA       │   0.1-1%    │     无        │    高     │   通用     │
├───────────────┼─────────────┼───────────────┼───────────┼────────────┤
│    QLoRA      │   0.1-1%    │  量化开销     │    高     │ 资源受限   │
├───────────────┼─────────────┼───────────────┼───────────┼────────────┤
│   Adapter     │   1-5%      │     有        │    高     │  多任务    │
├───────────────┼─────────────┼───────────────┼───────────┼────────────┤
│ Prefix Tuning │   0.1-1%    │  轻微增加     │   中高    │  生成任务  │
├───────────────┼─────────────┼───────────────┼───────────┼────────────┤
│ Prompt Tuning │   <0.1%     │  轻微增加     │    中     │  大模型    │
├───────────────┼─────────────┼───────────────┼───────────┼────────────┤
│    DoRA       │   0.1-1%    │     无        │   更高    │ 追求效果   │
└───────────────┴─────────────┴───────────────┴───────────┴────────────┘
```

---

## 6. 指令微调与对齐

### 6.1 指令微调（Instruction Tuning / SFT）

#### 6.1.1 基本概念

```
┌─────────────────────────────────────────────────────────────┐
│                      指令微调                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  目标：让模型学会理解和遵循人类指令                           │
│                                                             │
│  数据格式：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ Instruction: 请将以下英文翻译成中文                    │   │
│  │ Input: Hello, how are you?                          │   │
│  │ Output: 你好，你怎么样？                              │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  训练目标：最大化 P(Output | Instruction, Input)            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 6.1.2 数据模板示例

```
# Alpaca 风格
Below is an instruction that describes a task. Write a response.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}

# ChatML 风格
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>
```

#### 6.1.3 Loss 计算策略

```
┌─────────────────────────────────────────────────────────────┐
│                   Loss 掩码策略                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  输入序列：[Instruction] [Input] [Output]                   │
│  Loss 掩码：[   0...0   ] [ 0..0 ] [ 1...1 ]                │
│                                                             │
│  只在 Output 部分计算 Loss，避免学习复述输入                  │
│                                                             │
│  示例：                                                     │
│  Token:  [请] [翻] [译] [:] [Hello] [→] [你] [好]           │
│  Mask:   [ 0] [ 0] [0] [0] [  0  ] [0] [ 1] [ 1]           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 6.2 RLHF（基于人类反馈的强化学习）

#### 6.2.1 三阶段流程

```
┌─────────────────────────────────────────────────────────────┐
│                    RLHF 三阶段                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  阶段一：监督微调（SFT）                                     │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 预训练模型 + 高质量对话数据 → SFT 模型                 │   │
│  └─────────────────────────────────────────────────────┘   │
│                          ↓                                  │
│  阶段二：奖励模型训练（RM）                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 收集人类偏好数据（对比排序）                           │   │
│  │ Prompt → [Response A, Response B] → 人类标注偏好      │   │
│  │ 训练奖励模型预测人类偏好                               │   │
│  └─────────────────────────────────────────────────────┘   │
│                          ↓                                  │
│  阶段三：强化学习优化（PPO）                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 使用 RM 作为奖励信号                                  │   │
│  │ 用 PPO 算法优化策略模型                               │   │
│  │ 同时保持与 SFT 模型的 KL 散度约束                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 6.2.2 PPO 优化目标

$$\max_\pi \mathbb{E}_{x \sim D, y \sim \pi}[R(x, y)] - \beta \cdot KL[\pi || \pi_{ref}]$$

其中：

- $R(x, y)$：奖励模型给出的奖励
- $KL[\pi || \pi_{ref}]$：当前策略与参考策略的 KL 散度
- $\beta$：KL 惩罚系数

### 6.3 DPO（Direct Preference Optimization）

#### 6.3.1 核心思想

DPO 跳过显式的奖励模型，直接从偏好数据优化策略：

```
┌─────────────────────────────────────────────────────────────┐
│                 RLHF vs DPO                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  RLHF:                                                      │
│  偏好数据 → 训练 RM → 用 RM 指导 PPO → 优化模型              │
│     (复杂，需要多个模型，训练不稳定)                          │
│                                                             │
│  DPO:                                                       │
│  偏好数据 → 直接优化模型                                     │
│     (简单，单阶段，更稳定)                                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 6.3.2 DPO 损失函数

$$\mathcal{L}_{DPO} = -\mathbb{E}_{(x, y_w, y_l)}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]$$

其中：

- $y_w$：偏好的（winning）回答
- $y_l$：不偏好的（losing）回答
- $\sigma$：sigmoid 函数
- $\pi_{ref}$：参考模型（通常是 SFT 模型）

### 6.4 对齐方法对比

```
┌────────────────────────────────────────────────────────────────────┐
│                        对齐方法对比                                 │
├─────────────┬─────────────┬─────────────┬──────────────────────────┤
│    方法     │   复杂度    │   稳定性    │         适用场景          │
├─────────────┼─────────────┼─────────────┼──────────────────────────┤
│    SFT      │     低      │     高      │ 基础能力培养，格式学习    │
├─────────────┼─────────────┼─────────────┼──────────────────────────┤
│    RLHF     │     高      │     低      │ 需要精细控制的复杂对齐    │
├─────────────┼─────────────┼─────────────┼──────────────────────────┤
│    DPO      │     中      │     高      │ 偏好对齐，替代RLHF       │
├─────────────┼─────────────┼─────────────┼──────────────────────────┤
│    ORPO     │     低      │     高      │ 单阶段对齐，无需参考模型  │
├─────────────┼─────────────┼─────────────┼──────────────────────────┤
│   SimPO     │     低      │     高      │ 简化的偏好优化           │
└─────────────┴─────────────┴─────────────┴──────────────────────────┘
```

---

## 7. 微调数据工程

### 7.1 数据质量的重要性

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   "数据质量 > 数据数量"                                      │
│                                                             │
│   1000 条高质量数据的效果 ≥ 100000 条低质量数据              │
│                                                             │
│   LIMA 论文证明：仅 1000 条精心设计的数据                     │
│                  就能达到出色的对话效果                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 7.2 数据格式设计

#### 7.2.1 单轮对话

```json
{
  "instruction": "任务描述/问题",
  "input": "可选的额外输入",
  "output": "期望的输出"
}
```

#### 7.2.2 多轮对话

```json
{
  "conversations": [
    { "role": "system", "content": "系统提示" },
    { "role": "user", "content": "用户第一轮" },
    { "role": "assistant", "content": "助手回复" },
    { "role": "user", "content": "用户第二轮" },
    { "role": "assistant", "content": "助手回复" }
  ]
}
```

### 7.3 数据质量评估维度

```
┌─────────────────────────────────────────────────────────────┐
│                    数据质量维度                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  准确性（Accuracy）                                         │
│  └── 答案是否事实正确                                       │
│                                                             │
│  相关性（Relevance）                                        │
│  └── 回答是否切题                                           │
│                                                             │
│  完整性（Completeness）                                     │
│  └── 是否覆盖了问题的所有方面                                │
│                                                             │
│  清晰度（Clarity）                                          │
│  └── 表达是否清晰易懂                                       │
│                                                             │
│  多样性（Diversity）                                        │
│  └── 数据集是否覆盖足够多的场景和模式                        │
│                                                             │
│  一致性（Consistency）                                      │
│  └── 相似问题的回答风格是否统一                              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 7.4 数据配比策略

```
┌─────────────────────────────────────────────────────────────┐
│                    数据混合策略                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  任务均衡：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  任务类型    │  样本数   │  采样权重                  │   │
│  ├─────────────────────────────────────────────────────┤   │
│  │  对话问答    │   30%    │    1.0                    │   │
│  │  文本生成    │   25%    │    1.0                    │   │
│  │  代码生成    │   20%    │    1.2 (上采样)            │   │
│  │  推理任务    │   15%    │    1.5 (上采样)            │   │
│  │  其他任务    │   10%    │    0.8 (下采样)            │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  难度均衡：                                                  │
│  简单:中等:困难 ≈ 3:5:2                                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 7.5 数据增强技术

```
┌─────────────────────────────────────────────────────────────┐
│                     数据增强方法                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 指令改写                                                │
│     原始: "翻译成英文"                                       │
│     改写: "请把下面的中文转换成英文表达"                      │
│                                                             │
│  2. 回答扩展                                                │
│     添加解释、步骤分解、相关知识                             │
│                                                             │
│  3. Self-Instruct                                           │
│     使用模型自己生成新的指令-回答对                          │
│                                                             │
│  4. Evol-Instruct                                           │
│     逐步增加指令复杂度                                       │
│     简单 → 添加约束 → 深化问题 → 增加步骤                    │
│                                                             │
│  5. 反向构造                                                │
│     给定答案，让模型生成可能的问题                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 8. 微调中的关键技术

### 8.1 学习率策略

```
┌─────────────────────────────────────────────────────────────┐
│                    学习率调度                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  推荐策略：Warmup + Cosine Decay                            │
│                                                             │
│  学习率                                                      │
│    ↑                                                        │
│  max├────────╮                                              │
│     │       ╲                                               │
│     │        ╲                                              │
│     │         ╲                                             │
│     │          ╲                                            │
│  min├───────────╲────────────                               │
│     └────┬───────┴──────────→ steps                        │
│        warmup    decay                                      │
│                                                             │
│  典型值：                                                    │
│  - 全参数微调：1e-5 ~ 5e-5                                  │
│  - LoRA：1e-4 ~ 3e-4                                        │
│  - Warmup：总步数的 3-10%                                   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.2 批量大小与梯度累积

```
有效批量大小 = batch_size × gradient_accumulation_steps × num_gpus

示例配置（显存受限）：
┌─────────────────────────────────────────────────────────────┐
│  显存     │  batch_size  │  grad_accum  │  有效batch_size  │
├─────────────────────────────────────────────────────────────┤
│  24GB    │      1       │     16       │       16         │
│  48GB    │      2       │      8       │       16         │
│  80GB    │      4       │      4       │       16         │
└─────────────────────────────────────────────────────────────┘
```

### 8.3 混合精度训练

```
┌─────────────────────────────────────────────────────────────┐
│                   混合精度策略                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  FP32（全精度）                                             │
│  └── 优化器状态、主参数副本                                  │
│                                                             │
│  FP16/BF16（半精度）                                        │
│  └── 前向传播、反向传播、梯度                                │
│                                                             │
│  BF16 vs FP16：                                             │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ BF16：更大的动态范围，较少的精度位                     │  │
│  │       更适合深度学习，较少出现溢出                     │  │
│  │       推荐用于 LLM 训练                               │  │
│  │                                                      │  │
│  │ FP16：更高的精度，较小的动态范围                       │  │
│  │       需要 loss scaling 防止梯度下溢                  │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.4 长度截断与 Packing

```
┌─────────────────────────────────────────────────────────────┐
│                  序列处理策略                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  传统 Padding（浪费算力）：                                  │
│  ┌──────────────────────────────────────┐                  │
│  │ [实际内容] [PAD] [PAD] [PAD] [PAD]   │ 样本1            │
│  │ [实际内容很长很长]  [PAD] [PAD]      │ 样本2            │
│  │ [短] [PAD] [PAD] [PAD] [PAD] [PAD]  │ 样本3            │
│  └──────────────────────────────────────┘                  │
│                                                             │
│  Packing（高效利用）：                                       │
│  ┌──────────────────────────────────────┐                  │
│  │ [样本1] [EOS] [样本3] [EOS] [样本5]  │ 打包序列1         │
│  │ [样本2] [EOS] [样本4] [EOS] [PAD]   │ 打包序列2         │
│  └──────────────────────────────────────┘                  │
│                                                             │
│  注意：Packing 需要特殊的 attention mask                    │
│       防止不同样本间的交叉注意力                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.5 梯度检查点（Gradient Checkpointing）

```
┌─────────────────────────────────────────────────────────────┐
│                  梯度检查点原理                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  标准反向传播：保存所有激活值                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 前向：保存每层激活 → 显存 O(N)                        │   │
│  │ 反向：使用保存的激活计算梯度                          │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  梯度检查点：只保存检查点激活                                │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 前向：只保存检查点层的激活 → 显存 O(√N)               │   │
│  │ 反向：重新计算中间激活 → 时间增加 ~33%                │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  trade-off: 显存 ↓ 约 60-70%，时间 ↑ 约 30-40%             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.6 DeepSpeed ZeRO 优化

```
┌─────────────────────────────────────────────────────────────┐
│                    ZeRO 三阶段                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ZeRO-1：切分优化器状态                                      │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ GPU0: Opt_state[0:N/4]  GPU1: Opt_state[N/4:N/2]   │   │
│  │ GPU2: Opt_state[N/2:3N/4]  GPU3: Opt_state[3N/4:N] │   │
│  │ 显存节省：约 4x                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ZeRO-2：切分梯度                                           │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 在 ZeRO-1 基础上，梯度也分布存储                      │   │
│  │ 显存节省：约 8x                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ZeRO-3：切分模型参数                                       │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 参数、梯度、优化器状态全部切分                        │   │
│  │ 需要时通过通信获取                                   │   │
│  │ 显存节省：与 GPU 数量成线性关系                       │   │
│  │ 通信开销：最大                                       │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 9. 微调评估方法

### 9.1 评估维度

```
┌─────────────────────────────────────────────────────────────┐
│                     评估维度框架                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                    通用能力                          │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐   │   │
│  │  │语言理解 │ │知识问答 │ │推理能力 │ │代码能力 │   │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘   │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                    任务特定                          │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐   │   │
│  │  │任务准确率│ │格式符合 │ │领域知识 │ │风格一致 │   │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘   │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                    对齐质量                          │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐   │   │
│  │  │ 有帮助  │ │ 诚实   │ │ 无害性  │ │遵循指令 │   │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘   │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 9.2 自动评估指标

| 评估类型 | 指标        | 说明                     |
| -------- | ----------- | ------------------------ |
| 生成质量 | Perplexity  | 语言模型困惑度           |
|          | BLEU        | n-gram 匹配度            |
|          | ROUGE       | 召回导向的匹配           |
|          | BERTScore   | 语义相似度               |
| 分类任务 | Accuracy    | 准确率                   |
|          | F1 Score    | 精确率和召回率的调和平均 |
| 推理任务 | Exact Match | 精确匹配率               |

### 9.3 基准测试

```
┌─────────────────────────────────────────────────────────────┐
│                   常用评估基准                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  综合能力：                                                  │
│  ├── MMLU：多任务语言理解（57个学科）                         │
│  ├── C-Eval：中文综合评测                                    │
│  └── AGIEval：人类考试题目                                   │
│                                                             │
│  推理能力：                                                  │
│  ├── GSM8K：数学应用题                                       │
│  ├── MATH：竞赛数学题                                        │
│  └── BBH：复杂推理任务                                       │
│                                                             │
│  代码能力：                                                  │
│  ├── HumanEval：Python编程                                  │
│  ├── MBPP：基础编程题                                        │
│  └── MultiPL-E：多语言编程                                   │
│                                                             │
│  对话能力：                                                  │
│  ├── MT-Bench：多轮对话评分                                  │
│  └── AlpacaEval：指令遵循                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 9.4 人工评估

```
┌─────────────────────────────────────────────────────────────┐
│                    人工评估方法                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  打分评估：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  评估者对输出进行 1-5 分评分                          │   │
│  │  维度：准确性、流畅性、有用性、安全性                  │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  对比评估：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  给定同一问题的两个回答                               │   │
│  │  评估者选择更好的一个（或 tie）                       │   │
│  │  计算模型间的 win rate                               │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  LLM-as-Judge：                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  使用强大的 LLM（如 GPT-4）作为评估者                 │   │
│  │  成本低，可扩展性好                                   │   │
│  │  需注意位置偏见和自我偏好                             │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 10. 常见问题与解决方案

### 10.1 灾难性遗忘

```
┌─────────────────────────────────────────────────────────────┐
│                    灾难性遗忘                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  现象：微调后模型丢失预训练阶段学到的能力                     │
│                                                             │
│  解决方案：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  1. 使用较小的学习率                                  │   │
│  │  2. 混合通用数据和任务数据                            │   │
│  │  3. 使用 PEFT 方法（保持原始参数不变）                 │   │
│  │  4. 正则化约束（L2 正则、KL 约束）                     │   │
│  │  5. 经验回放（定期加入旧任务数据）                     │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 10.2 过拟合

```
┌─────────────────────────────────────────────────────────────┐
│                      过拟合                                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  现象：                                                      │
│  - 训练损失持续下降，验证损失上升                            │
│  - 模型在验证集表现变差                                      │
│  - 对训练数据"记忆"而非"学习"                               │
│                                                             │
│  解决方案：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  1. 增加数据量或数据多样性                            │   │
│  │  2. 减少训练轮数（Early Stopping）                    │   │
│  │  3. 添加 Dropout                                     │   │
│  │  4. 使用更小的 LoRA Rank                             │   │
│  │  5. 增大 weight decay                                │   │
│  │  6. 数据增强                                         │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 10.3 训练不稳定

```
┌─────────────────────────────────────────────────────────────┐
│                    训练不稳定                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  现象：                                                      │
│  - Loss 剧烈波动或突然飙升                                   │
│  - NaN / Inf 出现                                           │
│  - 梯度爆炸                                                  │
│                                                             │
│  解决方案：                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  1. 降低学习率                                        │   │
│  │  2. 使用梯度裁剪 (gradient clipping)                  │   │
│  │  3. 使用 BF16 代替 FP16                              │   │
│  │  4. 增加 warmup 步数                                 │   │
│  │  5. 检查数据中的异常值                                │   │
│  │  6. 使用更稳定的优化器（如 AdamW）                     │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 10.4 显存不足

```
┌─────────────────────────────────────────────────────────────┐
│                    显存优化策略                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  策略优先级（按效果排序）：                                   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 1. 使用 PEFT（LoRA/QLoRA）      效果：↓↓↓↓↓          │   │
│  │ 2. 启用梯度检查点               效果：↓↓↓↓           │   │
│  │ 3. 减小 batch size              效果：↓↓↓            │   │
│  │ 4. 使用混合精度训练             效果：↓↓             │   │
│  │ 5. 减小序列长度                 效果：↓↓             │   │
│  │ 6. 使用 DeepSpeed ZeRO          效果：↓↓↓ (多卡)     │   │
│  │ 7. CPU Offload                 效果：↓↓ (有速度代价) │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 10.5 超参数选择指南

```
┌─────────────────────────────────────────────────────────────┐
│                  超参数推荐值                                │
├──────────────────┬──────────────────────────────────────────┤
│      参数        │              推荐值                      │
├──────────────────┼──────────────────────────────────────────┤
│  学习率（全参数） │  1e-5 ~ 5e-5                            │
│  学习率（LoRA）   │  1e-4 ~ 3e-4                            │
│  Batch Size      │  根据显存，有效 batch 16-128             │
│  Epochs          │  1-5（数据少可多，数据多可少）            │
│  Weight Decay    │  0.01 ~ 0.1                              │
│  Warmup Ratio    │  0.03 ~ 0.1                              │
│  LoRA Rank       │  8-64（复杂任务可更高）                   │
│  LoRA Alpha      │  16-64（通常 = 2×Rank）                  │
│  Max Seq Length  │  根据任务，512-4096                      │
└──────────────────┴──────────────────────────────────────────┘
```

---

## 11. 总结与展望

### 11.1 微调技术总结

```
┌─────────────────────────────────────────────────────────────┐
│                    核心要点总结                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ✅ 微调的本质                                              │
│     将预训练模型适配到特定任务/领域                          │
│                                                             │
│  ✅ 方法选择                                                │
│     资源充足 → 全参数微调                                   │
│     资源受限 → LoRA/QLoRA                                   │
│     多任务场景 → Adapter                                    │
│                                                             │
│  ✅ 数据优先                                                │
│     质量 > 数量，精心设计的小数据集可能超过大而杂的数据集     │
│                                                             │
│  ✅ 评估完整                                                │
│     任务指标 + 通用能力 + 对齐质量，全面评估                 │
│                                                             │
│  ✅ 防止遗忘                                                │
│     使用 PEFT、混合数据、正则化等策略                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 11.2 技术发展趋势

```
┌─────────────────────────────────────────────────────────────┐
│                    未来发展方向                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  🔮 更高效的 PEFT 方法                                      │
│     更少参数达到更好效果                                    │
│                                                             │
│  🔮 自动化微调                                              │
│     自动选择超参数、方法、数据配比                          │
│                                                             │
│  🔮 持续学习                                                │
│     模型能够不断学习新知识而不遗忘旧知识                    │
│                                                             │
│  🔮 多模态微调                                              │
│     统一的视觉-语言-音频微调框架                            │
│                                                             │
│  🔮 合成数据                                                │
│     用模型生成高质量训练数据                                │
│                                                             │
│  🔮 端侧微调                                                │
│     在设备端进行个性化微调                                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 11.3 选型决策树

```
                        开始微调
                          │
                          ▼
                  ┌───────────────┐
                  │ 有足够GPU资源？│
                  └───────┬───────┘
                    ┌─────┴─────┐
                   Yes         No
                    │           │
                    ▼           ▼
            ┌─────────────┐  ┌─────────────┐
            │ 追求最佳效果？│  │  使用 QLoRA │
            └──────┬──────┘  └─────────────┘
              ┌────┴────┐
             Yes       No
              │         │
              ▼         ▼
        ┌──────────┐ ┌──────────┐
        │ 全参数微调│ │  LoRA    │
        └──────────┘ └──────────┘
```

---

## 参考文献

1. **LoRA**: Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models" (2021)
2. **QLoRA**: Dettmers et al., "QLoRA: Efficient Finetuning of Quantized LLMs" (2023)
3. **Adapter**: Houlsby et al., "Parameter-Efficient Transfer Learning for NLP" (2019)
4. **Prefix-Tuning**: Li & Liang, "Prefix-Tuning: Optimizing Continuous Prompts for Generation" (2021)
5. **RLHF**: Ouyang et al., "Training language models to follow instructions with human feedback" (2022)
6. **DPO**: Rafailov et al., "Direct Preference Optimization" (2023)
7. **DoRA**: Liu et al., "DoRA: Weight-Decomposed Low-Rank Adaptation" (2024)

---

_文档版本: 1.0 | 最后更新: 2024_
