"""
DPO è®­ç»ƒ
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ DPO åŸç†
    2. æŒæ¡åå¥½æ•°æ®å‡†å¤‡
    3. ä½¿ç”¨ TRL è¿›è¡Œ DPO

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Direct Preference Optimization
    - åå¥½å¯¹ (chosen/rejected)
    - RLHF æ›¿ä»£æ–¹æ¡ˆ

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install trl transformers peft
"""

import os
from dotenv import load_dotenv

load_dotenv()


def dpo_overview():
    """DPO æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šDPO æ¦‚è¿°")
    print("=" * 60)

    print("""
    Direct Preference Optimization (DPO)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ä»¥ç¬¦åˆäººç±»åå¥½ã€‚
    
    
    RLHF vs DPO
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    RLHF (å¤æ‚)                          â”‚
    â”‚                                                         â”‚
    â”‚   åå¥½æ•°æ® â”€â–¶ è®­ç»ƒå¥–åŠ±æ¨¡å‹ â”€â–¶ PPOè®­ç»ƒ â”€â–¶ å¯¹é½æ¨¡å‹        â”‚
    â”‚                    â”‚              â”‚                     â”‚
    â”‚                    â–¼              â–¼                     â”‚
    â”‚              éœ€è¦ç»´æŠ¤ä¸¤ä¸ªæ¨¡å‹ï¼Œè®­ç»ƒä¸ç¨³å®š                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    DPO (ç®€å•)                           â”‚
    â”‚                                                         â”‚
    â”‚   åå¥½æ•°æ® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ DPOè®­ç»ƒ â”€â–¶ å¯¹é½æ¨¡å‹        â”‚
    â”‚                                 â”‚                       â”‚
    â”‚                                 â–¼                       â”‚
    â”‚                    ç›´æ¥ä¼˜åŒ–ï¼Œè®­ç»ƒç¨³å®š                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    
    DPO åŸç†
    â”€â”€â”€â”€â”€â”€â”€
    
    æ ¸å¿ƒæ€æƒ³ï¼šå°†å¥–åŠ±æ¨¡å‹çš„ä¼˜åŒ–è½¬æ¢ä¸ºç­–ç•¥æ¨¡å‹çš„ä¼˜åŒ–
    
    ç›®æ ‡ï¼šæœ€å¤§åŒ– chosen å›ç­”çš„æ¦‚ç‡ï¼Œæœ€å°åŒ– rejected å›ç­”çš„æ¦‚ç‡
    
    Loss = -log(Ïƒ(Î² Ã— (log Ï€(chosen) - log Ï€(rejected) 
                       - log Ï€_ref(chosen) + log Ï€_ref(rejected))))
    
    å…¶ä¸­:
    - Ï€: å½“å‰ç­–ç•¥æ¨¡å‹
    - Ï€_ref: å‚è€ƒæ¨¡å‹ (å†»ç»“çš„ SFT æ¨¡å‹)
    - Î²: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åç¦»ç¨‹åº¦
    """)


def preference_data():
    """åå¥½æ•°æ®"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šåå¥½æ•°æ®å‡†å¤‡")
    print("=" * 60)

    print("""
    æ•°æ®æ ¼å¼
    â”€â”€â”€â”€â”€â”€â”€
    
    {
        "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "chosen": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿ...",
        "rejected": "æœºå™¨å­¦ä¹ å°±æ˜¯è®©æœºå™¨è‡ªå·±å­¦ä¹ "
    }
    
    
    æ•°æ®æ¥æº
    â”€â”€â”€â”€â”€â”€â”€
    
    1. äººå·¥æ ‡æ³¨
       - ç»™å®š promptï¼Œäººå·¥æ¯”è¾ƒä¸¤ä¸ªå›ç­”
       - è´¨é‡æœ€é«˜ï¼Œæˆæœ¬æœ€é«˜
    
    2. GPT-4 ç”Ÿæˆ
       - ä½¿ç”¨ GPT-4 ç”Ÿæˆ chosen å›ç­”
       - ä½¿ç”¨è¾ƒå¼±æ¨¡å‹ç”Ÿæˆ rejected å›ç­”
    
    3. æ¨¡å‹ç”Ÿæˆ + äººå·¥ç­›é€‰
       - æ¨¡å‹ç”Ÿæˆå¤šä¸ªå›ç­”
       - äººå·¥é€‰æ‹©æœ€å¥½å’Œæœ€å·®çš„
    
    
    æ•°æ®è´¨é‡åŸåˆ™
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. Chosen å’Œ Rejected è¦æœ‰æ˜æ˜¾åŒºåˆ†
    2. å¯¹æ¯”åº”è¯¥åŸºäºå…³é”®è´¨é‡ç»´åº¦:
       - å‡†ç¡®æ€§
       - æœ‰ç”¨æ€§
       - å®‰å…¨æ€§
    3. é¿å…è¿‡äºç›¸ä¼¼çš„å›ç­”å¯¹
    """)


def dpo_with_trl():
    """ä½¿ç”¨ TRL è¿›è¡Œ DPO"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ TRL è¿›è¡Œ DPO")
    print("=" * 60)

    code_example = """
    from trl import DPOTrainer, DPOConfig
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from datasets import load_dataset
    from peft import LoraConfig

    # 1. åŠ è½½ SFT æ¨¡å‹ä½œä¸ºèµ·ç‚¹
    model_name = "./sft_model"  # æˆ– SFT åçš„æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. åŠ è½½å‚è€ƒæ¨¡å‹ (å†»ç»“)
    ref_model = AutoModelForCausalLM.from_pretrained(model_name)

    # 3. åŠ è½½åå¥½æ•°æ®é›†
    dataset = load_dataset("Anthropic/hh-rlhf", split="train")

    # æ•°æ®æ ¼å¼åº”åŒ…å«: prompt, chosen, rejected

    # 4. LoRA é…ç½®
    peft_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],
    )

    # 5. DPO é…ç½®
    training_args = DPOConfig(
        output_dir="./dpo_output",
        num_train_epochs=1,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=5e-5,
        beta=0.1,              # DPO æ¸©åº¦å‚æ•°
        max_length=512,
        max_prompt_length=256,
        fp16=True,
        logging_steps=10,
    )

    # 6. åˆ›å»º DPOTrainer
    trainer = DPOTrainer(
        model=model,
        ref_model=ref_model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        peft_config=peft_config,
    )

    # 7. è®­ç»ƒ
    trainer.train()

    # 8. ä¿å­˜
    trainer.save_model("./dpo_model")
    """

    print(code_example)


def dpo_variants():
    """DPO å˜ä½“"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šDPO å˜ä½“å’ŒæŠ€å·§")
    print("=" * 60)

    print("""
    DPO å˜ä½“
    â”€â”€â”€â”€â”€â”€â”€
    
    1. IPO (Identity Preference Optimization)
       - ç®€åŒ– DPO çš„æ•°å­¦æ¨å¯¼
       - æ›´ç¨³å®šçš„è®­ç»ƒ
    
    2. KTO (Kahneman-Tversky Optimization)
       - ä¸éœ€è¦æˆå¯¹æ•°æ®
       - åªéœ€è¦æ ‡è®°å¥½/å
    
    3. ORPO (Odds Ratio Preference Optimization)
       - ä¸éœ€è¦å‚è€ƒæ¨¡å‹
       - æ˜¾å­˜æ›´çœ
    
    
    è¶…å‚æ•°è°ƒä¼˜
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1. Î² (æ¸©åº¦å‚æ•°)
       - è¾ƒå¤§ (0.5): æ›´ä¿å®ˆï¼Œæ¥è¿‘å‚è€ƒæ¨¡å‹
       - è¾ƒå° (0.1): æ›´æ¿€è¿›ï¼Œåç¦»æ›´å¤§
       - æ¨è: 0.1 - 0.5
    
    2. å­¦ä¹ ç‡
       - é€šå¸¸æ¯” SFT ä½: 1e-5 åˆ° 1e-4
    
    3. è®­ç»ƒè½®æ•°
       - é€šå¸¸ 1-3 è½®
       - è¿‡å¤šå®¹æ˜“è¿‡æ‹Ÿåˆ
    
    
    æœ€ä½³å®è·µ
    â”€â”€â”€â”€â”€â”€â”€
    
    1. ç¡®ä¿ä»è‰¯å¥½çš„ SFT æ¨¡å‹å¼€å§‹
    2. ä½¿ç”¨é«˜è´¨é‡åå¥½æ•°æ®
    3. ç›‘æ§ chosen/rejected çš„æ¦‚ç‡åˆ†ç¦»åº¦
    4. æ³¨æ„é˜²æ­¢è¿‡æ‹Ÿåˆ
    """)


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šæ„å»ºåå¥½æ•°æ®
        ä¸ºç‰¹å®šä»»åŠ¡æ„å»º 100 æ¡åå¥½å¯¹
    
    ç»ƒä¹  2ï¼šDPO è®­ç»ƒ
        åœ¨ SFT æ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œ DPO
    
    ç»ƒä¹  3ï¼šè¯„ä¼°å¯¹æ¯”
        æ¯”è¾ƒ SFT å’Œ DPO æ¨¡å‹çš„æ•ˆæœ
    
    æ€è€ƒé¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€
    1. DPO å’Œ RLHF å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ
    2. ä»€ä¹ˆæƒ…å†µä¸‹ DPO æ•ˆæœä¸å¥½ï¼Ÿ
    """)


def main():
    print("ğŸ¯ DPO è®­ç»ƒ")
    print("=" * 60)
    dpo_overview()
    preference_data()
    dpo_with_trl()
    dpo_variants()
    exercises()
    print("\nâœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š09-model-merging.py")


if __name__ == "__main__":
    main()
