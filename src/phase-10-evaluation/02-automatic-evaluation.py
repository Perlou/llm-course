"""
è‡ªåŠ¨è¯„ä¼°
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡ä¼ ç»Ÿ NLP è¯„ä¼°æŒ‡æ ‡çš„å®ç°
    2. ä½¿ç”¨åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹
    3. å®ç°è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Perplexityï¼šå›°æƒ‘åº¦
    - BLEU/ROUGEï¼šn-gram åŒ¹é…
    - åŸºå‡†æµ‹è¯•ï¼šæ ‡å‡†åŒ–è¯„ä¼°

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install transformers torch nltk rouge-score evaluate
"""

import math
from typing import List, Dict


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå›°æƒ‘åº¦ ====================


def perplexity_evaluation():
    """å›°æƒ‘åº¦è¯„ä¼°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå›°æƒ‘åº¦ (Perplexity)")
    print("=" * 60)

    print("""
    ğŸ“Œ å›°æƒ‘åº¦è§£é‡Šï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Perplexity = exp(Cross-Entropy Loss)                  â”‚
    â”‚                                                        â”‚
    â”‚  â€¢ PPL = 1    â†’  å®Œç¾é¢„æµ‹ï¼ˆä¸å¯èƒ½è¾¾åˆ°ï¼‰                â”‚
    â”‚  â€¢ PPL = 10   â†’  å¹³å‡æ¯ä¸ªä½ç½®æœ‰10ä¸ªç­‰æ¦‚ç‡é€‰æ‹©         â”‚
    â”‚  â€¢ PPL = 100  â†’  æ¨¡å‹è¾ƒå›°æƒ‘ï¼Œé¢„æµ‹ä¸ç¡®å®š               â”‚
    â”‚                                                        â”‚
    â”‚  æ³¨æ„ï¼šPPLåªèƒ½åœ¨ç›¸åŒè¯è¡¨çš„æ¨¡å‹é—´æ¯”è¾ƒ                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    code = '''
import torch
import math
from transformers import AutoModelForCausalLM, AutoTokenizer

def calculate_perplexity(model, tokenizer, text):
    """è®¡ç®—æ–‡æœ¬çš„å›°æƒ‘åº¦"""
    encodings = tokenizer(text, return_tensors='pt')

    with torch.no_grad():
        outputs = model(**encodings, labels=encodings['input_ids'])
        loss = outputs.loss

    perplexity = math.exp(loss.item())
    return perplexity

# ä½¿ç”¨ç¤ºä¾‹
# model = AutoModelForCausalLM.from_pretrained("gpt2")
# tokenizer = AutoTokenizer.from_pretrained("gpt2")
# text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚"
# ppl = calculate_perplexity(model, tokenizer, text)
# print(f"Perplexity: {ppl:.2f}")
'''
    print(code)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šBLEU å’Œ ROUGE ====================


def bleu_rouge_evaluation():
    """BLEU å’Œ ROUGE è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šBLEU å’Œ ROUGE")
    print("=" * 60)

    bleu_code = '''
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def calculate_bleu(reference, candidate):
    """è®¡ç®— BLEU åˆ†æ•°ï¼ˆç”¨äºç¿»è¯‘å’Œç”Ÿæˆï¼‰"""
    reference_tokens = [reference.split()]
    candidate_tokens = candidate.split()

    # ä½¿ç”¨å¹³æ»‘å‡½æ•°å¤„ç†çŸ­æ–‡æœ¬
    smoothie = SmoothingFunction().method4

    score = sentence_bleu(
        reference_tokens,
        candidate_tokens,
        weights=(0.25, 0.25, 0.25, 0.25),  # BLEU-4
        smoothing_function=smoothie
    )
    return score

# ç¤ºä¾‹
reference = "The cat sits on the mat"
candidate = "The cat is sitting on the mat"
bleu = calculate_bleu(reference, candidate)
print(f"BLEU: {bleu:.4f}")
'''
    print(bleu_code)

    rouge_code = '''
from rouge_score import rouge_scorer

def calculate_rouge(reference, candidate):
    """è®¡ç®— ROUGE åˆ†æ•°ï¼ˆç”¨äºæ‘˜è¦è¯„ä¼°ï¼‰"""
    scorer = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'],
        use_stemmer=True
    )
    scores = scorer.score(reference, candidate)

    return {
        'rouge1': scores['rouge1'].fmeasure,
        'rouge2': scores['rouge2'].fmeasure,
        'rougeL': scores['rougeL'].fmeasure
    }
'''
    print(rouge_code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼° ====================


def benchmark_evaluation():
    """åŸºå‡†æµ‹è¯•è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼°")
    print("=" * 60)

    code = '''
from tqdm import tqdm

class BenchmarkEvaluator:
    """åŸºå‡†æµ‹è¯•è¯„ä¼°å™¨"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def evaluate_mmlu(self, dataset):
        """è¯„ä¼° MMLUï¼ˆçŸ¥è¯†é—®ç­”ï¼‰"""
        results = {'correct': 0, 'total': 0}

        for item in tqdm(dataset):
            # æ„é€ å¤šé€‰é¢˜ prompt
            prompt = self._format_mmlu_prompt(item)
            response = self._generate(prompt)
            predicted = self._extract_choice(response)

            if predicted == item['answer']:
                results['correct'] += 1
            results['total'] += 1

        results['accuracy'] = results['correct'] / results['total']
        return results

    def evaluate_gsm8k(self, dataset, use_cot=True):
        """è¯„ä¼° GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰"""
        correct = 0
        total = len(dataset)

        for item in tqdm(dataset):
            if use_cot:
                prompt = f"""Question: {item['question']}

Let's solve this step by step:
"""
            else:
                prompt = f"Question: {item['question']}\\nAnswer:"

            response = self._generate(prompt)
            predicted = self._extract_number(response)

            if str(predicted) == str(item['answer']):
                correct += 1

        return {'accuracy': correct / total}

    def _generate(self, prompt, max_tokens=256):
        inputs = self.tokenizer(prompt, return_tensors='pt')
        outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ Evaluate åº“ ====================


def evaluate_library():
    """ä½¿ç”¨ Evaluate åº“"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ HuggingFace Evaluate åº“")
    print("=" * 60)

    code = """
import evaluate

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

# è®¡ç®— BLEU
predictions = ["hello world", "how are you"]
references = [["hello world"], ["how are you doing"]]
bleu_result = bleu.compute(predictions=predictions, references=references)
print(f"BLEU: {bleu_result['bleu']:.4f}")

# è®¡ç®— ROUGE
rouge_result = rouge.compute(predictions=predictions, references=references)
print(f"ROUGE-L: {rouge_result['rougeL']:.4f}")

# è®¡ç®— BERTScoreï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
bert_result = bertscore.compute(
    predictions=predictions,
    references=references,
    lang="en"
)
print(f"BERTScore F1: {sum(bert_result['f1'])/len(bert_result['f1']):.4f}")
"""
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ ====================


def automated_pipeline():
    """è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹")
    print("=" * 60)

    code = '''
class AutomatedEvaluator:
    """è‡ªåŠ¨åŒ–è¯„ä¼°å™¨"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.results = {}

    def run_full_evaluation(self, test_sets):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        for name, dataset in test_sets.items():
            print(f"è¯„ä¼° {name}...")

            if name == "mmlu":
                self.results[name] = self._evaluate_mmlu(dataset)
            elif name == "gsm8k":
                self.results[name] = self._evaluate_gsm8k(dataset)
            elif name == "humaneval":
                self.results[name] = self._evaluate_humaneval(dataset)

        return self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = "=" * 50 + "\\n"
        report += "æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\\n"
        report += "=" * 50 + "\\n"

        for name, result in self.results.items():
            report += f"\\n{name.upper()}:\\n"
            for metric, value in result.items():
                report += f"  {metric}: {value:.4f}\\n"

        return report
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªè®¡ç®— BLEU å’Œ ROUGE çš„å‡½æ•°

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
        from rouge_score import rouge_scorer
        from typing import Dict, List
        
        class TextEvaluator:
            '''æ–‡æœ¬è¯„ä¼°å™¨'''
            
            def __init__(self):
                self.rouge_scorer = rouge_scorer.RougeScorer(
                    ['rouge1', 'rouge2', 'rougeL'],
                    use_stemmer=True
                )
                self.smoothie = SmoothingFunction().method4
            
            def calculate_bleu(
                self, 
                reference: str, 
                candidate: str,
                weights: tuple = (0.25, 0.25, 0.25, 0.25)
            ) -> float:
                '''è®¡ç®— BLEU åˆ†æ•°'''
                ref_tokens = [reference.split()]
                cand_tokens = candidate.split()
                
                return sentence_bleu(
                    ref_tokens, 
                    cand_tokens,
                    weights=weights,
                    smoothing_function=self.smoothie
                )
            
            def calculate_rouge(
                self, 
                reference: str, 
                candidate: str
            ) -> Dict[str, float]:
                '''è®¡ç®— ROUGE åˆ†æ•°'''
                scores = self.rouge_scorer.score(reference, candidate)
                return {
                    'rouge1': scores['rouge1'].fmeasure,
                    'rouge2': scores['rouge2'].fmeasure,
                    'rougeL': scores['rougeL'].fmeasure
                }
            
            def evaluate(
                self, 
                reference: str, 
                candidate: str
            ) -> Dict[str, float]:
                '''ç»¼åˆè¯„ä¼°'''
                return {
                    'bleu': self.calculate_bleu(reference, candidate),
                    **self.calculate_rouge(reference, candidate)
                }
        
        # ä½¿ç”¨ç¤ºä¾‹
        evaluator = TextEvaluator()
        ref = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯"
        cand = "æœºå™¨å­¦ä¹ å±äºäººå·¥æ™ºèƒ½é¢†åŸŸ"
        scores = evaluator.evaluate(ref, cand)
        print(f"BLEU: {scores['bleu']:.4f}, ROUGE-L: {scores['rougeL']:.4f}")
        ```
    
    ç»ƒä¹  2ï¼šåœ¨ MMLU å­é›†ä¸Šè¯„ä¼°ä¸€ä¸ªå°æ¨¡å‹

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from datasets import load_dataset
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from tqdm import tqdm
        
        class MMLUEvaluator:
            '''MMLU è¯„ä¼°å™¨'''
            
            def __init__(self, model_name: str):
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForCausalLM.from_pretrained(model_name)
            
            def format_prompt(self, item: dict) -> str:
                '''æ ¼å¼åŒ– MMLU é—®é¢˜'''
                choices = ['A', 'B', 'C', 'D']
                prompt = f"Question: {item['question']}\\n"
                for i, choice in enumerate(item['choices']):
                    prompt += f"{choices[i]}. {choice}\\n"
                prompt += "Answer:"
                return prompt
            
            def evaluate_subset(
                self, 
                subject: str = "abstract_algebra",
                split: str = "test"
            ) -> dict:
                '''è¯„ä¼° MMLU å­é›†'''
                dataset = load_dataset("cais/mmlu", subject, split=split)
                
                correct = 0
                total = 0
                
                for item in tqdm(dataset):
                    prompt = self.format_prompt(item)
                    pred = self._generate_answer(prompt)
                    
                    if pred == ['A', 'B', 'C', 'D'][item['answer']]:
                        correct += 1
                    total += 1
                
                return {
                    'subject': subject,
                    'accuracy': correct / total,
                    'correct': correct,
                    'total': total
                }
        
        # ä½¿ç”¨ç¤ºä¾‹
        # evaluator = MMLUEvaluator("Qwen/Qwen2-0.5B-Instruct")
        # result = evaluator.evaluate_subset("high_school_physics")
        # print(f"Accuracy: {result['accuracy']:.2%}")
        ```

    æ€è€ƒé¢˜ï¼šä¸ºä»€ä¹ˆè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æœ‰æ—¶ä¸äººç±»è¯„ä¼°ä¸ä¸€è‡´ï¼Ÿ

        âœ… ç­”ï¼š
        1. è¯æ±‡åŒ¹é…å±€é™ - BLEU/ROUGE åŸºäº n-gram åŒ¹é…ï¼Œæ— æ³•ç†è§£è¯­ä¹‰ç­‰ä»·
        2. åˆ›é€ æ€§å¿½è§† - è‡ªåŠ¨æŒ‡æ ‡æƒ©ç½šåˆ›é€ æ€§è¡¨è¾¾ï¼Œå³ä½¿æ›´å¥½
        3. ä¸»è§‚åå¥½ - äººç±»åå¥½éš¾ä»¥é‡åŒ–ï¼ˆå¦‚å¹½é»˜æ„Ÿã€æ–‡é£ï¼‰
        4. ä¸Šä¸‹æ–‡ç†è§£ - è‡ªåŠ¨æŒ‡æ ‡éš¾ä»¥è¯„ä¼°ä¸Šä¸‹æ–‡é€‚å½“æ€§
        5. ä»»åŠ¡ç‰¹å¼‚æ€§ - é€šç”¨æŒ‡æ ‡æ— æ³•æ•æ‰ç‰¹å®šä»»åŠ¡çš„å…³é”®è¦ç´ 
    """)


def main():
    perplexity_evaluation()
    bleu_rouge_evaluation()
    benchmark_evaluation()
    evaluate_library()
    automated_pipeline()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š03-llm-as-judge.py")


if __name__ == "__main__":
    main()
