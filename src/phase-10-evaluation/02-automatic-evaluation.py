"""
è‡ªåŠ¨è¯„ä¼°
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. æŒæ¡ä¼ ç»Ÿ NLP è¯„ä¼°æŒ‡æ ‡çš„å®ç°
    2. ä½¿ç”¨åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹
    3. å®ç°è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Perplexityï¼šå›°æƒ‘åº¦
    - BLEU/ROUGEï¼šn-gram åŒ¹é…
    - åŸºå‡†æµ‹è¯•ï¼šæ ‡å‡†åŒ–è¯„ä¼°

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install transformers torch nltk rouge-score evaluate
"""

import math
from typing import List, Dict


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå›°æƒ‘åº¦ ====================


def perplexity_evaluation():
    """å›°æƒ‘åº¦è¯„ä¼°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå›°æƒ‘åº¦ (Perplexity)")
    print("=" * 60)

    print("""
    ğŸ“Œ å›°æƒ‘åº¦è§£é‡Šï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Perplexity = exp(Cross-Entropy Loss)                  â”‚
    â”‚                                                        â”‚
    â”‚  â€¢ PPL = 1    â†’  å®Œç¾é¢„æµ‹ï¼ˆä¸å¯èƒ½è¾¾åˆ°ï¼‰                â”‚
    â”‚  â€¢ PPL = 10   â†’  å¹³å‡æ¯ä¸ªä½ç½®æœ‰10ä¸ªç­‰æ¦‚ç‡é€‰æ‹©         â”‚
    â”‚  â€¢ PPL = 100  â†’  æ¨¡å‹è¾ƒå›°æƒ‘ï¼Œé¢„æµ‹ä¸ç¡®å®š               â”‚
    â”‚                                                        â”‚
    â”‚  æ³¨æ„ï¼šPPLåªèƒ½åœ¨ç›¸åŒè¯è¡¨çš„æ¨¡å‹é—´æ¯”è¾ƒ                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    code = '''
import torch
import math
from transformers import AutoModelForCausalLM, AutoTokenizer

def calculate_perplexity(model, tokenizer, text):
    """è®¡ç®—æ–‡æœ¬çš„å›°æƒ‘åº¦"""
    encodings = tokenizer(text, return_tensors='pt')

    with torch.no_grad():
        outputs = model(**encodings, labels=encodings['input_ids'])
        loss = outputs.loss

    perplexity = math.exp(loss.item())
    return perplexity

# ä½¿ç”¨ç¤ºä¾‹
# model = AutoModelForCausalLM.from_pretrained("gpt2")
# tokenizer = AutoTokenizer.from_pretrained("gpt2")
# text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚"
# ppl = calculate_perplexity(model, tokenizer, text)
# print(f"Perplexity: {ppl:.2f}")
'''
    print(code)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šBLEU å’Œ ROUGE ====================


def bleu_rouge_evaluation():
    """BLEU å’Œ ROUGE è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šBLEU å’Œ ROUGE")
    print("=" * 60)

    bleu_code = '''
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def calculate_bleu(reference, candidate):
    """è®¡ç®— BLEU åˆ†æ•°ï¼ˆç”¨äºç¿»è¯‘å’Œç”Ÿæˆï¼‰"""
    reference_tokens = [reference.split()]
    candidate_tokens = candidate.split()

    # ä½¿ç”¨å¹³æ»‘å‡½æ•°å¤„ç†çŸ­æ–‡æœ¬
    smoothie = SmoothingFunction().method4

    score = sentence_bleu(
        reference_tokens,
        candidate_tokens,
        weights=(0.25, 0.25, 0.25, 0.25),  # BLEU-4
        smoothing_function=smoothie
    )
    return score

# ç¤ºä¾‹
reference = "The cat sits on the mat"
candidate = "The cat is sitting on the mat"
bleu = calculate_bleu(reference, candidate)
print(f"BLEU: {bleu:.4f}")
'''
    print(bleu_code)

    rouge_code = '''
from rouge_score import rouge_scorer

def calculate_rouge(reference, candidate):
    """è®¡ç®— ROUGE åˆ†æ•°ï¼ˆç”¨äºæ‘˜è¦è¯„ä¼°ï¼‰"""
    scorer = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'],
        use_stemmer=True
    )
    scores = scorer.score(reference, candidate)

    return {
        'rouge1': scores['rouge1'].fmeasure,
        'rouge2': scores['rouge2'].fmeasure,
        'rougeL': scores['rougeL'].fmeasure
    }
'''
    print(rouge_code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼° ====================


def benchmark_evaluation():
    """åŸºå‡†æµ‹è¯•è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºå‡†æµ‹è¯•è¯„ä¼°")
    print("=" * 60)

    code = '''
from tqdm import tqdm

class BenchmarkEvaluator:
    """åŸºå‡†æµ‹è¯•è¯„ä¼°å™¨"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def evaluate_mmlu(self, dataset):
        """è¯„ä¼° MMLUï¼ˆçŸ¥è¯†é—®ç­”ï¼‰"""
        results = {'correct': 0, 'total': 0}

        for item in tqdm(dataset):
            # æ„é€ å¤šé€‰é¢˜ prompt
            prompt = self._format_mmlu_prompt(item)
            response = self._generate(prompt)
            predicted = self._extract_choice(response)

            if predicted == item['answer']:
                results['correct'] += 1
            results['total'] += 1

        results['accuracy'] = results['correct'] / results['total']
        return results

    def evaluate_gsm8k(self, dataset, use_cot=True):
        """è¯„ä¼° GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰"""
        correct = 0
        total = len(dataset)

        for item in tqdm(dataset):
            if use_cot:
                prompt = f"""Question: {item['question']}

Let's solve this step by step:
"""
            else:
                prompt = f"Question: {item['question']}\\nAnswer:"

            response = self._generate(prompt)
            predicted = self._extract_number(response)

            if str(predicted) == str(item['answer']):
                correct += 1

        return {'accuracy': correct / total}

    def _generate(self, prompt, max_tokens=256):
        inputs = self.tokenizer(prompt, return_tensors='pt')
        outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ Evaluate åº“ ====================


def evaluate_library():
    """ä½¿ç”¨ Evaluate åº“"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ HuggingFace Evaluate åº“")
    print("=" * 60)

    code = """
import evaluate

# åŠ è½½è¯„ä¼°æŒ‡æ ‡
bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

# è®¡ç®— BLEU
predictions = ["hello world", "how are you"]
references = [["hello world"], ["how are you doing"]]
bleu_result = bleu.compute(predictions=predictions, references=references)
print(f"BLEU: {bleu_result['bleu']:.4f}")

# è®¡ç®— ROUGE
rouge_result = rouge.compute(predictions=predictions, references=references)
print(f"ROUGE-L: {rouge_result['rougeL']:.4f}")

# è®¡ç®— BERTScoreï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
bert_result = bertscore.compute(
    predictions=predictions,
    references=references,
    lang="en"
)
print(f"BERTScore F1: {sum(bert_result['f1'])/len(bert_result['f1']):.4f}")
"""
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ ====================


def automated_pipeline():
    """è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹")
    print("=" * 60)

    code = '''
class AutomatedEvaluator:
    """è‡ªåŠ¨åŒ–è¯„ä¼°å™¨"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.results = {}

    def run_full_evaluation(self, test_sets):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        for name, dataset in test_sets.items():
            print(f"è¯„ä¼° {name}...")

            if name == "mmlu":
                self.results[name] = self._evaluate_mmlu(dataset)
            elif name == "gsm8k":
                self.results[name] = self._evaluate_gsm8k(dataset)
            elif name == "humaneval":
                self.results[name] = self._evaluate_humaneval(dataset)

        return self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = "=" * 50 + "\\n"
        report += "æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\\n"
        report += "=" * 50 + "\\n"

        for name, result in self.results.items():
            report += f"\\n{name.upper()}:\\n"
            for metric, value in result.items():
                report += f"  {metric}: {value:.4f}\\n"

        return report
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªè®¡ç®— BLEU å’Œ ROUGE çš„å‡½æ•°
    ç»ƒä¹  2ï¼šåœ¨ MMLU å­é›†ä¸Šè¯„ä¼°ä¸€ä¸ªå°æ¨¡å‹

    æ€è€ƒé¢˜ï¼šä¸ºä»€ä¹ˆè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æœ‰æ—¶ä¸äººç±»è¯„ä¼°ä¸ä¸€è‡´ï¼Ÿ
    ç­”æ¡ˆï¼šè‡ªåŠ¨æŒ‡æ ‡ä¸»è¦åŸºäºè¯æ±‡åŒ¹é…æˆ–è¯­è¨€æ¨¡å‹æ¦‚ç‡ï¼Œ
          éš¾ä»¥æ•æ‰è¯­ä¹‰ç­‰ä»·ã€åˆ›é€ æ€§è¡¨è¾¾ç­‰äººç±»åå¥½ã€‚
    """)


def main():
    perplexity_evaluation()
    bleu_rouge_evaluation()
    benchmark_evaluation()
    evaluate_library()
    automated_pipeline()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š03-llm-as-judge.py")


if __name__ == "__main__":
    main()
