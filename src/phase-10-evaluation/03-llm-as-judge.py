"""
LLM ä½œä¸ºè¯„åˆ¤è€…
=============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ LLM-as-Judge çš„åŸç†å’Œä¼˜åŠ¿
    2. å®ç°å•æ¨¡å‹è¯„ä¼°å’Œæˆå¯¹æ¯”è¾ƒ
    3. æŒæ¡è¯„ä¼° prompt è®¾è®¡æŠ€å·§

æ ¸å¿ƒæ¦‚å¿µï¼š
    - LLM-as-Judgeï¼šç”¨ LLM è¯„ä¼° LLM
    - å•ç‚¹è¯„åˆ†ï¼šå¯¹å•ä¸ªå›å¤æ‰“åˆ†
    - æˆå¯¹æ¯”è¾ƒï¼šæ¯”è¾ƒä¸¤ä¸ªå›å¤çš„ä¼˜åŠ£

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install openai
    - éœ€è¦ Google API Key
"""

import json
import os
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šLLM-as-Judge æ¦‚è¿° ====================


def introduction():
    """LLM-as-Judge æ¦‚è¿°"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šLLM-as-Judge æ¦‚è¿°")
    print("=" * 60)

    print("""
    ğŸ“Œ ä¸ºä»€ä¹ˆä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…ï¼Ÿ
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â€¢ äººå·¥è¯„ä¼°æˆæœ¬é«˜ã€é€Ÿåº¦æ…¢                             â”‚
    â”‚  â€¢ è‡ªåŠ¨æŒ‡æ ‡éš¾ä»¥è¯„ä¼°å¼€æ”¾å¼ç”Ÿæˆ                          â”‚
    â”‚  â€¢ LLM å¯ä»¥ç†è§£è¯­ä¹‰å’Œæ„å›¾                              â”‚
    â”‚  â€¢ å¯æ‰©å±•æ€§å¼ºï¼Œæ”¯æŒå¤§è§„æ¨¡è¯„ä¼°                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ è¯„ä¼°æ¨¡å¼ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   å•ç‚¹è¯„åˆ†     â”‚ å¯¹å•ä¸ªå›å¤æŒ‰å¤šä¸ªç»´åº¦æ‰“åˆ† (1-10)      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   æˆå¯¹æ¯”è¾ƒ     â”‚ æ¯”è¾ƒä¸¤ä¸ªå›å¤ï¼Œé€‰æ‹©æ›´å¥½çš„ä¸€ä¸ª          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   å‚è€ƒç­”æ¡ˆæ¯”è¾ƒ â”‚ ä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”æ‰“åˆ†                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ å¸¸ç”¨è¯„åˆ¤æ¨¡å‹ï¼š
    - GPT-4 (æœ€å¸¸ç”¨ï¼Œæ•ˆæœå¥½)
    - Claude 3.5
    - å¼€æºæ¨¡å‹ (éœ€è¦è¶³å¤Ÿå¤§)
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šå•ç‚¹è¯„åˆ† ====================


def single_point_scoring():
    """å•ç‚¹è¯„åˆ†"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šå•ç‚¹è¯„åˆ†")
    print("=" * 60)

    code = '''
import google.generativeai as genai

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-1.5-flash')

JUDGE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯„ä¼°åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°AIåŠ©æ‰‹çš„å›å¤è´¨é‡ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
1. å‡†ç¡®æ€§ (1-10): å›ç­”æ˜¯å¦äº‹å®æ­£ç¡®
2. ç›¸å…³æ€§ (1-10): å›ç­”æ˜¯å¦åˆ‡é¢˜
3. å®Œæ•´æ€§ (1-10): å›ç­”æ˜¯å¦å…¨é¢
4. æ¸…æ™°åº¦ (1-10): è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
5. æœ‰ç”¨æ€§ (1-10): å¯¹ç”¨æˆ·æ˜¯å¦æœ‰å®é™…å¸®åŠ©

ç”¨æˆ·é—®é¢˜ï¼š
{question}

AIå›å¤ï¼š
{response}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„åˆ†ï¼š
{{
    "accuracy": <score>,
    "relevance": <score>,
    "completeness": <score>,
    "clarity": <score>,
    "helpfulness": <score>,
    "overall": <score>,
    "explanation": "<ç®€è¦è§£é‡Š>"
}}
"""

def evaluate_single(question: str, response: str) -> dict:
    """å•ç‚¹è¯„åˆ†è¯„ä¼°"""
    prompt = JUDGE_PROMPT.format(question=question, response=response)

    result = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return json.loads(result.choices[0].message.content)

# ä½¿ç”¨ç¤ºä¾‹
# scores = evaluate_single(
#     "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
#     "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
# )
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæˆå¯¹æ¯”è¾ƒ ====================


def pairwise_comparison():
    """æˆå¯¹æ¯”è¾ƒ"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šæˆå¯¹æ¯”è¾ƒ")
    print("=" * 60)

    code = '''
PAIRWISE_PROMPT = """
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªAIåŠ©æ‰‹çš„å›å¤ï¼Œåˆ¤æ–­å“ªä¸ªæ›´å¥½ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›å¤Aï¼š
{response_a}

å›å¤Bï¼š
{response_b}

è¯·ä»ä»¥ä¸‹æ–¹é¢æ¯”è¾ƒï¼š
1. å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§
2. å›ç­”çš„å®Œæ•´æ€§å’Œæ·±åº¦
3. è¡¨è¾¾çš„æ¸…æ™°åº¦
4. å¯¹ç”¨æˆ·çš„å¸®åŠ©ç¨‹åº¦

è¯·é€‰æ‹©æ›´å¥½çš„å›å¤ï¼Œå¹¶è§£é‡ŠåŸå› ã€‚
è¾“å‡ºæ ¼å¼ï¼š
{{
    "winner": "A" æˆ– "B" æˆ– "tie",
    "explanation": "<è¯¦ç»†åŸå› >"
}}
"""

def pairwise_compare(question: str, response_a: str, response_b: str) -> dict:
    """æˆå¯¹æ¯”è¾ƒè¯„ä¼°"""
    prompt = PAIRWISE_PROMPT.format(
        question=question,
        response_a=response_a,
        response_b=response_b
    )

    result = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    return json.loads(result.choices[0].message.content)

def evaluate_with_position_swap(question, response_a, response_b):
    """æ¶ˆé™¤ä½ç½®åè§çš„æˆå¯¹æ¯”è¾ƒ"""
    # æ­£å‘æ¯”è¾ƒ
    result1 = pairwise_compare(question, response_a, response_b)
    # äº¤æ¢ä½ç½®å†æ¯”è¾ƒ
    result2 = pairwise_compare(question, response_b, response_a)

    # ç»¼åˆåˆ¤æ–­
    if result1["winner"] == "A" and result2["winner"] == "B":
        return {"winner": "A", "confidence": "high"}
    elif result1["winner"] == "B" and result2["winner"] == "A":
        return {"winner": "B", "confidence": "high"}
    else:
        return {"winner": "tie", "confidence": "low"}
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šMT-Bench é£æ ¼è¯„ä¼° ====================


def mt_bench_style():
    """MT-Bench é£æ ¼è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šMT-Bench é£æ ¼è¯„ä¼°")
    print("=" * 60)

    code = '''
"""
MT-Bench: å¤šè½®å¯¹è¯è¯„ä¼°æ¡†æ¶
- 80ä¸ªé«˜è´¨é‡å¤šè½®å¯¹è¯é—®é¢˜
- è¦†ç›–8ä¸ªèƒ½åŠ›ç»´åº¦
"""

MT_BENCH_CATEGORIES = [
    "writing",      # å†™ä½œ
    "roleplay",     # è§’è‰²æ‰®æ¼”
    "extraction",   # ä¿¡æ¯æå–
    "reasoning",    # æ¨ç†
    "math",         # æ•°å­¦
    "coding",       # ç¼–ç¨‹
    "knowledge",    # çŸ¥è¯†
    "generic"       # é€šç”¨
]

class MTBenchEvaluator:
    def __init__(self, judge_model="gpt-4"):
        self.judge_model = judge_model

    def evaluate_turn(self, question, response, reference=None):
        """è¯„ä¼°å•è½®å¯¹è¯"""
        prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹AIå›å¤çš„è´¨é‡ï¼Œç»™å‡º1-10åˆ†ã€‚

é—®é¢˜ï¼š{question}

å›å¤ï¼š{response}

è¯„åˆ†æ ‡å‡†ï¼š
- 1-3: å·®ï¼Œç­”éæ‰€é—®æˆ–æœ‰æ˜æ˜¾é”™è¯¯
- 4-6: ä¸­ç­‰ï¼ŒåŸºæœ¬å›ç­”äº†é—®é¢˜
- 7-8: å¥½ï¼Œå›ç­”å‡†ç¡®ä¸”æœ‰å¸®åŠ©
- 9-10: ä¼˜ç§€ï¼Œè¶…å‡ºé¢„æœŸçš„é«˜è´¨é‡å›ç­”

è¾“å‡ºæ ¼å¼ï¼š{{"score": <åˆ†æ•°>, "reason": "<ç†ç”±>"}}
"""
        result = model.generate_content(prompt)
        return json.loads(result.text)

    def evaluate_model(self, model, questions):
        """è¯„ä¼°æ¨¡å‹åœ¨æ‰€æœ‰é—®é¢˜ä¸Šçš„è¡¨ç°"""
        scores_by_category = {cat: [] for cat in MT_BENCH_CATEGORIES}

        for q in questions:
            response = model.generate(q["prompt"])
            result = self.evaluate_turn(q["prompt"], response)
            scores_by_category[q["category"]].append(result["score"])

        # è®¡ç®—å¹³å‡åˆ†
        return {cat: sum(s)/len(s) for cat, s in scores_by_category.items() if s}
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ ====================


def best_practices():
    """æœ€ä½³å®è·µ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
    ğŸ“Œ LLM-as-Judge æœ€ä½³å®è·µï¼š

    âœ… è¯„åˆ¤æ¨¡å‹é€‰æ‹©
    - ä½¿ç”¨æ¯”è¢«è¯„ä¼°æ¨¡å‹æ›´å¼ºçš„æ¨¡å‹
    - GPT-4 æ˜¯ç›®å‰æœ€å¸¸ç”¨çš„è¯„åˆ¤æ¨¡å‹
    - å¼€æºæ¨¡å‹éœ€è¦è¶³å¤Ÿå¤§ (70B+)

    âœ… Prompt è®¾è®¡
    - æ˜ç¡®è¯„ä¼°ç»´åº¦å’Œæ ‡å‡†
    - ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º (JSON)
    - è¦æ±‚ç»™å‡ºè¯„åˆ†ç†ç”±

    âœ… æ¶ˆé™¤åè§
    - æˆå¯¹æ¯”è¾ƒæ—¶äº¤æ¢ä½ç½®
    - å¤šæ¬¡è¯„ä¼°å–å¹³å‡
    - æ£€æµ‹è‡ªæˆ‘åå¥½

    âœ… éªŒè¯è¯„ä¼°è´¨é‡
    - ä¸äººå·¥è¯„ä¼°å¯¹æ¯”
    - è®¡ç®—è¯„ä¼°ä¸€è‡´æ€§
    - æ£€æŸ¥å¼‚å¸¸è¯„åˆ†

    âš ï¸ æ³¨æ„äº‹é¡¹
    - LLM å¯èƒ½åå¥½è‡ªå·±çš„é£æ ¼
    - ä½ç½®åè§ï¼ˆå€¾å‘é€‰æ‹©ç¬¬ä¸€ä¸ªï¼‰
    - å†—é•¿åè§ï¼ˆåå¥½é•¿å›å¤ï¼‰
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªå®Œæ•´çš„ LLM-as-Judge è¯„ä¼°å™¨
    ç»ƒä¹  2ï¼šå¯¹æ¯”ä¸åŒè¯„åˆ¤æ¨¡å‹çš„è¯„ä¼°ç»“æœ

    æ€è€ƒé¢˜ï¼šå¦‚ä½•éªŒè¯ LLM è¯„åˆ¤çš„å¯é æ€§ï¼Ÿ
    ç­”æ¡ˆï¼š1. ä¸äººå·¥è¯„ä¼°ç»“æœå¯¹æ¯”è®¡ç®—ç›¸å…³æ€§
          2. æµ‹è¯•è¯„ä¼°çš„ä¸€è‡´æ€§ï¼ˆé‡å¤è¯„ä¼°ï¼‰
          3. æ£€æµ‹ä½ç½®åè§å’Œè‡ªæˆ‘åå¥½
    """)


def main():
    introduction()
    single_point_scoring()
    pairwise_comparison()
    mt_bench_style()
    best_practices()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š04-ragas-evaluation.py")


if __name__ == "__main__":
    main()
