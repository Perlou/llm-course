"""
ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„ä¼°
===============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„é‡è¦æ€§
    2. å®ç°ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡å’Œå¬å›ç‡è®¡ç®—
    3. ä¼˜åŒ–æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Context Precisionï¼šæ£€ç´¢ç²¾ç¡®ç‡
    - Context Recallï¼šæ£€ç´¢å¬å›ç‡
    - Semantic Similarityï¼šè¯­ä¹‰ç›¸ä¼¼åº¦

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install sentence-transformers numpy
"""

import numpy as np
from typing import List


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡ç›¸å…³æ€§æ¦‚å¿µ ====================


def introduction():
    """ä¸Šä¸‹æ–‡ç›¸å…³æ€§æ¦‚å¿µ"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡ç›¸å…³æ€§æ¦‚å¿µ")
    print("=" * 60)

    print("""
    ğŸ“Œ ä¸ºä»€ä¹ˆå…³æ³¨ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Ÿ
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RAG ç³»ç»Ÿçš„æ•ˆæœå¾ˆå¤§ç¨‹åº¦å–å†³äºæ£€ç´¢è´¨é‡                   â”‚
    â”‚  â€¢ æ£€ç´¢åˆ°ä¸ç›¸å…³å†…å®¹ â†’ å™ªå£°å¹²æ‰°ï¼Œé™ä½å›ç­”è´¨é‡            â”‚
    â”‚  â€¢ æ£€ç´¢ä¸åˆ°ç›¸å…³å†…å®¹ â†’ ä¿¡æ¯ç¼ºå¤±ï¼Œæ— æ³•æ­£ç¡®å›ç­”            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æ ¸å¿ƒæŒ‡æ ‡ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Context Precisionâ”‚ æ£€ç´¢ç»“æœä¸­ç›¸å…³å†…å®¹çš„æ¯”ä¾‹           â”‚
    â”‚                  â”‚ = ç›¸å…³æ–‡æ¡£æ•° / æ£€ç´¢æ–‡æ¡£æ€»æ•°         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Context Recall   â”‚ ç›¸å…³å†…å®¹è¢«æ£€ç´¢åˆ°çš„ç¨‹åº¦             â”‚
    â”‚                  â”‚ = æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ / æ‰€éœ€æ€»ä¿¡æ¯    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ è¯„ä¼°æµç¨‹ï¼š
    é—®é¢˜ â†’ æ£€ç´¢æ–‡æ¡£ â†’ è®¡ç®—ç›¸å…³æ€§ â†’ ç²¾ç¡®ç‡/å¬å›ç‡
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®— ====================


def semantic_similarity():
    """è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—")
    print("=" * 60)

    code = '''
from sentence_transformers import SentenceTransformer
import numpy as np

# åŠ è½½åµŒå…¥æ¨¡å‹
model = SentenceTransformer('all-MiniLM-L6-v2')

def compute_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
    embeddings = model.encode([text1, text2])
    similarity = np.dot(embeddings[0], embeddings[1]) / (
        np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
    )
    return float(similarity)

def compute_relevance_scores(question: str, contexts: list) -> list:
    """è®¡ç®—é—®é¢˜ä¸æ¯ä¸ªä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§åˆ†æ•°"""
    question_embedding = model.encode([question])[0]
    context_embeddings = model.encode(contexts)

    scores = []
    for ctx_emb in context_embeddings:
        similarity = np.dot(question_embedding, ctx_emb) / (
            np.linalg.norm(question_embedding) * np.linalg.norm(ctx_emb)
        )
        scores.append(float(similarity))

    return scores

# ä½¿ç”¨ç¤ºä¾‹
question = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
contexts = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆæ•£æ­¥ã€‚",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ã€‚"
]
scores = compute_relevance_scores(question, contexts)
# [0.72, 0.15, 0.68] - ç¬¬1å’Œç¬¬3ä¸ªæ–‡æ¡£ç›¸å…³
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡ç²¾ç¡®ç‡ ====================


def context_precision():
    """ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡ç²¾ç¡®ç‡ (Context Precision)")
    print("=" * 60)

    code = '''
def calculate_context_precision(
    question: str,
    contexts: list,
    threshold: float = 0.5
) -> dict:
    """
    è®¡ç®—ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡
    - æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­æœ‰å¤šå°‘æ˜¯çœŸæ­£ç›¸å…³çš„
    """
    scores = compute_relevance_scores(question, contexts)
    relevant_count = sum(1 for s in scores if s >= threshold)
    total_count = len(contexts)

    precision = relevant_count / total_count if total_count > 0 else 0

    return {
        "precision": precision,
        "relevant_count": relevant_count,
        "total_count": total_count,
        "scores": scores
    }

# ä½¿ç”¨ç¤ºä¾‹
result = calculate_context_precision(
    question="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
    contexts=[
        "RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯...",  # ç›¸å…³
        "ä»Šå¤©è‚¡å¸‚ä¸Šæ¶¨...",           # ä¸ç›¸å…³
        "æ£€ç´¢æ˜¯ RAG çš„æ ¸å¿ƒç»„ä»¶..."    # ç›¸å…³
    ]
)
# precision = 2/3 = 0.67
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡å¬å›ç‡ ====================


def context_recall():
    """ä¸Šä¸‹æ–‡å¬å›ç‡"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall)")
    print("=" * 60)

    code = '''
def calculate_context_recall(
    answer: str,
    contexts: list,
    ground_truth: str
) -> dict:
    """
    è®¡ç®—ä¸Šä¸‹æ–‡å¬å›ç‡
    - æ­£ç¡®ç­”æ¡ˆä¸­çš„ä¿¡æ¯æœ‰å¤šå°‘è¢«æ£€ç´¢ä¸Šä¸‹æ–‡è¦†ç›–

    éœ€è¦: æ ‡å‡†ç­”æ¡ˆ (ground_truth)
    """
    # å°†æ ‡å‡†ç­”æ¡ˆåˆ†è§£ä¸ºå…³é”®ä¿¡æ¯ç‚¹
    truth_sentences = ground_truth.split('ã€‚')
    truth_sentences = [s.strip() for s in truth_sentences if s.strip()]

    # è®¡ç®—æ¯ä¸ªä¿¡æ¯ç‚¹æ˜¯å¦è¢«ä¸Šä¸‹æ–‡è¦†ç›–
    covered = 0
    context_text = ' '.join(contexts)

    for sentence in truth_sentences:
        # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­æ˜¯å¦è¦†ç›–
        similarity = compute_similarity(sentence, context_text)
        if similarity >= 0.6:  # é˜ˆå€¼
            covered += 1

    recall = covered / len(truth_sentences) if truth_sentences else 0

    return {
        "recall": recall,
        "covered": covered,
        "total_points": len(truth_sentences)
    }
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šä¼˜åŒ–ç­–ç•¥ ====================


def optimization():
    """ä¼˜åŒ–ç­–ç•¥"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šä¼˜åŒ–ä¸Šä¸‹æ–‡ç›¸å…³æ€§")
    print("=" * 60)

    print("""
    ğŸ“Œ æé«˜ Precision çš„æ–¹æ³•ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. ä½¿ç”¨ Reranker é‡æ’åº                                â”‚
    â”‚ 2. å‡å°‘ top_kï¼Œåªä¿ç•™æœ€ç›¸å…³çš„                          â”‚
    â”‚ 3. è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤                                  â”‚
    â”‚ 4. ä½¿ç”¨æ›´å¥½çš„åµŒå…¥æ¨¡å‹                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æé«˜ Recall çš„æ–¹æ³•ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. å¢åŠ  top_k å¬å›æ•°é‡                                 â”‚
    â”‚ 2. ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå…³é”®è¯ + å‘é‡ï¼‰                       â”‚
    â”‚ 3. æŸ¥è¯¢æ‰©å±•/æ”¹å†™                                       â”‚
    â”‚ 4. ä¼˜åŒ–æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ Precision vs Recall æƒè¡¡ï¼š
    - é«˜ Precisionï¼Œä½ Recall â†’ ä¿¡æ¯ä¸å…¨
    - ä½ Precisionï¼Œé«˜ Recall â†’ å™ªå£°è¿‡å¤š
    - ç›®æ ‡ï¼šä¸¤è€…å¹³è¡¡ (F1 Score)
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„ä¼°å‡½æ•°

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from sentence_transformers import SentenceTransformer
        import numpy as np
        from typing import List, Dict
        
        class ContextRelevanceEvaluator:
            '''ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„ä¼°å™¨'''
            
            def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
                self.model = SentenceTransformer(model_name)
            
            def compute_similarity(
                self, 
                text1: str, 
                text2: str
            ) -> float:
                '''è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦'''
                emb = self.model.encode([text1, text2])
                return float(np.dot(emb[0], emb[1]) / (
                    np.linalg.norm(emb[0]) * np.linalg.norm(emb[1])
                ))
            
            def evaluate_precision(
                self,
                question: str,
                contexts: List[str],
                threshold: float = 0.5
            ) -> Dict:
                '''è¯„ä¼°ä¸Šä¸‹æ–‡ç²¾ç¡®ç‡'''
                q_emb = self.model.encode([question])[0]
                ctx_embs = self.model.encode(contexts)
                
                scores = []
                for ctx_emb in ctx_embs:
                    sim = np.dot(q_emb, ctx_emb) / (
                        np.linalg.norm(q_emb) * np.linalg.norm(ctx_emb)
                    )
                    scores.append(float(sim))
                
                relevant = sum(1 for s in scores if s >= threshold)
                
                return {
                    'precision': relevant / len(contexts),
                    'scores': scores,
                    'relevant_count': relevant,
                    'total': len(contexts)
                }
            
            def evaluate_recall(
                self,
                contexts: List[str],
                ground_truth: str,
                threshold: float = 0.6
            ) -> Dict:
                '''è¯„ä¼°ä¸Šä¸‹æ–‡å¬å›ç‡'''
                truth_sentences = [s.strip() for s in ground_truth.split('ã€‚') if s.strip()]
                context_text = ' '.join(contexts)
                
                covered = 0
                for sentence in truth_sentences:
                    sim = self.compute_similarity(sentence, context_text)
                    if sim >= threshold:
                        covered += 1
                
                return {
                    'recall': covered / len(truth_sentences) if truth_sentences else 0,
                    'covered': covered,
                    'total_points': len(truth_sentences)
                }
            
            def evaluate_f1(
                self,
                question: str,
                contexts: List[str],
                ground_truth: str
            ) -> Dict:
                '''è®¡ç®— F1 åˆ†æ•°'''
                p = self.evaluate_precision(question, contexts)['precision']
                r = self.evaluate_recall(contexts, ground_truth)['recall']
                
                f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0
                
                return {'precision': p, 'recall': r, 'f1': f1}
        
        # ä½¿ç”¨ç¤ºä¾‹
        evaluator = ContextRelevanceEvaluator()
        result = evaluator.evaluate_f1(
            question="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
            contexts=["RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ...", "ä»Šå¤©å¤©æ°”å¾ˆå¥½"],
            ground_truth="RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆ"
        )
        print(f"F1: {result['f1']:.2f}")
        ```
    
    ç»ƒä¹  2ï¼šå¯¹æ¯”ä¸åŒ top_k è®¾ç½®å¯¹ Precision/Recall çš„å½±å“

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        class TopKExperiment:
            '''top_k å®éªŒ'''
            
            def __init__(self, retriever, evaluator):
                self.retriever = retriever
                self.evaluator = evaluator
            
            def run_experiment(
                self,
                questions: List[str],
                ground_truths: List[str],
                k_values: List[int] = [1, 3, 5, 10, 20]
            ) -> Dict:
                '''è¿è¡Œ top_k å¯¹æ¯”å®éªŒ'''
                results = {}
                
                for k in k_values:
                    precisions, recalls = [], []
                    
                    for q, gt in zip(questions, ground_truths):
                        contexts = self.retriever.retrieve(q, top_k=k)
                        metrics = self.evaluator.evaluate_f1(q, contexts, gt)
                        precisions.append(metrics['precision'])
                        recalls.append(metrics['recall'])
                    
                    results[k] = {
                        'precision': sum(precisions) / len(precisions),
                        'recall': sum(recalls) / len(recalls),
                        'f1': 2 * results[k]['precision'] * results[k]['recall'] / (
                            results[k]['precision'] + results[k]['recall'] + 1e-6
                        )
                    }
                
                # æ‰¾åˆ°æœ€ä¼˜ k
                best_k = max(results.keys(), key=lambda k: results[k]['f1'])
                
                return {
                    'results': results,
                    'best_k': best_k,
                    'best_f1': results[best_k]['f1']
                }
        
        # é¢„æœŸç»“æœè¶‹åŠ¿ï¼š
        # k â†‘ â†’ Recall â†‘, Precision â†“
        # éœ€è¦æ‰¾åˆ° F1 æœ€ä¼˜çš„ k å€¼
        ```

    æ€è€ƒé¢˜ï¼šå¦‚ä½•åœ¨ Precision å’Œ Recall ä¹‹é—´å–å¾—å¹³è¡¡ï¼Ÿ

        âœ… ç­”ï¼š
        1. ä¸¤æ®µå¼å¬å› - å…ˆå¤§é‡å¬å›ï¼ˆé«˜ Recallï¼‰ï¼Œå† Rerank ç²¾é€‰ï¼ˆé«˜ Precisionï¼‰
        2. åŠ¨æ€ k - æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ top_k
        3. é˜ˆå€¼è¿‡æ»¤ - è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¿‡æ»¤ä½è´¨é‡ç»“æœ
        4. æ··åˆæ£€ç´¢ - ç»“åˆå…³é”®è¯å’Œå‘é‡æ£€ç´¢çš„ä¼˜åŠ¿
        5. F1 ä¼˜åŒ– - é€‰æ‹© F1 æœ€é«˜çš„é…ç½®ä½œä¸ºå¹³è¡¡ç‚¹
    """)


def main():
    introduction()
    semantic_similarity()
    context_precision()
    context_recall()
    optimization()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š06-faithfulness.py")


if __name__ == "__main__":
    main()
