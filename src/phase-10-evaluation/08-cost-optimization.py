"""
æˆæœ¬ä¼˜åŒ–
========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ LLM åº”ç”¨çš„æˆæœ¬æ„æˆ
    2. æŒæ¡ Token ä¼˜åŒ–æŠ€å·§
    3. å®ç°æˆæœ¬ç›‘æ§å’Œæ§åˆ¶

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Token æˆæœ¬ï¼šè¾“å…¥/è¾“å‡º token å®šä»·
    - Cachingï¼šç¼“å­˜å¤ç”¨
    - æ¨¡å‹é€‰æ‹©ï¼šåœ¨è´¨é‡å’Œæˆæœ¬é—´å¹³è¡¡

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install tiktoken openai
"""

import os
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæˆæœ¬æ„æˆåˆ†æ ====================


def introduction():
    """æˆæœ¬æ„æˆåˆ†æ"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šLLM æˆæœ¬æ„æˆ")
    print("=" * 60)

    print("""
    ğŸ“Œ LLM åº”ç”¨æˆæœ¬æ„æˆï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  1. API è°ƒç”¨æˆæœ¬ (æœ€å¤§)                                 â”‚
    â”‚     - è¾“å…¥ Token è´¹ç”¨                                   â”‚
    â”‚     - è¾“å‡º Token è´¹ç”¨ï¼ˆé€šå¸¸æ›´è´µï¼‰                       â”‚
    â”‚                                                         â”‚
    â”‚  2. åŸºç¡€è®¾æ–½æˆæœ¬                                        â”‚
    â”‚     - æœåŠ¡å™¨/GPU è´¹ç”¨                                   â”‚
    â”‚     - å‘é‡æ•°æ®åº“                                        â”‚
    â”‚                                                         â”‚
    â”‚  3. å­˜å‚¨æˆæœ¬                                            â”‚
    â”‚     - å‘é‡ç´¢å¼•å­˜å‚¨                                      â”‚
    â”‚     - ç¼“å­˜å­˜å‚¨                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ ä¸»æµæ¨¡å‹å®šä»·å¯¹æ¯” ($/1M tokens)ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     æ¨¡å‹       â”‚  è¾“å…¥    â”‚   è¾“å‡º   â”‚   æ€§èƒ½/æˆæœ¬  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ GPT-4o         â”‚  $2.5    â”‚  $10     â”‚   ä¸­         â”‚
    â”‚ GPT-4o-mini    â”‚  $0.15   â”‚  $0.6    â”‚   é«˜         â”‚
    â”‚ Claude 3.5     â”‚  $3      â”‚  $15     â”‚   ä¸­         â”‚
    â”‚ Claude 3 Haiku â”‚  $0.25   â”‚  $1.25   â”‚   é«˜         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šToken ä¼˜åŒ– ====================


def token_optimization():
    """Token ä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šToken ä¼˜åŒ–")
    print("=" * 60)

    code = '''
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def estimate_cost(
    input_text: str,
    output_tokens: int,
    model: str = "gpt-4o"
) -> float:
    """ä¼°ç®— API è°ƒç”¨æˆæœ¬"""
    prices = {
        "gpt-4o": {"input": 2.5/1e6, "output": 10/1e6},
        "gpt-4o-mini": {"input": 0.15/1e6, "output": 0.6/1e6},
    }

    input_tokens = count_tokens(input_text, model)
    price = prices[model]

    cost = input_tokens * price["input"] + output_tokens * price["output"]
    return cost

# ä¼˜åŒ–ç¤ºä¾‹ï¼šç²¾ç®€æç¤ºè¯
verbose_prompt = """
æˆ‘æƒ³è¯·ä½ å¸®æˆ‘å®Œæˆä¸€ä¸ªä»»åŠ¡ã€‚è¿™ä¸ªä»»åŠ¡éå¸¸é‡è¦ã€‚
è¯·ä½ ä»”ç»†é˜…è¯»ä»¥ä¸‹å†…å®¹ï¼Œç„¶åç»™å‡ºä½ çš„å›ç­”ã€‚
åœ¨å›ç­”æ—¶ï¼Œè¯·ç¡®ä¿......ï¼ˆå†—é•¿çš„è¯´æ˜ï¼‰
"""

concise_prompt = """
ä»»åŠ¡ï¼š{task}
è¦æ±‚ï¼šç®€æ´å›ç­”ï¼Œ2å¥è¯ä»¥å†…
"""

print(f"å†—é•¿ç‰ˆ tokens: {count_tokens(verbose_prompt)}")
print(f"ç²¾ç®€ç‰ˆ tokens: {count_tokens(concise_prompt)}")
'''
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¼“å­˜ç­–ç•¥ ====================


def caching_strategy():
    """ç¼“å­˜ç­–ç•¥"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¼“å­˜ç­–ç•¥")
    print("=" * 60)

    code = '''
import hashlib
import json
from functools import lru_cache

class LLMCache:
    """LLM å“åº”ç¼“å­˜"""

    def __init__(self, redis_client=None):
        self.redis = redis_client
        self.local_cache = {}

    def _get_cache_key(self, prompt: str, model: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{model}:{prompt}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, prompt: str, model: str) -> str | None:
        """è·å–ç¼“å­˜"""
        key = self._get_cache_key(prompt, model)

        # å…ˆæŸ¥æœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            return self.local_cache[key]

        # å†æŸ¥ Redis
        if self.redis:
            cached = self.redis.get(key)
            if cached:
                return cached.decode()

        return None

    def set(self, prompt: str, model: str, response: str, ttl: int = 3600):
        """è®¾ç½®ç¼“å­˜"""
        key = self._get_cache_key(prompt, model)
        self.local_cache[key] = response

        if self.redis:
            self.redis.setex(key, ttl, response)

# ä½¿ç”¨ç¤ºä¾‹
cache = LLMCache()

def generate_with_cache(prompt: str, model: str = "gpt-4o"):
    # æ£€æŸ¥ç¼“å­˜
    cached = cache.get(prompt, model)
    if cached:
        print("å‘½ä¸­ç¼“å­˜ï¼ŒèŠ‚çœ API è°ƒç”¨")
        return cached

    # è°ƒç”¨ API
    response = client.chat.completions.create(...)

    # å­˜å…¥ç¼“å­˜
    cache.set(prompt, model, response.choices[0].message.content)
    return response.choices[0].message.content
'''
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹è·¯ç”± ====================


def model_routing():
    """æ¨¡å‹è·¯ç”±ç­–ç•¥"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹è·¯ç”±ç­–ç•¥")
    print("=" * 60)

    code = '''
class SmartRouter:
    """æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨"""

    def __init__(self):
        self.models = {
            "simple": "gpt-4o-mini",    # ç®€å•ä»»åŠ¡
            "complex": "gpt-4o",         # å¤æ‚ä»»åŠ¡
            "creative": "gpt-4o",        # åˆ›æ„ä»»åŠ¡
        }

    def classify_task(self, prompt: str) -> str:
        """åˆ†ç±»ä»»åŠ¡å¤æ‚åº¦"""
        # ç®€å•è§„åˆ™åˆ¤æ–­
        if len(prompt) < 100 and "ç®€å•" in prompt:
            return "simple"
        elif any(w in prompt for w in ["ä»£ç ", "åˆ†æ", "æ¨ç†", "å¤æ‚"]):
            return "complex"
        else:
            return "simple"

    def route(self, prompt: str) -> str:
        """è·¯ç”±åˆ°åˆé€‚çš„æ¨¡å‹"""
        task_type = self.classify_task(prompt)
        return self.models[task_type]

# ä½¿ç”¨
router = SmartRouter()

def smart_generate(prompt: str):
    model = router.route(prompt)
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    return response
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæˆæœ¬ç›‘æ§ ====================


def cost_monitoring():
    """æˆæœ¬ç›‘æ§"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šæˆæœ¬ç›‘æ§")
    print("=" * 60)

    code = '''
class CostTracker:
    """æˆæœ¬è¿½è¸ªå™¨"""

    def __init__(self):
        self.usage_log = []
        self.total_cost = 0

    def log_usage(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        user_id: str = None
    ):
        """è®°å½•ä½¿ç”¨é‡"""
        cost = self._calculate_cost(model, input_tokens, output_tokens)

        self.usage_log.append({
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "user_id": user_id
        })

        self.total_cost += cost
        return cost

    def get_daily_report(self) -> dict:
        """è·å–æ—¥æŠ¥"""
        today = datetime.now().date()
        today_logs = [
            log for log in self.usage_log
            if log["timestamp"].startswith(str(today))
        ]

        return {
            "date": str(today),
            "total_requests": len(today_logs),
            "total_tokens": sum(l["input_tokens"] + l["output_tokens"] for l in today_logs),
            "total_cost": sum(l["cost"] for l in today_logs),
            "by_model": self._group_by_model(today_logs)
        }

    def check_budget(self, budget: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…é¢„ç®—"""
        if self.total_cost >= budget:
            print(f"âš ï¸ å·²è¾¾åˆ°é¢„ç®—ä¸Šé™: ${budget}")
            return False
        return True
'''
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°ä¸€ä¸ªå¸¦ç¼“å­˜çš„ LLM è°ƒç”¨å‡½æ•°

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import hashlib
        import json
        from functools import lru_cache
        from typing import Optional
        import redis
        
        class CachedLLMClient:
            '''å¸¦ç¼“å­˜çš„ LLM å®¢æˆ·ç«¯'''
            
            def __init__(
                self, 
                model,
                redis_url: str = None,
                local_cache_size: int = 1000,
                ttl: int = 3600
            ):
                self.model = model
                self.ttl = ttl
                self.local_cache = {}
                self.redis = redis.from_url(redis_url) if redis_url else None
                self.stats = {'hits': 0, 'misses': 0}
            
            def _cache_key(self, prompt: str, **kwargs) -> str:
                '''ç”Ÿæˆç¼“å­˜é”®'''
                content = json.dumps({'prompt': prompt, **kwargs}, sort_keys=True)
                return hashlib.md5(content.encode()).hexdigest()
            
            def _get_cache(self, key: str) -> Optional[str]:
                '''è·å–ç¼“å­˜'''
                # å…ˆæŸ¥æœ¬åœ°
                if key in self.local_cache:
                    self.stats['hits'] += 1
                    return self.local_cache[key]
                
                # å†æŸ¥ Redis
                if self.redis:
                    cached = self.redis.get(key)
                    if cached:
                        self.stats['hits'] += 1
                        return cached.decode()
                
                self.stats['misses'] += 1
                return None
            
            def _set_cache(self, key: str, value: str):
                '''è®¾ç½®ç¼“å­˜'''
                self.local_cache[key] = value
                if self.redis:
                    self.redis.setex(key, self.ttl, value)
            
            def generate(
                self, 
                prompt: str, 
                use_cache: bool = True,
                **kwargs
            ) -> str:
                '''ç”Ÿæˆï¼ˆå¸¦ç¼“å­˜ï¼‰'''
                if use_cache:
                    key = self._cache_key(prompt, **kwargs)
                    cached = self._get_cache(key)
                    if cached:
                        return cached
                
                # è°ƒç”¨æ¨¡å‹
                response = self.model.generate(prompt, **kwargs)
                
                if use_cache:
                    self._set_cache(key, response)
                
                return response
            
            def get_stats(self) -> dict:
                '''è·å–ç¼“å­˜ç»Ÿè®¡'''
                total = self.stats['hits'] + self.stats['misses']
                return {
                    **self.stats,
                    'hit_rate': self.stats['hits'] / total if total > 0 else 0,
                    'estimated_savings': self.stats['hits'] * 0.01  # å‡è®¾æ¯æ¬¡èŠ‚çœ $0.01
                }
        ```
    
    ç»ƒä¹  2ï¼šè®¾è®¡ä¸€ä¸ªåŸºäºä»»åŠ¡å¤æ‚åº¦çš„æ¨¡å‹è·¯ç”±ç­–ç•¥

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from enum import Enum
        from typing import Dict, Callable
        
        class TaskComplexity(Enum):
            SIMPLE = 'simple'      # ç®€å•é—®ç­”
            MEDIUM = 'medium'      # ä¸€èˆ¬ä»»åŠ¡
            COMPLEX = 'complex'    # å¤æ‚æ¨ç†
            CREATIVE = 'creative'  # åˆ›æ„ä»»åŠ¡
        
        class SmartModelRouter:
            '''æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨'''
            
            def __init__(self):
                # æ¨¡å‹é…ç½®ï¼š(æ¨¡å‹å, æ¯ 1K tokens æˆæœ¬)
                self.models = {
                    TaskComplexity.SIMPLE: ('gpt-4o-mini', 0.00015),
                    TaskComplexity.MEDIUM: ('gpt-4o-mini', 0.00015),
                    TaskComplexity.COMPLEX: ('gpt-4o', 0.0025),
                    TaskComplexity.CREATIVE: ('gpt-4o', 0.0025),
                }
                
                # å¤æ‚åº¦å…³é”®è¯
                self.complexity_keywords = {
                    TaskComplexity.COMPLEX: [
                        'åˆ†æ', 'æ¨ç†', 'æ¯”è¾ƒ', 'ä»£ç ', 'æ•°å­¦',
                        'è§£é‡Šä¸ºä»€ä¹ˆ', 'è¯¦ç»†è¯´æ˜'
                    ],
                    TaskComplexity.CREATIVE: [
                        'å†™ä½œ', 'åˆ›ä½œ', 'è®¾è®¡', 'æ•…äº‹', 'è¯—æ­Œ'
                    ]
                }
            
            def classify(self, prompt: str) -> TaskComplexity:
                '''åˆ†ç±»ä»»åŠ¡å¤æ‚åº¦'''
                prompt_lower = prompt.lower()
                
                # æ£€æŸ¥å…³é”®è¯
                for complexity, keywords in self.complexity_keywords.items():
                    if any(kw in prompt_lower for kw in keywords):
                        return complexity
                
                # æ ¹æ®é•¿åº¦åˆ¤æ–­
                if len(prompt) > 500:
                    return TaskComplexity.MEDIUM
                
                return TaskComplexity.SIMPLE
            
            def route(self, prompt: str) -> tuple:
                '''è·¯ç”±åˆ°åˆé€‚çš„æ¨¡å‹'''
                complexity = self.classify(prompt)
                model, cost = self.models[complexity]
                
                return {
                    'model': model,
                    'complexity': complexity.value,
                    'estimated_cost_per_1k': cost
                }
            
            def generate_with_fallback(
                self, 
                prompt: str,
                quality_threshold: float = 0.7
            ) -> str:
                '''å¸¦é™çº§çš„ç”Ÿæˆ'''
                route_info = self.route(prompt)
                
                # å…ˆç”¨ä¾¿å®œæ¨¡å‹
                response = self._generate(route_info['model'], prompt)
                quality = self._check_quality(response)
                
                # è´¨é‡ä¸è¶³åˆ™å‡çº§æ¨¡å‹
                if quality < quality_threshold:
                    response = self._generate('gpt-4o', prompt)
                
                return response
        ```

    æ€è€ƒé¢˜ï¼šå¦‚ä½•åœ¨è´¨é‡å’Œæˆæœ¬ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Ÿ

        âœ… ç­”ï¼š
        1. ä»»åŠ¡åˆ†å±‚ - é‡è¦ä»»åŠ¡ç”¨å¼ºæ¨¡å‹ï¼Œç®€å•ä»»åŠ¡ç”¨ä¾¿å®œæ¨¡å‹
        2. ç¼“å­˜å¤ç”¨ - ç›¸åŒ/ç›¸ä¼¼è¯·æ±‚å¤ç”¨ç»“æœï¼Œå‡å°‘è°ƒç”¨
        3. Prompt ç²¾ç®€ - å‡å°‘å†—ä½™è¡¨è¾¾ï¼Œé™ä½ token æ¶ˆè€—
        4. æ‰¹é‡å¤„ç† - åˆå¹¶è¯·æ±‚ï¼Œåˆ©ç”¨æ‰¹é‡æŠ˜æ‰£
        5. é¢„ç®—å‘Šè­¦ - è®¾ç½®æ—¥/æœˆé¢„ç®—ä¸Šé™ï¼Œè¶…é¢„ç®—é™çº§æœåŠ¡
        6. è´¨é‡ç›‘æ§ - ç›‘æ§ä½æˆæœ¬æ¨¡å‹çš„è´¨é‡ï¼Œç¡®ä¿å¯æ¥å—
        7. A/B æµ‹è¯• - æµ‹è¯•ä¸åŒæ¨¡å‹é…ç½®çš„æ€§ä»·æ¯”
    """)


def main():
    introduction()
    token_optimization()
    caching_strategy()
    model_routing()
    cost_monitoring()
    exercises()
    print("\n" + "=" * 60)
    print("ğŸ‰ Phase 10 è¯¾ç¨‹å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
