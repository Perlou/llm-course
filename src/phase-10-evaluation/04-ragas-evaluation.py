"""
Ragas è¯„ä¼°æ¡†æ¶
=============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ Ragas çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡
    2. ä½¿ç”¨ Ragas è¯„ä¼° RAG ç³»ç»Ÿ
    3. è§£è¯»è¯„ä¼°ç»“æœå¹¶ä¼˜åŒ–

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Faithfulnessï¼šå¿ å®åº¦
    - Answer Relevancyï¼šå›ç­”ç›¸å…³æ€§
    - Context Precision/Recallï¼šä¸Šä¸‹æ–‡ç²¾ç¡®ç‡/å¬å›ç‡

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install ragas datasets
"""

import os
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šRagas ç®€ä»‹ ====================


def introduction():
    """Ragas ç®€ä»‹"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šRagas ç®€ä»‹")
    print("=" * 60)

    print("""
    ğŸ“Œ Ragas æ˜¯ä»€ä¹ˆï¼Ÿ
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ragas (RAG Assessment) æ˜¯ä¸“é—¨ç”¨äºè¯„ä¼° RAG ç³»ç»Ÿçš„æ¡†æ¶   â”‚
    â”‚  æä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å‚è€ƒç­”æ¡ˆ       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Faithfulness     â”‚ å›ç­”æ˜¯å¦å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡       â”‚
    â”‚ Answer Relevancy â”‚ å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦               â”‚
    â”‚ Context Precisionâ”‚ æ£€ç´¢ç»“æœä¸­ç›¸å…³å†…å®¹çš„æ¯”ä¾‹           â”‚
    â”‚ Context Recall   â”‚ æ­£ç¡®ç­”æ¡ˆè¢«æ£€ç´¢ä¸Šä¸‹æ–‡è¦†ç›–çš„ç¨‹åº¦     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ è¯„ä¼°æµç¨‹ï¼š
    é—®é¢˜ â†’ RAG ç³»ç»Ÿ â†’ å›ç­” + ä¸Šä¸‹æ–‡ â†’ Ragas è¯„ä¼° â†’ æŒ‡æ ‡åˆ†æ•°
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæŒ‡æ ‡è¯¦è§£ ====================


def core_metrics():
    """æ ¸å¿ƒæŒ‡æ ‡è¯¦è§£"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæŒ‡æ ‡è¯¦è§£")
    print("=" * 60)

    print("""
    ğŸ“Œ Faithfulnessï¼ˆå¿ å®åº¦ï¼‰
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å®šä¹‰ï¼šå›ç­”ä¸­çš„å£°æ˜æ˜¯å¦èƒ½ä»ä¸Šä¸‹æ–‡ä¸­æ¨æ–­å‡ºæ¥             â”‚
    â”‚  è®¡ç®—ï¼š(å¯ä»ä¸Šä¸‹æ–‡æ¨æ–­çš„å£°æ˜æ•°) / (å›ç­”ä¸­çš„æ€»å£°æ˜æ•°)    â”‚
    â”‚  èŒƒå›´ï¼š0-1ï¼Œè¶Šé«˜è¶Šå¥½                                   â”‚
    â”‚  æ„ä¹‰ï¼šæ£€æµ‹å¹»è§‰ï¼Œç¡®ä¿å›ç­”åŸºäºäº‹å®                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ Answer Relevancyï¼ˆå›ç­”ç›¸å…³æ€§ï¼‰
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å®šä¹‰ï¼šå›ç­”ä¸é—®é¢˜çš„è¯­ä¹‰ç›¸å…³ç¨‹åº¦                         â”‚
    â”‚  è®¡ç®—ï¼šç”Ÿæˆé—®é¢˜ä¸åŸé—®é¢˜çš„ç›¸ä¼¼åº¦                         â”‚
    â”‚  èŒƒå›´ï¼š0-1ï¼Œè¶Šé«˜è¶Šå¥½                                   â”‚
    â”‚  æ„ä¹‰ï¼šç¡®ä¿å›ç­”åˆ‡é¢˜ï¼Œæ²¡æœ‰ç­”éæ‰€é—®                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ Context Precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®ç‡ï¼‰
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å®šä¹‰ï¼šæ£€ç´¢ç»“æœä¸­ä¸å›ç­”ç›¸å…³çš„å†…å®¹æ¯”ä¾‹                   â”‚
    â”‚  æ„ä¹‰ï¼šè¯„ä¼°æ£€ç´¢è´¨é‡ï¼Œé¿å…å™ªå£°å¹²æ‰°                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ Context Recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å®šä¹‰ï¼šæ­£ç¡®ç­”æ¡ˆä¸­çš„ä¿¡æ¯è¢«æ£€ç´¢ä¸Šä¸‹æ–‡è¦†ç›–çš„ç¨‹åº¦           â”‚
    â”‚  æ„ä¹‰ï¼šç¡®ä¿æ£€ç´¢åˆ°è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ Ragas ====================


def using_ragas():
    """ä½¿ç”¨ Ragas"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šä½¿ç”¨ Ragas è¯„ä¼°")
    print("=" * 60)

    code = """
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®
eval_data = {
    "question": [
        "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
        "LangChain æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"
    ],
    "answer": [
        "RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆæ¥å›ç­”é—®é¢˜ã€‚",
        "LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚"
    ],
    "contexts": [
        ["RAGï¼ˆRetrieval Augmented Generationï¼‰æ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆçš„æŠ€æœ¯ã€‚"],
        ["LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚"]
    ],
    "ground_truth": [  # å¯é€‰ï¼Œç”¨äº context_recall
        "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢ç³»ç»Ÿå’Œå¤§è¯­è¨€æ¨¡å‹ã€‚",
        "LangChain æ˜¯æ„å»º LLM åº”ç”¨çš„å¼€æºæ¡†æ¶ã€‚"
    ]
}

dataset = Dataset.from_dict(eval_data)

# è¿è¡Œè¯„ä¼°
result = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
)

print(result)
# è¾“å‡ºç¤ºä¾‹ï¼š
# {'faithfulness': 0.95, 'answer_relevancy': 0.88, ...}
"""
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šè¯„ä¼° RAG ç³»ç»Ÿ ====================


def evaluate_rag_system():
    """è¯„ä¼° RAG ç³»ç»Ÿ"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šè¯„ä¼°å®Œæ•´ RAG ç³»ç»Ÿ")
    print("=" * 60)

    code = '''
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from datasets import Dataset

class RAGEvaluator:
    """RAG ç³»ç»Ÿè¯„ä¼°å™¨"""

    def __init__(self, rag_system):
        self.rag_system = rag_system

    def prepare_dataset(self, test_questions: list) -> Dataset:
        """å‡†å¤‡è¯„ä¼°æ•°æ®é›†"""
        data = {
            "question": [],
            "answer": [],
            "contexts": []
        }

        for question in test_questions:
            # è°ƒç”¨ RAG ç³»ç»Ÿ
            result = self.rag_system.query(question)

            data["question"].append(question)
            data["answer"].append(result["answer"])
            data["contexts"].append(result["retrieved_docs"])

        return Dataset.from_dict(data)

    def evaluate(self, test_questions: list) -> dict:
        """è¯„ä¼° RAG ç³»ç»Ÿ"""
        dataset = self.prepare_dataset(test_questions)

        result = evaluate(
            dataset,
            metrics=[faithfulness, answer_relevancy]
        )

        return {
            "faithfulness": result["faithfulness"],
            "answer_relevancy": result["answer_relevancy"],
            "overall": (result["faithfulness"] + result["answer_relevancy"]) / 2
        }

# ä½¿ç”¨ç¤ºä¾‹
# evaluator = RAGEvaluator(my_rag_system)
# scores = evaluator.evaluate(["é—®é¢˜1", "é—®é¢˜2", ...])
'''
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»“æœè§£è¯»ä¸ä¼˜åŒ– ====================


def interpretation():
    """ç»“æœè§£è¯»ä¸ä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šç»“æœè§£è¯»ä¸ä¼˜åŒ–")
    print("=" * 60)

    print("""
    ğŸ“Œ å¦‚ä½•è§£è¯»è¯„ä¼°ç»“æœï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æŒ‡æ ‡ä½çš„å¯èƒ½åŸå›        â”‚ ä¼˜åŒ–æ–¹å‘                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Faithfulness ä½       â”‚ æ£€æŸ¥å¹»è§‰ï¼Œå¢å¼ºä¸Šä¸‹æ–‡çº¦æŸ       â”‚
    â”‚                       â”‚ ä½¿ç”¨æ›´å¼ºçš„æç¤ºè¯çº¦æŸ           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Answer Relevancy ä½   â”‚ ä¼˜åŒ–ç”Ÿæˆ prompt                â”‚
    â”‚                       â”‚ æ£€æŸ¥é—®é¢˜ç†è§£æ˜¯å¦æ­£ç¡®           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Context Precision ä½  â”‚ ä¼˜åŒ–æ£€ç´¢æ¨¡å‹/é‡æ’åº            â”‚
    â”‚                       â”‚ è°ƒæ•´ top_k å‚æ•°                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Context Recall ä½     â”‚ æ‰©å¤§æ£€ç´¢èŒƒå›´                   â”‚
    â”‚                       â”‚ ä¼˜åŒ–æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ ä¼˜åŒ–å»ºè®®ï¼š
    1. Faithfulness ä½ â†’ åŠ å¼º prompt çº¦æŸï¼Œä½¿ç”¨å¼•ç”¨æ ¼å¼
    2. Relevancy ä½ â†’ ä¼˜åŒ–é—®é¢˜æ”¹å†™ï¼Œä½¿ç”¨ HyDE
    3. Precision ä½ â†’ æ·»åŠ  Reranker
    4. Recall ä½ â†’ ä½¿ç”¨æ··åˆæ£€ç´¢ï¼Œå¢åŠ å¬å›æ•°é‡
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šä½¿ç”¨ Ragas è¯„ä¼°ä½ çš„ RAG ç³»ç»Ÿ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        )
        from datasets import Dataset
        
        class RAGEvaluator:
            '''RAG ç³»ç»Ÿ Ragas è¯„ä¼°å™¨'''
            
            def __init__(self, rag_system):
                self.rag_system = rag_system
                self.metrics = [
                    faithfulness,
                    answer_relevancy,
                    context_precision,
                    context_recall
                ]
            
            def collect_data(
                self, 
                questions: list,
                ground_truths: list = None
            ) -> Dataset:
                '''æ”¶é›†è¯„ä¼°æ•°æ®'''
                data = {
                    "question": [],
                    "answer": [],
                    "contexts": []
                }
                
                if ground_truths:
                    data["ground_truth"] = []
                
                for i, q in enumerate(questions):
                    result = self.rag_system.query(q)
                    data["question"].append(q)
                    data["answer"].append(result["answer"])
                    data["contexts"].append(result["contexts"])
                    
                    if ground_truths:
                        data["ground_truth"].append(ground_truths[i])
                
                return Dataset.from_dict(data)
            
            def evaluate(
                self, 
                questions: list,
                ground_truths: list = None
            ) -> dict:
                '''è¿è¡Œè¯„ä¼°'''
                dataset = self.collect_data(questions, ground_truths)
                
                result = evaluate(
                    dataset,
                    metrics=self.metrics
                )
                
                return {
                    "faithfulness": result["faithfulness"],
                    "answer_relevancy": result["answer_relevancy"],
                    "context_precision": result["context_precision"],
                    "context_recall": result["context_recall"],
                    "overall": (
                        result["faithfulness"] + 
                        result["answer_relevancy"]
                    ) / 2
                }
        
        # ä½¿ç”¨ç¤ºä¾‹
        # evaluator = RAGEvaluator(my_rag)
        # questions = ["ä»€ä¹ˆæ˜¯ RAGï¼Ÿ", "LangChain æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"]
        # scores = evaluator.evaluate(questions)
        # print(f"Faithfulness: {scores['faithfulness']:.2f}")
        ```
    
    ç»ƒä¹  2ï¼šæ ¹æ®è¯„ä¼°ç»“æœä¼˜åŒ– RAG ç³»ç»Ÿå¹¶é‡æ–°è¯„ä¼°

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        class RAGOptimizer:
            '''åŸºäºè¯„ä¼°ç»“æœä¼˜åŒ– RAG'''
            
            def __init__(self, rag_system, evaluator):
                self.rag = rag_system
                self.evaluator = evaluator
            
            def optimize_based_on_scores(
                self, 
                scores: dict,
                test_questions: list
            ) -> dict:
                '''æ ¹æ®è¯„åˆ†ä¼˜åŒ–'''
                optimizations = []
                
                # 1. Faithfulness ä½ â†’ å¢å¼ºæç¤ºè¯çº¦æŸ
                if scores["faithfulness"] < 0.7:
                    self.rag.update_prompt(
                        "ä»…åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦æ·»åŠ é¢å¤–ä¿¡æ¯ã€‚"
                        "å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜ã€‚"
                    )
                    optimizations.append("å¢å¼ºå¿ å®åº¦çº¦æŸ")
                
                # 2. Context Precision ä½ â†’ æ·»åŠ  Reranker
                if scores["context_precision"] < 0.7:
                    self.rag.add_reranker()
                    optimizations.append("æ·»åŠ  Reranker")
                
                # 3. Context Recall ä½ â†’ å¢åŠ å¬å›æ•°é‡
                if scores["context_recall"] < 0.7:
                    self.rag.update_config(top_k=10)
                    self.rag.enable_hybrid_search()
                    optimizations.append("æ‰©å¤§å¬å›èŒƒå›´")
                
                # 4. Answer Relevancy ä½ â†’ ä¼˜åŒ–é—®é¢˜æ”¹å†™
                if scores["answer_relevancy"] < 0.7:
                    self.rag.enable_query_rewrite()
                    optimizations.append("å¯ç”¨æŸ¥è¯¢æ”¹å†™")
                
                # é‡æ–°è¯„ä¼°
                new_scores = self.evaluator.evaluate(test_questions)
                
                return {
                    "before": scores,
                    "after": new_scores,
                    "optimizations": optimizations,
                    "improvement": {
                        k: new_scores[k] - scores[k]
                        for k in scores if k in new_scores
                    }
                }
        ```

    æ€è€ƒé¢˜ï¼šä¸ºä»€ä¹ˆ Ragas ä¸éœ€è¦äººå·¥æ ‡æ³¨çš„å‚è€ƒç­”æ¡ˆï¼Ÿ

        âœ… ç­”ï¼š
        1. LLM ä½œä¸ºè¯„åˆ¤ - ä½¿ç”¨ LLM åˆ†æå›ç­”ä¸ä¸Šä¸‹æ–‡çš„å…³ç³»
        2. å£°æ˜çº§æ£€éªŒ - å°†å›ç­”åˆ†è§£ä¸ºå£°æ˜ï¼Œé€æ¡éªŒè¯æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡æ”¯æŒ
        3. è‡ªæ´½æ€§éªŒè¯ - é€šè¿‡ç”Ÿæˆé—®é¢˜å†æ¯”è¾ƒçš„æ–¹å¼è¯„ä¼°ç›¸å…³æ€§
        4. è‡ªåŠ¨åŒ–æµç¨‹ - æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹æ— éœ€äººå·¥å¹²é¢„
        5. ä½†æ³¨æ„ï¼šContext Recall ä»éœ€è¦ ground_truth æ¥è®¡ç®—å¬å›ç‡
    """)


def main():
    introduction()
    core_metrics()
    using_ragas()
    evaluate_rag_system()
    interpretation()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š05-contextual-relevance.py")


if __name__ == "__main__":
    main()
