"""
ä¸Šä¸‹æ–‡å‹ç¼©
==========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ä¸Šä¸‹æ–‡å‹ç¼©çš„å¿…è¦æ€§
    2. æŒæ¡æå–å¼å’Œç”Ÿæˆå¼å‹ç¼©
    3. å­¦ä¼š LangChain å‹ç¼©ç»„ä»¶

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Context Compressionï¼šå‡å°‘æ— å…³å†…å®¹
    - æå–å¼ï¼šé€‰æ‹©ç›¸å…³å¥å­
    - ç”Ÿæˆå¼ï¼šLLM é‡å†™å‹ç¼©

å‰ç½®çŸ¥è¯†ï¼š
    - 07-hypothetical-questions.py

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install langchain langchain-google-genai chromadb python-dotenv
"""

import os
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡å‹ç¼©æ¦‚å¿µ ====================


def compression_concept():
    """ä¸Šä¸‹æ–‡å‹ç¼©æ¦‚å¿µ"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸Šä¸‹æ–‡å‹ç¼©æ¦‚å¿µ")
    print("=" * 60)

    print("""
    ä¸ºä»€ä¹ˆéœ€è¦ä¸Šä¸‹æ–‡å‹ç¼©ï¼Ÿ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    é—®é¢˜ 1ï¼šæ£€ç´¢åˆ°çš„æ–‡æ¡£åŒ…å«æ— å…³å†…å®¹
    é—®é¢˜ 2ï¼šä¸Šä¸‹æ–‡å¤ªé•¿ï¼Œè¶…å‡º token é™åˆ¶
    é—®é¢˜ 3ï¼šæ— å…³å†…å®¹å½±å“ç”Ÿæˆè´¨é‡
    
    è§£å†³æ–¹æ¡ˆï¼šä¸Šä¸‹æ–‡å‹ç¼©
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 åŸå§‹æ£€ç´¢ç»“æœ                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æ¡£ï¼ŒåŒ…å«äº†å¾ˆå¤šå†…å®¹ã€‚        â”‚   â”‚
    â”‚  â”‚  å…¶ä¸­æœ‰äº›æ˜¯ç›¸å…³çš„ï¼Œæ¯”å¦‚ Python æ˜¯ä¸€ç§         â”‚   â”‚
    â”‚  â”‚  ç¼–ç¨‹è¯­è¨€ã€‚ä½†ä¹Ÿæœ‰å¾ˆå¤šæ— å…³çš„è¯´æ˜...           â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                        â”‚                           â”‚
    â”‚                        â–¼                           â”‚
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚              â”‚   ä¸Šä¸‹æ–‡å‹ç¼©å™¨     â”‚                â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                        â”‚                           â”‚
    â”‚                        â–¼                           â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ã€‚                      â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                 å‹ç¼©åçš„ä¸Šä¸‹æ–‡                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæå–å¼å‹ç¼© ====================


def extractive_compression():
    """æå–å¼å‹ç¼©"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæå–å¼å‹ç¼©")
    print("=" * 60)

    try:
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        import numpy as np

        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

        # ç¤ºä¾‹æ–‡æ¡£
        document = """
Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚å®ƒç”± Guido van Rossum åˆ›å»ºã€‚
ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚Python æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ã€‚
å’–å•¡æ˜¯ä¸€ç§é¥®æ–™ã€‚Python æœ‰ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿã€‚
"""

        query = "Python çš„ç‰¹ç‚¹"

        # åˆ†å‰²æˆå¥å­
        sentences = [s.strip() for s in document.split("ã€‚") if s.strip()]

        # è®¡ç®—ç›¸ä¼¼åº¦
        query_emb = embeddings.embed_query(query)
        sent_embs = embeddings.embed_documents(sentences)

        def cosine_sim(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

        similarities = [cosine_sim(query_emb, s) for s in sent_embs]

        # é€‰æ‹©ç›¸å…³å¥å­
        threshold = 0.8
        relevant = [
            (s, sim) for s, sim in zip(sentences, similarities) if sim > threshold
        ]

        print(f"ğŸ“Œ æŸ¥è¯¢: '{query}'")
        print(f"\næå–çš„ç›¸å…³å¥å­ï¼ˆé˜ˆå€¼={threshold}ï¼‰ï¼š")
        for sent, sim in sorted(relevant, key=lambda x: -x[1]):
            print(f"  [{sim:.3f}] {sent}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šLangChain ContextualCompressionRetriever ====================


def langchain_compression():
    """LangChain ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šLangChain ContextualCompressionRetriever")
    print("=" * 60)

    try:
        from langchain.retrievers import ContextualCompressionRetriever
        from langchain.retrievers.document_compressors import LLMChainExtractor
        from langchain_google_genai import (
            ChatGoogleGenerativeAI,
            GoogleGenerativeAIEmbeddings,
        )
        from langchain_chroma import Chroma
        from langchain_core.documents import Document

        # å‡†å¤‡æ–‡æ¡£
        docs = [
            Document(
                page_content="""
Python æ˜¯ä¸€ç§è§£é‡Šå‹ç¼–ç¨‹è¯­è¨€ã€‚å®ƒä»¥ç®€æ´çš„è¯­æ³•è‘—ç§°ã€‚
æœ€è¿‘æˆ‘å»äº†ä¸€è¶Ÿè¶…å¸‚ï¼Œä¹°äº†å¾ˆå¤šæ°´æœã€‚
Python æ”¯æŒé¢å‘å¯¹è±¡ã€å‡½æ•°å¼ç­‰å¤šç§ç¼–ç¨‹èŒƒå¼ã€‚
ä»Šå¤©çš„å¤©æ°”é¢„æŠ¥è¯´ä¼šä¸‹é›¨ã€‚
Python çš„æ ‡å‡†åº“éå¸¸ä¸°å¯Œï¼Œæ¶µç›–ç½‘ç»œã€æ–‡ä»¶å¤„ç†ç­‰åŠŸèƒ½ã€‚
"""
            ),
        ]

        # åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        vectorstore = Chroma.from_documents(docs, embeddings)
        base_retriever = vectorstore.as_retriever()

        # åˆ›å»ºå‹ç¼©å™¨
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
        compressor = LLMChainExtractor.from_llm(llm)

        # åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )

        # æ£€ç´¢
        query = "Python æœ‰ä»€ä¹ˆç‰¹ç‚¹"
        results = compression_retriever.invoke(query)

        print(f"ğŸ“Œ æŸ¥è¯¢: '{query}'")
        print("\nå‹ç¼©åçš„ç»“æœï¼š")
        for doc in results:
            print(f"  {doc.page_content}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šç”Ÿæˆå¼å‹ç¼© ====================


def generative_compression():
    """ç”Ÿæˆå¼å‹ç¼©"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šç”Ÿæˆå¼å‹ç¼©")
    print("=" * 60)

    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain_core.prompts import ChatPromptTemplate

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

        document = """
Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºã€‚
å®ƒçš„è®¾è®¡å¼ºè°ƒä»£ç å¯è¯»æ€§å’Œç®€æ´çš„è¯­æ³•ã€‚Python æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼Œ
åŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‘½ä»¤å¼ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚Python çš„è§£é‡Šå™¨
å¯ä»¥åœ¨å¤šä¸ªå¹³å°ä¸Šè¿è¡Œã€‚Python æœ‰ä¸€ä¸ªå…¨é¢çš„æ ‡å‡†åº“ï¼Œæä¾›äº†
è®¸å¤šæ¨¡å—å’Œå·¥å…·ã€‚æ­¤å¤–ï¼ŒPython æ‹¥æœ‰æ´»è·ƒçš„ç¤¾åŒºå’Œä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ã€‚
"""

        query = "Python çš„æ ¸å¿ƒç‰¹ç‚¹"

        prompt = ChatPromptTemplate.from_template("""
è¯·æ ¹æ®æŸ¥è¯¢ï¼Œå‹ç¼©ä»¥ä¸‹æ–‡æ¡£ï¼Œåªä¿ç•™å›ç­”æŸ¥è¯¢æ‰€å¿…éœ€çš„å…³é”®ä¿¡æ¯ã€‚
å‹ç¼©åä¸è¶…è¿‡ 100 å­—ã€‚

æŸ¥è¯¢: {query}
æ–‡æ¡£: {document}

å‹ç¼©åçš„å†…å®¹:""")

        chain = prompt | llm
        result = chain.invoke({"query": query, "document": document})

        print(f"ğŸ“Œ æŸ¥è¯¢: '{query}'")
        print(f"\nåŸæ–‡é•¿åº¦: {len(document)} å­—ç¬¦")
        print(f"\nå‹ç¼©åçš„å†…å®¹ï¼š")
        print(f"  {result.content}")
        print(f"\nå‹ç¼©åé•¿åº¦: {len(result.content)} å­—ç¬¦")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šè¿‡æ»¤å¼å‹ç¼© ====================


def filter_compression():
    """è¿‡æ»¤å¼å‹ç¼©"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šè¿‡æ»¤å¼å‹ç¼©")
    print("=" * 60)

    try:
        from langchain.retrievers.document_compressors import EmbeddingsFilter
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        from langchain_core.documents import Document

        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

        # åˆ›å»ºç›¸å…³æ€§è¿‡æ»¤å™¨
        embeddings_filter = EmbeddingsFilter(
            embeddings=embeddings, similarity_threshold=0.75
        )

        # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
        docs = [
            Document(page_content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"),
            Document(page_content="ä»Šå¤©å¤©æ°”å¾ˆå¥½"),
            Document(page_content="Python æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼"),
        ]

        query = "Python çš„ç‰¹ç‚¹"
        filtered = embeddings_filter.compress_documents(docs, query)

        print(f"ğŸ“Œ æŸ¥è¯¢: '{query}'")
        print(f"\nè¿‡æ»¤å‰: {len(docs)} ä¸ªæ–‡æ¡£")
        print(f"è¿‡æ»¤å: {len(filtered)} ä¸ªæ–‡æ¡£")
        print("\nä¿ç•™çš„æ–‡æ¡£ï¼š")
        for doc in filtered:
            print(f"  - {doc.page_content}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šå‹ç¼©ç®¡é“ ====================


def compression_pipeline():
    """å‹ç¼©ç®¡é“"""
    print("\n" + "=" * 60)
    print("ç¬¬å…­éƒ¨åˆ†ï¼šå‹ç¼©ç®¡é“")
    print("=" * 60)

    code_example = """
from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain.text_splitter import CharacterTextSplitter

# åˆ›å»ºå‹ç¼©ç®¡é“
splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
llm_extractor = LLMChainExtractor.from_llm(llm)

# ç»„åˆå¤šä¸ªå‹ç¼©å™¨
pipeline = DocumentCompressorPipeline(
    transformers=[
        splitter,           # 1. å…ˆåˆ†å‰²
        embeddings_filter,  # 2. è¿‡æ»¤ä¸ç›¸å…³
        llm_extractor,      # 3. æå–å…³é”®ä¿¡æ¯
    ]
)

# ä½¿ç”¨ç®¡é“
compression_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline,
    base_retriever=base_retriever
)
"""
    print("ğŸ“Œ å‹ç¼©ç®¡é“ç¤ºä¾‹ï¼š")
    print(code_example)


# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå¯¹æ¯”å®éªŒ
        å¯¹æ¯”å‹ç¼©å‰å RAG çš„å›ç­”è´¨é‡ã€‚

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain.retrievers import ContextualCompressionRetriever
        from langchain.retrievers.document_compressors import LLMChainExtractor

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

        def compare_compression(query: str):
            # æ— å‹ç¼©æ£€ç´¢
            raw_docs = base_retriever.invoke(query)
            raw_context = "\\n".join(d.page_content for d in raw_docs)
            
            # å‹ç¼©æ£€ç´¢
            compressor = LLMChainExtractor.from_llm(llm)
            compression_retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=base_retriever
            )
            compressed_docs = compression_retriever.invoke(query)
            compressed_context = "\\n".join(d.page_content for d in compressed_docs)
            
            print(f"åŸå§‹ä¸Šä¸‹æ–‡é•¿åº¦: {len(raw_context)} å­—ç¬¦")
            print(f"å‹ç¼©åé•¿åº¦: {len(compressed_context)} å­—ç¬¦")
            print(f"å‹ç¼©ç‡: {len(compressed_context)/len(raw_context):.2%}")
            
            # å¯¹æ¯”å›ç­”è´¨é‡
            raw_answer = generate_answer(raw_context, query)
            compressed_answer = generate_answer(compressed_context, query)
            
            return {"raw": raw_answer, "compressed": compressed_answer}
        ```

    ç»ƒä¹  2ï¼šé˜ˆå€¼è°ƒä¼˜
        æµ‹è¯•ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼çš„è¿‡æ»¤æ•ˆæœã€‚

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from langchain.retrievers.document_compressors import EmbeddingsFilter

        thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
        query = "Python æ€§èƒ½ä¼˜åŒ–"

        for threshold in thresholds:
            embeddings_filter = EmbeddingsFilter(
                embeddings=embeddings,
                similarity_threshold=threshold
            )
            
            compression_retriever = ContextualCompressionRetriever(
                base_compressor=embeddings_filter,
                base_retriever=base_retriever
            )
            
            results = compression_retriever.invoke(query)
            
            print(f"é˜ˆå€¼={threshold}: {len(results)} ä¸ªæ–‡æ¡£è¢«ä¿ç•™")
            for doc in results:
                print(f"  {doc.page_content[:50]}...")
        ```

    ç»ƒä¹  3ï¼šè‡ªå®šä¹‰å‹ç¼©å™¨
        å®ç°ä¸€ä¸ªåŸºäºå…³é”®è¯çš„å‹ç¼©å™¨ã€‚

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from langchain.retrievers.document_compressors import BaseDocumentCompressor
        from langchain_core.documents import Document
        import re

        class KeywordCompressor(BaseDocumentCompressor):
            def __init__(self, top_sentences: int = 3):
                self.top_sentences = top_sentences

            def compress_documents(self, documents, query, callbacks=None):
                # æå–æŸ¥è¯¢å…³é”®è¯
                keywords = set(query.lower().split())
                
                compressed = []
                for doc in documents:
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', doc.page_content)
                    
                    # å¯¹æ¯ä¸ªå¥å­æ‰“åˆ†
                    scored = []
                    for sent in sentences:
                        if sent.strip():
                            score = sum(1 for kw in keywords if kw in sent.lower())
                            scored.append((score, sent))
                    
                    # å–å¾—åˆ†æœ€é«˜çš„å¥å­
                    scored.sort(reverse=True)
                    top_sents = [s for _, s in scored[:self.top_sentences]]
                    
                    if top_sents:
                        compressed.append(Document(
                            page_content="ã€‚".join(top_sents),
                            metadata=doc.metadata
                        ))
                
                return compressed
        ```

    æ€è€ƒé¢˜ï¼š
        1. å‹ç¼©å¯èƒ½ä¸¢å¤±å“ªäº›é‡è¦ä¿¡æ¯ï¼Ÿ
           
           âœ… ç­”æ¡ˆï¼š
           - ä¸Šä¸‹æ–‡å…³è”ï¼šå‰åæ–‡å…³ç³»å¯èƒ½ä¸¢å¤±
           - éšå¼ä¿¡æ¯ï¼šéœ€è¦æ¨ç†æ‰èƒ½å¾—å‡ºçš„ä¿¡æ¯
           - èƒŒæ™¯çŸ¥è¯†ï¼šç†è§£ç­”æ¡ˆæ‰€éœ€çš„å‰ç½®ä¿¡æ¯
           - ä¾‹å­å’Œç»†èŠ‚ï¼šå…·ä½“æ¡ˆä¾‹å¯èƒ½è¢«è¿‡æ»¤

        2. å¦‚ä½•å¹³è¡¡å‹ç¼©ç‡å’Œä¿¡æ¯ä¿ç•™ï¼Ÿ
           
           âœ… ç­”æ¡ˆï¼š
           - åŠ¨æ€é˜ˆå€¼ï¼šæ ¹æ®æ–‡æ¡£ç›¸ä¼¼åº¦è°ƒæ•´å‹ç¼©ç¨‹åº¦
           - åˆ†å±‚å‹ç¼©ï¼šå…ˆç²—è¿‡æ»¤ï¼Œå†ç²¾æå–
           - ä¿ç•™å…ƒæ•°æ®ï¼šå³ä½¿å‹ç¼©å†…å®¹ï¼Œä¿ç•™æ¥æºä¿¡æ¯
           - A/B æµ‹è¯•ï¼šæ‰¾åˆ°æœ€ä½³å‹ç¼©ç‡
           - ç”¨æˆ·åé¦ˆï¼šæ ¹æ®å›ç­”è´¨é‡è°ƒæ•´å‚æ•°
    """)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸Šä¸‹æ–‡å‹ç¼©")
    print("=" * 60)

    try:
        compression_concept()
        extractive_compression()
        langchain_compression()
        generative_compression()
        filter_compression()
        compression_pipeline()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š09-ensemble-retrieval.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
