"""
çˆ¶æ–‡æ¡£æ£€ç´¢
==========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£çˆ¶æ–‡æ¡£æ£€ç´¢çš„åŸç†
    2. æŒæ¡å°å—æ£€ç´¢ã€å¤§å—è¿”å›ç­–ç•¥
    3. å­¦ä¼šæ„å»ºå±‚æ¬¡åŒ–æ–‡æ¡£ç»“æ„

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Parent Chunkï¼šçˆ¶å—ï¼ˆå¤§å—ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
    - Child Chunkï¼šå­å—ï¼ˆå°å—ï¼Œç²¾ç¡®æ£€ç´¢ï¼‰
    - åŒå‘æ˜ å°„ï¼šå­å— â†’ çˆ¶å—

å‰ç½®çŸ¥è¯†ï¼š
    - 02-reranking.py

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install langchain langchain-openai chromadb python-dotenv
"""

import os
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šçˆ¶æ–‡æ¡£æ£€ç´¢æ¦‚å¿µ ====================


def parent_document_concept():
    """çˆ¶æ–‡æ¡£æ£€ç´¢æ¦‚å¿µ"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šçˆ¶æ–‡æ¡£æ£€ç´¢æ¦‚å¿µ")
    print("=" * 60)

    print("""
    å°å—æ£€ç´¢çš„é—®é¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - æ£€ç´¢ç²¾ç¡®ï¼Œä½†ä¸Šä¸‹æ–‡ä¸è¶³
    - LLM å¯èƒ½æ— æ³•è·å¾—å®Œæ•´ä¿¡æ¯
    
    å¤§å—æ£€ç´¢çš„é—®é¢˜ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - ä¸Šä¸‹æ–‡ä¸°å¯Œï¼Œä½†æ£€ç´¢ä¸ç²¾ç¡®
    - åŒ…å«å¤ªå¤šæ— å…³å†…å®¹
    
    çˆ¶æ–‡æ¡£æ£€ç´¢ç­–ç•¥ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    å°å—æ£€ç´¢ + å¤§å—è¿”å› = æœ€ä½³å¹³è¡¡ï¼
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  çˆ¶æ–‡æ¡£ (2000 å­—ç¬¦)                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡                            â”‚   â”‚
    â”‚  â”‚                                         â”‚   â”‚
    â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
    â”‚  â”‚   â”‚ å­å—1  â”‚ â”‚ å­å—2  â”‚ â”‚ å­å—3  â”‚     â”‚   â”‚
    â”‚  â”‚   â”‚(400å­—) â”‚ â”‚(400å­—) â”‚ â”‚(400å­—) â”‚     â”‚   â”‚
    â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
    â”‚  â”‚       â–²           åŒ¹é…ï¼                  â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚          â”‚                                     â”‚
    â”‚     Query åŒ¹é…å­å—2 â†’ è¿”å›æ•´ä¸ªçˆ¶æ–‡æ¡£            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ‰‹åŠ¨å®ç° ====================


def manual_implementation():
    """æ‰‹åŠ¨å®ç°çˆ¶æ–‡æ¡£æ£€ç´¢"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæ‰‹åŠ¨å®ç°")
    print("=" * 60)

    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document
    import uuid

    # å‡†å¤‡æ–‡æ¡£
    document = """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸã€‚å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ˜¯ä¸‰ç§ä¸»è¦çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç”¨äºå›¾åƒå¤„ç†ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç”¨äºåºåˆ—æ•°æ®å¤„ç†ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©æœºå™¨ç†è§£äººç±»è¯­è¨€ã€‚GPTã€BERT ç­‰æ¨¡å‹åœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
"""

    # åˆ›å»ºçˆ¶åˆ†å‰²å™¨
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)

    # åˆ›å»ºå­åˆ†å‰²å™¨
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=20)

    # åˆ†å‰²å¹¶å»ºç«‹æ˜ å°„
    parent_chunks = parent_splitter.split_text(document)
    parent_docs = {}
    child_docs = []

    for parent_text in parent_chunks:
        parent_id = str(uuid.uuid4())
        parent_docs[parent_id] = parent_text

        # åˆ›å»ºå­å—
        child_texts = child_splitter.split_text(parent_text)
        for child_text in child_texts:
            child_docs.append(
                Document(page_content=child_text, metadata={"parent_id": parent_id})
            )

    print(f"ğŸ“Œ åˆ†å‰²ç»“æœï¼š")
    print(f"  çˆ¶å—æ•°é‡: {len(parent_docs)}")
    print(f"  å­å—æ•°é‡: {len(child_docs)}")

    print("\nçˆ¶å—ç¤ºä¾‹ï¼š")
    for pid, ptext in list(parent_docs.items())[:1]:
        print(f"  [{pid[:8]}...] {ptext[:50]}...")

    print("\nå­å—ç¤ºä¾‹ï¼š")
    for cdoc in child_docs[:2]:
        print(
            f"  [parent: {cdoc.metadata['parent_id'][:8]}...] {cdoc.page_content[:40]}..."
        )


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šLangChain ParentDocumentRetriever ====================


def langchain_parent_retriever():
    """LangChain çˆ¶æ–‡æ¡£æ£€ç´¢å™¨"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šLangChain ParentDocumentRetriever")
    print("=" * 60)

    try:
        from langchain.retrievers import ParentDocumentRetriever
        from langchain.storage import InMemoryStore
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_openai import OpenAIEmbeddings
        from langchain_chroma import Chroma
        from langchain_core.documents import Document

        # å‡†å¤‡æ–‡æ¡£
        docs = [
            Document(
                page_content="""
Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºã€‚å®ƒçš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç å¯è¯»æ€§ï¼Œä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ã€‚Python æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‘½ä»¤å¼å’Œå‡½æ•°å¼ç¼–ç¨‹ã€‚
            """
            ),
            Document(
                page_content="""
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®ï¼Œæ— ç›‘ç£å­¦ä¹ å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼ã€‚å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥ã€‚
            """
            ),
        ]

        # åˆ›å»ºå­˜å‚¨
        store = InMemoryStore()

        # åˆ›å»ºåˆ†å‰²å™¨
        parent_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=100)

        # åˆ›å»ºæ£€ç´¢å™¨
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma(
            collection_name="child_chunks", embedding_function=embeddings
        )

        retriever = ParentDocumentRetriever(
            vectorstore=vectorstore,
            docstore=store,
            child_splitter=child_splitter,
            parent_splitter=parent_splitter,
        )

        # æ·»åŠ æ–‡æ¡£
        retriever.add_documents(docs)

        # æ£€ç´¢
        query = "Python çš„è®¾è®¡ç†å¿µ"
        results = retriever.invoke(query)

        print(f"ğŸ“Œ æŸ¥è¯¢: '{query}'")
        print(f"\næ£€ç´¢åˆ° {len(results)} ä¸ªçˆ¶æ–‡æ¡£ï¼š")
        for doc in results:
            print(f"  {doc.page_content[:80]}...")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šå¤šçº§å±‚æ¬¡ç»“æ„ ====================


def multi_level_hierarchy():
    """å¤šçº§å±‚æ¬¡ç»“æ„"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šå¤šçº§å±‚æ¬¡ç»“æ„")
    print("=" * 60)

    print("""
    ä¸‰çº§å±‚æ¬¡ç¤ºä¾‹ï¼š
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    æ–‡æ¡£çº§ (å®Œæ•´æ–‡ç« )
        â”‚
        â”œâ”€â”€ ç« èŠ‚çº§ (æ¯ç« çº¦ 2000 å­—)
        â”‚       â”‚
        â”‚       â”œâ”€â”€ æ®µè½çº§ (æ¯æ®µçº¦ 500 å­—)
        â”‚       â”‚       â”‚
        â”‚       â”‚       â””â”€â”€ å¥å­çº§ (æ£€ç´¢å•å…ƒ)
        â”‚       â”‚
        â”‚       â””â”€â”€ ...
        â”‚
        â””â”€â”€ ...
    
    æ£€ç´¢ç­–ç•¥ï¼š
    - ç”¨å¥å­çº§åšç²¾ç¡®åŒ¹é…
    - è¿”å›æ®µè½çº§æˆ–ç« èŠ‚çº§
    - æ ¹æ®éœ€æ±‚çµæ´»è°ƒæ•´
    """)

    code_example = '''
class MultiLevelRetriever:
    """å¤šçº§å±‚æ¬¡æ£€ç´¢å™¨"""
    
    def __init__(self, vectorstore, level_map):
        self.vectorstore = vectorstore
        self.level_map = level_map  # child_id -> parent_ids
    
    def retrieve(self, query, return_level="paragraph"):
        # åœ¨å¥å­çº§æ£€ç´¢
        matches = self.vectorstore.similarity_search(query)
        
        # æ ¹æ®éœ€è¦è¿”å›çš„å±‚çº§è·å–çˆ¶æ–‡æ¡£
        parent_ids = set()
        for match in matches:
            chain = self.level_map[match.metadata["id"]]
            parent_ids.add(chain[return_level])
        
        return [self.get_doc(pid) for pid in parent_ids]
'''
    print("ğŸ“Œ å¤šçº§æ£€ç´¢å™¨ç¤ºä¾‹ï¼š")
    print(code_example)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå‚æ•°è°ƒä¼˜
        æµ‹è¯•ä¸åŒçˆ¶å—/å­å—å¤§å°å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“ã€‚

    ç»ƒä¹  2ï¼šæŒä¹…åŒ–å­˜å‚¨
        ä½¿ç”¨ Redis æˆ– SQLite æ›¿ä»£ InMemoryStoreã€‚

    ç»ƒä¹  3ï¼šæ•ˆæœå¯¹æ¯”
        å¯¹æ¯”æ™®é€šæ£€ç´¢å’Œçˆ¶æ–‡æ¡£æ£€ç´¢çš„å›ç­”è´¨é‡ã€‚

    æ€è€ƒé¢˜ï¼š
        1. å­å—åº”è¯¥å¤šå°ï¼Ÿçˆ¶å—åº”è¯¥å¤šå¤§ï¼Ÿ
        2. é‡å ç‡å¦‚ä½•è®¾ç½®ï¼Ÿ
    """)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ çˆ¶æ–‡æ¡£æ£€ç´¢")
    print("=" * 60)

    try:
        parent_document_concept()
        manual_implementation()
        langchain_parent_retriever()
        multi_level_hierarchy()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š04-query-expansion.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
