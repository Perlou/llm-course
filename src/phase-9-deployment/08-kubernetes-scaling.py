"""
Kubernetes æ‰©å±•
===============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ K8s åŸºç¡€æ¦‚å¿µ
    2. éƒ¨ç½² LLM æœåŠ¡åˆ° K8s
    3. é…ç½® GPU è°ƒåº¦å’Œè‡ªåŠ¨æ‰©ç¼©å®¹

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Deploymentï¼šæ— çŠ¶æ€åº”ç”¨éƒ¨ç½²
    - Serviceï¼šæœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡
    - HPAï¼šæ°´å¹³è‡ªåŠ¨æ‰©ç¼©å®¹

ç¯å¢ƒè¦æ±‚ï¼š
    - kubectl
    - Kubernetes é›†ç¾¤ with GPU æ”¯æŒ
"""


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šK8s åŸºç¡€ ====================


def introduction():
    """K8s åŸºç¡€"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šK8s åŸºç¡€æ¦‚å¿µ")
    print("=" * 60)

    print("""
    ğŸ“Œ K8s æ ¸å¿ƒèµ„æºï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Pod      â†’ æœ€å°éƒ¨ç½²å•ä½ï¼ŒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨          â”‚
    â”‚  Deployment â†’ ç®¡ç† Pod çš„å‰¯æœ¬å’Œæ›´æ–°ç­–ç•¥              â”‚
    â”‚  Service  â†’ æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡                        â”‚
    â”‚  Ingress  â†’ å¤–éƒ¨è®¿é—®å…¥å£                               â”‚
    â”‚  HPA      â†’ æ°´å¹³è‡ªåŠ¨æ‰©ç¼©å®¹                            â”‚
    â”‚  PVC      â†’ æŒä¹…åŒ–å­˜å‚¨                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ LLM æœåŠ¡æ¶æ„ï¼š
    Ingress â†’ Service â†’ Deployment (Pod Ã— N) â†’ PVC (æ¨¡å‹å­˜å‚¨)
                              â†“
                         GPU Node Pool
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šDeployment é…ç½® ====================


def deployment_config():
    """Deployment é…ç½®"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šDeployment é…ç½®")
    print("=" * 60)

    yaml = """
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-service
  template:
    metadata:
      labels:
        app: llm-service
    spec:
      containers:
      - name: vllm
        image: llm-service:v1.0
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
          requests:
            memory: "16Gi"

        # æ¢é’ˆé…ç½®
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60  # æ¨¡å‹åŠ è½½éœ€è¦æ—¶é—´
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30

      # GPU èŠ‚ç‚¹è°ƒåº¦
      nodeSelector:
        nvidia.com/gpu.product: "NVIDIA-A100"

      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
"""
    print(yaml)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šService å’Œ Ingress ====================


def service_ingress():
    """Service å’Œ Ingress"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šService å’Œ Ingress")
    print("=" * 60)

    yaml = """
# Service
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm-service
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
# Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
spec:
  rules:
  - host: llm.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llm-service
            port:
              number: 80
"""
    print(yaml)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šHPA è‡ªåŠ¨æ‰©ç¼©å®¹ ====================


def hpa_config():
    """HPA é…ç½®"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šHPA è‡ªåŠ¨æ‰©ç¼©å®¹")
    print("=" * 60)

    yaml = """
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: requests_queue_size
      target:
        type: AverageValue
        averageValue: "50"  # é˜Ÿåˆ—ç§¯å‹è¶…50è§¦å‘æ‰©å®¹

# kubectl get hpa llm-hpa -w  # ç›‘æ§æ‰©ç¼©å®¹
"""
    print(yaml)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šç¼–å†™ LLM æœåŠ¡çš„ K8s éƒ¨ç½²æ¸…å•
    ç»ƒä¹  2ï¼šé…ç½® HPA å¹¶æµ‹è¯•è‡ªåŠ¨æ‰©ç¼©å®¹

    æ€è€ƒé¢˜ï¼šK8s éƒ¨ç½² LLM æœåŠ¡æœ‰ä»€ä¹ˆæŒ‘æˆ˜ï¼Ÿ
    ç­”æ¡ˆï¼š1. GPU è°ƒåº¦å¤æ‚ 2. æ¨¡å‹åŠ è½½æ—¶é—´é•¿ 3. èµ„æºæˆæœ¬é«˜
    """)


def main():
    introduction()
    deployment_config()
    service_ingress()
    hpa_config()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š09-monitoring-logging.py")


if __name__ == "__main__":
    main()
