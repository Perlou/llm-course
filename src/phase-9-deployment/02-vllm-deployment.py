"""
vLLM éƒ¨ç½²å®æˆ˜
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ vLLM çš„æ ¸å¿ƒæŠ€æœ¯ï¼ˆPagedAttentionã€Continuous Batchingï¼‰
    2. æŒæ¡ vLLM æœåŠ¡éƒ¨ç½²å’Œå‚æ•°è°ƒä¼˜
    3. ä½¿ç”¨ OpenAI å…¼å®¹ API è°ƒç”¨

æ ¸å¿ƒæ¦‚å¿µï¼š
    - PagedAttentionï¼šé«˜æ•ˆçš„ KV Cache ç®¡ç†
    - Continuous Batchingï¼šåŠ¨æ€æ‰¹å¤„ç†

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install vllm
    - NVIDIA GPU with CUDA 11.8+
"""

import os


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼švLLM æ ¸å¿ƒæŠ€æœ¯ ====================


def introduction():
    """vLLM æ ¸å¿ƒæŠ€æœ¯ä»‹ç»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼švLLM æ ¸å¿ƒæŠ€æœ¯")
    print("=" * 60)

    print("""
    ğŸ“Œ vLLM æ ¸å¿ƒä¼˜åŠ¿ï¼š
    1. é«˜ååé‡ - æ¯” HuggingFace å¿« 24x
    2. PagedAttention - æ˜¾å­˜åˆ©ç”¨ç‡æå‡ 2-4 å€
    3. Continuous Batching - åŠ¨æ€æ‰¹å¤„ç†
    4. OpenAI å…¼å®¹ API

    PagedAttention åŸç†ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç‰©ç†é¡µæ± : [P1][P2][P3][P4][P5][P6]...  â”‚
    â”‚ Seq1 -> [P1, P3, P5]  æŒ‰éœ€åˆ†é…         â”‚
    â”‚ Seq2 -> [P2, P4]      æ— ç¢ç‰‡           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€ä½¿ç”¨ ====================


def basic_usage():
    """vLLM åŸºç¡€ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€ä½¿ç”¨")
    print("=" * 60)

    print("""
    # å®‰è£…
    pip install vllm

    # ç¦»çº¿æ¨ç†ç¤ºä¾‹
    from vllm import LLM, SamplingParams

    llm = LLM(model="Qwen/Qwen2-7B-Instruct")
    sampling_params = SamplingParams(temperature=0.7, max_tokens=256)

    prompts = ["ä»‹ç»ä¸€ä¸‹ vLLM", "ä»€ä¹ˆæ˜¯ LLMï¼Ÿ"]
    outputs = llm.generate(prompts, sampling_params)

    for output in outputs:
        print(output.outputs[0].text)
    """)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæœåŠ¡éƒ¨ç½² ====================


def server_deployment():
    """æœåŠ¡éƒ¨ç½²"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šæœåŠ¡éƒ¨ç½²")
    print("=" * 60)

    print("""
    # å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡
    python -m vllm.entrypoints.openai.api_server \\
        --model Qwen/Qwen2-7B-Instruct \\
        --port 8000 \\
        --gpu-memory-utilization 0.9 \\
        --max-num-seqs 64

    # Python å®¢æˆ·ç«¯è°ƒç”¨
    from openai import OpenAI

    client = OpenAI(base_url="http://localhost:8000/v1", api_key="x")
    response = client.chat.completions.create(
        model="Qwen/Qwen2-7B-Instruct",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    print(response.choices[0].message.content)
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šå‚æ•°è°ƒä¼˜ ====================


def parameter_tuning():
    """å‚æ•°è°ƒä¼˜"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šå‚æ•°è°ƒä¼˜")
    print("=" * 60)

    print("""
    å…³é”®å‚æ•°ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        å‚æ•°            â”‚     è¯´æ˜       â”‚   æ¨èå€¼     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ gpu-memory-utilizationâ”‚ æ˜¾å­˜ä½¿ç”¨æ¯”ä¾‹   â”‚ 0.85-0.95   â”‚
    â”‚ max-num-seqs          â”‚ æœ€å¤§å¹¶å‘åºåˆ—   â”‚ 32-128      â”‚
    â”‚ max-model-len         â”‚ æœ€å¤§åºåˆ—é•¿åº¦   â”‚ æŒ‰éœ€è®¾ç½®     â”‚
    â”‚ tensor-parallel-size  â”‚ GPU å¹¶è¡Œæ•°     â”‚ GPU å¡æ•°    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    # é‡åŒ–æ¨¡å‹éƒ¨ç½²
    python -m vllm.entrypoints.openai.api_server \\
        --model Qwen/Qwen2-7B-Instruct-AWQ \\
        --quantization awq
    """)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šéƒ¨ç½² vLLM æœåŠ¡å¹¶æµ‹è¯•

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```bash
        # 1. å®‰è£… vLLM
        pip install vllm

        # 2. å¯åŠ¨æœåŠ¡
        python -m vllm.entrypoints.openai.api_server \\
            --model Qwen/Qwen2-1.5B-Instruct \\
            --port 8000 \\
            --gpu-memory-utilization 0.85

        # 3. æµ‹è¯•è°ƒç”¨
        curl http://localhost:8000/v1/chat/completions \\
            -H "Content-Type: application/json" \\
            -d '{
                "model": "Qwen/Qwen2-1.5B-Instruct",
                "messages": [{"role": "user", "content": "Hello!"}],
                "max_tokens": 100
            }'
        ```
        
        ```python
        # Python å®¢æˆ·ç«¯æµ‹è¯•
        from openai import OpenAI
        
        client = OpenAI(base_url="http://localhost:8000/v1", api_key="x")
        
        response = client.chat.completions.create(
            model="Qwen/Qwen2-1.5B-Instruct",
            messages=[{"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç» vLLM"}],
            max_tokens=100,
        )
        print(response.choices[0].message.content)
        ```
    
    ç»ƒä¹  2ï¼šå¯¹æ¯”ä¸åŒå‚æ•°é…ç½®çš„æ€§èƒ½å·®å¼‚

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import time
        import concurrent.futures
        from openai import OpenAI
        
        def benchmark_config(config_name, port):
            client = OpenAI(base_url=f"http://localhost:{port}/v1", api_key="x")
            
            prompts = ["ä»‹ç»äººå·¥æ™ºèƒ½", "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", "è§£é‡Šæ·±åº¦å­¦ä¹ "] * 10
            
            start = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                futures = [
                    executor.submit(
                        client.chat.completions.create,
                        model="Qwen/Qwen2-1.5B-Instruct",
                        messages=[{"role": "user", "content": p}],
                        max_tokens=100,
                    )
                    for p in prompts
                ]
                results = [f.result() for f in futures]
            
            total_time = time.time() - start
            total_tokens = sum(r.usage.completion_tokens for r in results)
            
            return {
                "config": config_name,
                "throughput": total_tokens / total_time,
                "avg_latency": total_time / len(prompts),
            }
        
        # æµ‹è¯•ä¸åŒé…ç½® (éœ€è¦å¯åŠ¨å¤šä¸ªæœåŠ¡)
        # é…ç½®1: max-num-seqs=16
        # é…ç½®2: max-num-seqs=64
        # é…ç½®3: max-num-seqs=128
        ```

    æ€è€ƒé¢˜ï¼švLLM ç›¸æ¯” HuggingFace generate æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

        âœ… ç­”ï¼š
        1. æ›´é«˜ååé‡ - PagedAttention å‡å°‘æ˜¾å­˜ç¢ç‰‡ï¼Œæ”¯æŒæ›´å¤šå¹¶å‘
        2. æ›´å¥½æ˜¾å­˜åˆ©ç”¨ç‡ - åŠ¨æ€ KV Cache åˆ†é…
        3. Continuous Batching - å®Œæˆå³å¡«å……ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨
        4. åŸç”Ÿ OpenAI API å…¼å®¹ - æ— ç¼æ›¿æ¢
        5. é‡åŒ–æ¨¡å‹æ”¯æŒ - AWQ/GPTQ æ— ç¼åŠ è½½
    """)


def main():
    introduction()
    basic_usage()
    server_deployment()
    parameter_tuning()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š03-tgi-deployment.py")


if __name__ == "__main__":
    main()
