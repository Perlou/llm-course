"""
FastAPI LLM æœåŠ¡
================

å­¦ä¹ ç›®æ ‡ï¼š
    1. ä½¿ç”¨ FastAPI æ„å»º LLM API æœåŠ¡
    2. å®ç°æµå¼å“åº” (SSE)
    3. æ·»åŠ ä¸­é—´ä»¶ï¼ˆè®¤è¯ã€é™æµï¼‰

æ ¸å¿ƒæ¦‚å¿µï¼š
    - FastAPIï¼šé«˜æ€§èƒ½ Python Web æ¡†æ¶
    - SSEï¼šServer-Sent Events æµå¼å“åº”
    - ä¸­é—´ä»¶ï¼šè¯·æ±‚/å“åº”å¤„ç†ç®¡é“

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install fastapi uvicorn pydantic openai
"""

import os
import json
import time
from typing import List, Optional
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæœåŠ¡æ¶æ„ ====================


def introduction():
    """æœåŠ¡æ¶æ„ä»‹ç»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæœåŠ¡æ¶æ„")
    print("=" * 60)

    print("""
    ğŸ“Œ LLM æœåŠ¡æ¶æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    FastAPI æœåŠ¡å±‚                       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚   â”‚  è·¯ç”±   â”‚ â†’  â”‚ ä¸­é—´ä»¶  â”‚ â†’  â”‚ å¤„ç†å™¨  â”‚           â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â”‚        â†“                                               â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚   â”‚           æ¨ç†å¼•æ“ (vLLM/TGI)           â”‚         â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    é¡¹ç›®ç»“æ„ï¼š
    llm-service/
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ main.py           # å…¥å£
    â”‚   â”œâ”€â”€ routers/          # è·¯ç”±
    â”‚   â”œâ”€â”€ models/           # æ•°æ®æ¨¡å‹
    â”‚   â””â”€â”€ services/         # ä¸šåŠ¡é€»è¾‘
    â””â”€â”€ requirements.txt
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€æœåŠ¡ ====================


def basic_service():
    """åŸºç¡€æœåŠ¡å®ç°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€æœåŠ¡å®ç°")
    print("=" * 60)

    code = """
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from openai import OpenAI

app = FastAPI(title="LLM API Service")

# è¿æ¥åç«¯æ¨ç†æœåŠ¡
client = OpenAI(base_url="http://localhost:8000/v1", api_key="x")

# è¯·æ±‚æ¨¡å‹
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "Qwen/Qwen2-7B-Instruct"
    max_tokens: int = 2048
    temperature: float = 0.7
    stream: bool = False

# éæµå¼å“åº”
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    response = client.chat.completions.create(
        model=request.model,
        messages=[m.dict() for m in request.messages],
        max_tokens=request.max_tokens,
        temperature=request.temperature
    )
    return response

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health():
    return {"status": "ok"}

# å¯åŠ¨: uvicorn app.main:app --host 0.0.0.0 --port 8080
"""
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæµå¼å“åº” ====================


def streaming_response():
    """æµå¼å“åº”å®ç°"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šæµå¼å“åº” (SSE)")
    print("=" * 60)

    code = """
from fastapi.responses import StreamingResponse
import json

@app.post("/v1/chat/completions/stream")
async def chat_stream(request: ChatRequest):
    async def generate():
        stream = client.chat.completions.create(
            model=request.model,
            messages=[m.dict() for m in request.messages],
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            stream=True
        )
        for chunk in stream:
            if chunk.choices[0].delta.content:
                data = {
                    "choices": [{
                        "delta": {"content": chunk.choices[0].delta.content},
                        "index": 0
                    }]
                }
                yield f"data: {json.dumps(data)}\\n\\n"
        yield "data: [DONE]\\n\\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )
"""
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä¸­é—´ä»¶ ====================


def middleware():
    """ä¸­é—´ä»¶å®ç°"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šä¸­é—´ä»¶")
    print("=" * 60)

    code = """
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
import time

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start = time.time()
        response = await call_next(request)
        duration = time.time() - start
        print(f"{request.method} {request.url.path} - {duration:.3f}s")
        return response

# API Key è®¤è¯ä¸­é—´ä»¶
class AuthMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        if request.url.path in ["/health", "/docs"]:
            return await call_next(request)

        api_key = request.headers.get("Authorization")
        if not api_key or not api_key.startswith("Bearer "):
            return JSONResponse({"error": "Unauthorized"}, 401)

        # éªŒè¯ API Key...
        return await call_next(request)

# æ³¨å†Œä¸­é—´ä»¶
app.add_middleware(LoggingMiddleware)
app.add_middleware(AuthMiddleware)
"""
    print(code)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šé€Ÿç‡é™åˆ¶ ====================


def rate_limiting():
    """é€Ÿç‡é™åˆ¶"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šé€Ÿç‡é™åˆ¶")
    print("=" * 60)

    code = """
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/v1/chat/completions")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿ 10 æ¬¡
async def chat_completions(request: Request, chat_request: ChatRequest):
    # ... å¤„ç†é€»è¾‘
    pass

# ä½¿ç”¨ Redis è¿›è¡Œåˆ†å¸ƒå¼é™æµ
from slowapi import Limiter
from slowapi.util import get_remote_address
import redis

redis_client = redis.Redis(host="localhost", port=6379)
limiter = Limiter(
    key_func=get_remote_address,
    storage_uri="redis://localhost:6379"
)
"""
    print(code)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šå®ç°å®Œæ•´çš„ FastAPI LLM æœåŠ¡

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        # app/main.py
        from fastapi import FastAPI, HTTPException
        from fastapi.responses import StreamingResponse
        from pydantic import BaseModel
        from typing import List, Optional
        from openai import OpenAI
        import json
        
        app = FastAPI(title="LLM API Service")
        client = OpenAI(base_url="http://localhost:8000/v1", api_key="x")
        
        class Message(BaseModel):
            role: str
            content: str
        
        class ChatRequest(BaseModel):
            messages: List[Message]
            model: str = "Qwen/Qwen2-7B-Instruct"
            max_tokens: int = 2048
            temperature: float = 0.7
            stream: bool = False
        
        @app.post("/v1/chat/completions")
        async def chat_completions(request: ChatRequest):
            if request.stream:
                return await stream_chat(request)
            
            response = client.chat.completions.create(
                model=request.model,
                messages=[m.dict() for m in request.messages],
                max_tokens=request.max_tokens,
                temperature=request.temperature,
            )
            return response
        
        async def stream_chat(request: ChatRequest):
            async def generate():
                stream = client.chat.completions.create(
                    model=request.model,
                    messages=[m.dict() for m in request.messages],
                    stream=True,
                )
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        yield f"data: {json.dumps({'content': chunk.choices[0].delta.content})}\\n\\n"
                yield "data: [DONE]\\n\\n"
            
            return StreamingResponse(generate(), media_type="text/event-stream")
        
        @app.get("/health")
        async def health():
            return {"status": "ok"}
        ```
    
    ç»ƒä¹  2ï¼šæ·»åŠ ç”¨æˆ·è®¤è¯å’Œé€Ÿç‡é™åˆ¶

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from fastapi import Request, HTTPException
        from fastapi.responses import JSONResponse
        from starlette.middleware.base import BaseHTTPMiddleware
        from slowapi import Limiter
        from slowapi.util import get_remote_address
        import time
        
        # API Key è®¤è¯
        VALID_API_KEYS = {"sk-abc123": "user1", "sk-xyz789": "user2"}
        
        class AuthMiddleware(BaseHTTPMiddleware):
            async def dispatch(self, request: Request, call_next):
                if request.url.path in ["/health", "/docs", "/openapi.json"]:
                    return await call_next(request)
                
                auth = request.headers.get("Authorization", "")
                if not auth.startswith("Bearer "):
                    return JSONResponse({"error": "Missing API key"}, 401)
                
                api_key = auth.replace("Bearer ", "")
                if api_key not in VALID_API_KEYS:
                    return JSONResponse({"error": "Invalid API key"}, 401)
                
                request.state.user = VALID_API_KEYS[api_key]
                return await call_next(request)
        
        # é€Ÿç‡é™åˆ¶
        limiter = Limiter(key_func=get_remote_address)
        
        @app.post("/v1/chat/completions")
        @limiter.limit("20/minute")
        async def chat_completions(request: Request, chat_request: ChatRequest):
            # ... å¤„ç†é€»è¾‘
            pass
        
        app.add_middleware(AuthMiddleware)
        ```

    æ€è€ƒé¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦åœ¨ LLM æœåŠ¡å‰æ·»åŠ ä¸€å±‚ API ç½‘å…³ï¼Ÿ

        âœ… ç­”ï¼š
        1. ç»Ÿä¸€è®¤è¯é‰´æƒ - é›†ä¸­ç®¡ç† API Key
        2. è´Ÿè½½å‡è¡¡ - åˆ†å‘åˆ°å¤šä¸ªæ¨ç†å®ä¾‹
        3. è¯·æ±‚é™æµ - ä¿æŠ¤åç«¯æœåŠ¡
        4. æ—¥å¿—å®¡è®¡ - è®°å½•æ‰€æœ‰è¯·æ±‚ç”¨äºåˆ†æ
        5. åè®®è½¬æ¢ - ç»Ÿä¸€ API æ ¼å¼
        6. ç¼“å­˜åŠ é€Ÿ - ç›¸åŒè¯·æ±‚å¯ç¼“å­˜
    """)


def main():
    introduction()
    basic_service()
    streaming_response()
    middleware()
    rate_limiting()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š05-async-processing.py")


if __name__ == "__main__":
    main()
