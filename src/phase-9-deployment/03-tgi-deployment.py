"""
TGI éƒ¨ç½²å®æˆ˜
===========

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ TGI çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯
    2. æŒæ¡ TGI Docker éƒ¨ç½²
    3. äº†è§£ TGI ä¸ vLLM çš„å¯¹æ¯”é€‰æ‹©

æ ¸å¿ƒæ¦‚å¿µï¼š
    - TGIï¼šHuggingFace å®˜æ–¹æ¨ç†æœåŠ¡
    - Flash Attentionï¼šé«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
    - å¤š LoRA æ”¯æŒï¼šåŠ¨æ€åŠ è½½é€‚é…å™¨

ç¯å¢ƒè¦æ±‚ï¼š
    - Docker with NVIDIA GPU support
    - æˆ– pip install text-generation
"""


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šTGI ä»‹ç» ====================


def introduction():
    """TGI ä»‹ç»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šTGI ä»‹ç»")
    print("=" * 60)

    print("""
    ğŸ“Œ TGI vs vLLM å¯¹æ¯”ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    ç‰¹æ€§    â”‚       TGI        â”‚       vLLM       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   ç”Ÿæ€     â”‚ HuggingFaceåŸç”Ÿ  â”‚ ç‹¬ç«‹é¡¹ç›®         â”‚
    â”‚   åŠŸèƒ½     â”‚ åŠŸèƒ½ä¸°å¯Œ         â”‚ ä¸“æ³¨æ¨ç†æ€§èƒ½     â”‚
    â”‚   æ€§èƒ½     â”‚ ä¼˜ç§€             â”‚ æ›´ä¼˜             â”‚
    â”‚   é‡åŒ–     â”‚ GPTQ/AWQ/EETQ    â”‚ GPTQ/AWQ/FP8     â”‚
    â”‚   éƒ¨ç½²     â”‚ Dockerä¼˜å…ˆ       â”‚ Python/Docker    â”‚
    â”‚   å¤šLoRA   â”‚ âœ… åŸç”Ÿæ”¯æŒ      â”‚ âœ… æ”¯æŒ          â”‚
    â”‚   æ°´å°     â”‚ âœ… æ”¯æŒ          â”‚ âŒ ä¸æ”¯æŒ        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    é€‰æ‹©å»ºè®®ï¼š
    - è¿½æ±‚æè‡´æ€§èƒ½ â†’ vLLM
    - éœ€è¦ HuggingFace ç”Ÿæ€ç‰¹æ€§ â†’ TGI
    - éœ€è¦è¾“å‡ºæ°´å°è¿½è¸ª â†’ TGI
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šDocker éƒ¨ç½² ====================


def docker_deployment():
    """Docker éƒ¨ç½²"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šDocker éƒ¨ç½²")
    print("=" * 60)

    print("""
    # åŸºç¡€éƒ¨ç½²
    docker run --gpus all \\
        -v ~/.cache/huggingface:/data \\
        -p 8080:80 \\
        ghcr.io/huggingface/text-generation-inference:latest \\
        --model-id Qwen/Qwen2-7B-Instruct

    # ç”Ÿäº§çº§é…ç½®
    docker run --gpus all \\
        -v ~/.cache/huggingface:/data \\
        -p 8080:80 \\
        ghcr.io/huggingface/text-generation-inference:latest \\
        --model-id Qwen/Qwen2-7B-Instruct \\
        --max-input-length 4096 \\
        --max-total-tokens 8192 \\
        --max-batch-prefill-tokens 4096 \\
        --quantize awq  # ä½¿ç”¨é‡åŒ–æ¨¡å‹

    # å…³é”®å‚æ•°è¯´æ˜
    --max-input-length      # æœ€å¤§è¾“å…¥é•¿åº¦
    --max-total-tokens      # æœ€å¤§æ€» token æ•°
    --max-batch-prefill-tokens  # é¢„å¡«å……æ‰¹å¤„ç† token æ•°
    --quantize              # é‡åŒ–æ–¹å¼: awq/gptq/eetq
    --num-shard             # GPU åˆ†ç‰‡æ•°ï¼ˆå¤šå¡ï¼‰
    """)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šAPI è°ƒç”¨ ====================


def api_usage():
    """API è°ƒç”¨"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šAPI è°ƒç”¨")
    print("=" * 60)

    print("""
    # Python å®¢æˆ·ç«¯
    from text_generation import Client

    client = Client("http://localhost:8080")

    # æ™®é€šç”Ÿæˆ
    response = client.generate(
        "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        max_new_tokens=256,
        temperature=0.7
    )
    print(response.generated_text)

    # æµå¼ç”Ÿæˆ
    for token in client.generate_stream("è®²ä¸€ä¸ªç¬‘è¯"):
        print(token.token.text, end="", flush=True)

    # REST API è°ƒç”¨
    curl http://localhost:8080/generate \\
        -H "Content-Type: application/json" \\
        -d '{
            "inputs": "Hello!",
            "parameters": {"max_new_tokens": 100}
        }'
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šé«˜çº§ç‰¹æ€§ ====================


def advanced_features():
    """é«˜çº§ç‰¹æ€§"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šé«˜çº§ç‰¹æ€§")
    print("=" * 60)

    print("""
    ğŸ“Œ TGI ç‰¹è‰²åŠŸèƒ½ï¼š

    1. å¤š LoRA åŠ¨æ€åŠ è½½
    docker run ... \\
        --lora-adapters my-adapter=/path/to/adapter

    2. è¾“å‡ºæ°´å°ï¼ˆç”¨äºè¿½è¸ªç”Ÿæˆå†…å®¹ï¼‰
    --watermark-gamma 0.5 --watermark-delta 2.0

    3. å†…ç½® Prometheus æŒ‡æ ‡
    curl http://localhost:8080/metrics

    4. OpenAI å…¼å®¹ç«¯ç‚¹
    curl http://localhost:8080/v1/chat/completions \\
        -H "Content-Type: application/json" \\
        -d '{"model": "tgi", "messages": [...]}'
    """)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šä½¿ç”¨ Docker éƒ¨ç½² TGI æœåŠ¡

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```bash
        # 1. æ‹‰å–é•œåƒ
        docker pull ghcr.io/huggingface/text-generation-inference:latest

        # 2. å¯åŠ¨æœåŠ¡
        docker run --gpus all \\
            -v ~/.cache/huggingface:/data \\
            -p 8080:80 \\
            ghcr.io/huggingface/text-generation-inference:latest \\
            --model-id Qwen/Qwen2-1.5B-Instruct \\
            --max-input-length 2048 \\
            --max-total-tokens 4096

        # 3. æµ‹è¯•è°ƒç”¨
        curl http://localhost:8080/generate \\
            -H "Content-Type: application/json" \\
            -d '{"inputs": "ä»‹ç»ä¸€ä¸‹ TGI", "parameters": {"max_new_tokens": 100}}'
        ```
        
        ```python
        # Python å®¢æˆ·ç«¯æµ‹è¯•
        from text_generation import Client
        
        client = Client("http://localhost:8080")
        
        # æ™®é€šç”Ÿæˆ
        response = client.generate("ä»€ä¹ˆæ˜¯ TGIï¼Ÿ", max_new_tokens=100)
        print(response.generated_text)
        
        # æµå¼ç”Ÿæˆ
        for token in client.generate_stream("è®²ä¸€ä¸ªç¬‘è¯", max_new_tokens=200):
            print(token.token.text, end="", flush=True)
        ```
    
    ç»ƒä¹  2ï¼šå¯¹æ¯” TGI å’Œ vLLM çš„å“åº”å»¶è¿Ÿ

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        import time
        import requests
        
        def test_tgi(prompt, n=10):
            latencies = []
            for _ in range(n):
                start = time.time()
                requests.post(
                    "http://localhost:8080/generate",
                    json={"inputs": prompt, "parameters": {"max_new_tokens": 100}}
                )
                latencies.append(time.time() - start)
            return sum(latencies) / len(latencies)
        
        def test_vllm(prompt, n=10):
            latencies = []
            for _ in range(n):
                start = time.time()
                requests.post(
                    "http://localhost:8000/v1/chat/completions",
                    json={
                        "model": "Qwen/Qwen2-1.5B-Instruct",
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": 100
                    }
                )
                latencies.append(time.time() - start)
            return sum(latencies) / len(latencies)
        
        prompt = "è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹"
        print(f"TGI å¹³å‡å»¶è¿Ÿ: {test_tgi(prompt):.3f}s")
        print(f"vLLM å¹³å‡å»¶è¿Ÿ: {test_vllm(prompt):.3f}s")
        ```

    æ€è€ƒé¢˜ï¼šä»€ä¹ˆåœºæ™¯ä¸‹é€‰æ‹© TGI è€Œä¸æ˜¯ vLLMï¼Ÿ

        âœ… ç­”ï¼š
        1. éœ€è¦æ°´å°è¿½è¸ª - TGI åŸç”Ÿæ”¯æŒè¾“å‡ºæ°´å°
        2. å¤š LoRA åŠ¨æ€åˆ‡æ¢ - è¿è¡Œæ—¶åˆ‡æ¢ä¸åŒé€‚é…å™¨
        3. HuggingFace ç”Ÿæ€é›†æˆ - ä¸ Hub æ·±åº¦é›†æˆ
        4. éœ€è¦ EETQ é‡åŒ– - TGI ç‰¹æœ‰
        5. ä¼ä¸šåˆè§„è¦æ±‚ - è¾“å‡ºå¯è¿½æº¯
    """)


def main():
    introduction()
    docker_deployment()
    api_usage()
    advanced_features()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š04-fastapi-llm-service.py")


if __name__ == "__main__":
    main()
