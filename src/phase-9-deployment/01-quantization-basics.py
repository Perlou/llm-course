"""
æ¨¡å‹é‡åŒ–åŸºç¡€
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£æ¨¡å‹é‡åŒ–çš„åŸç†å’Œå¿…è¦æ€§
    2. æŒæ¡å¸¸è§é‡åŒ–æ–¹æ³•ï¼ˆFP16ã€INT8ã€INT4ï¼‰
    3. å­¦ä¹ ä½¿ç”¨ GPTQã€AWQã€GGUF è¿›è¡Œæ¨¡å‹é‡åŒ–
    4. è¯„ä¼°é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

æ ¸å¿ƒæ¦‚å¿µï¼š
    - é‡åŒ–ï¼šå°†é«˜ç²¾åº¦æ•°å€¼è½¬æ¢ä¸ºä½ç²¾åº¦è¡¨ç¤º
    - ç²¾åº¦æŸå¤±ï¼šé‡åŒ–å¸¦æ¥çš„æ¨¡å‹èƒ½åŠ›ä¸‹é™
    - æ˜¾å­˜ä¼˜åŒ–ï¼šé‡åŒ–åæ¨¡å‹å ç”¨æ›´å°‘èµ„æº

å‰ç½®çŸ¥è¯†ï¼š
    - Phase 8 çš„å¾®è°ƒçŸ¥è¯†
    - äº†è§£æ¨¡å‹å‚æ•°å’Œæ¨ç†åŸºç¡€

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install transformers torch bitsandbytes accelerate
    - pip install auto-gptq autoawq  # é‡åŒ–å·¥å…·
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡åŒ–åŸºç¡€æ¦‚å¿µ ====================


def introduction():
    """
    æ¨¡å‹é‡åŒ–ä»‹ç»

    é‡åŒ–æ˜¯å°†æ¨¡å‹å‚æ•°ä»é«˜ç²¾åº¦ï¼ˆå¦‚FP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆå¦‚INT8ã€INT4ï¼‰çš„æŠ€æœ¯ã€‚
    è¿™å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶çš„æ˜¾å­˜éœ€æ±‚ã€‚
    """
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹é‡åŒ–æ¦‚è¿°")
    print("=" * 60)

    overview = """
    ğŸ“Œ ä»€ä¹ˆæ˜¯æ¨¡å‹é‡åŒ–ï¼Ÿ

    é‡åŒ–æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡é™ä½å‚æ•°çš„æ•°å€¼ç²¾åº¦æ¥å‡å°‘æ¨¡å‹å¤§å°ã€‚

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    é‡åŒ–å¯¹æ¯”ç¤ºä¾‹                          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    ç²¾åº¦       â”‚    æ¯å‚æ•°å­—èŠ‚  â”‚    7Bæ¨¡å‹å¤§å°           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    FP32      â”‚    4 bytes    â”‚    28 GB               â”‚
    â”‚    FP16      â”‚    2 bytes    â”‚    14 GB               â”‚
    â”‚    INT8      â”‚    1 byte     â”‚    7 GB                â”‚
    â”‚    INT4      â”‚    0.5 bytes  â”‚    3.5 GB              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ¯ é‡åŒ–çš„å¥½å¤„ï¼š
    1. å‡å°‘æ˜¾å­˜å ç”¨ - è®©å¤§æ¨¡å‹èƒ½åœ¨æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œ
    2. åŠ é€Ÿæ¨ç† - ä½ç²¾åº¦è®¡ç®—æ›´å¿«ï¼ˆä¾èµ–ç¡¬ä»¶æ”¯æŒï¼‰
    3. é™ä½æˆæœ¬ - ä½¿ç”¨æ›´ä¾¿å®œçš„ç¡¬ä»¶éƒ¨ç½²

    âš ï¸ é‡åŒ–çš„ä»£ä»·ï¼š
    1. ç²¾åº¦æŸå¤± - æ¨¡å‹èƒ½åŠ›å¯èƒ½ç•¥æœ‰ä¸‹é™
    2. é‡åŒ–è€—æ—¶ - æŸäº›æ–¹æ³•éœ€è¦æ ¡å‡†æ•°æ®
    3. å…¼å®¹æ€§ - éœ€è¦ç‰¹å®šåº“æ”¯æŒ
    """
    print(overview)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šBitsAndBytes åŠ¨æ€é‡åŒ– ====================


def bitsandbytes_quantization():
    """
    ä½¿ç”¨ BitsAndBytes è¿›è¡ŒåŠ¨æ€é‡åŒ–

    BitsAndBytes æ˜¯æœ€ç®€å•çš„é‡åŒ–æ–¹æ¡ˆï¼Œåªéœ€åŠ è½½æ—¶æŒ‡å®šå‚æ•°å³å¯ã€‚
    """
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šBitsAndBytes åŠ¨æ€é‡åŒ–")
    print("=" * 60)

    print("""
    ğŸ“Œ BitsAndBytes ç‰¹ç‚¹ï¼š
    - åŠ è½½æ—¶é‡åŒ–ï¼Œæ— éœ€é¢„å¤„ç†
    - æ”¯æŒ INT8 å’Œ INT4ï¼ˆNF4ï¼‰
    - ä¸ HuggingFace æ— ç¼é›†æˆ
    """)

    # INT8 é‡åŒ–é…ç½®
    print("\n1. INT8 é‡åŒ–é…ç½®ï¼š")
    print("-" * 40)

    int8_config = """
# INT8 é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,              # å¯ç”¨ 8bit é‡åŒ–
    llm_int8_threshold=6.0,         # ç¦»ç¾¤å€¼é˜ˆå€¼
    llm_int8_has_fp16_weight=False  # æ˜¯å¦ä¿ç•™ FP16 æƒé‡
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-1.5B-Instruct",
    quantization_config=quantization_config,
    device_map="auto"
)
"""
    print(int8_config)

    # INT4 (NF4) é‡åŒ–é…ç½®
    print("\n2. INT4 (NF4) é‡åŒ–é…ç½®ï¼š")
    print("-" * 40)

    int4_config = """
# INT4 é‡åŒ–é…ç½®ï¼ˆæ¨èä½¿ç”¨ NF4ï¼‰
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,                       # å¯ç”¨ 4bit é‡åŒ–
    bnb_4bit_quant_type="nf4",               # é‡åŒ–ç±»å‹: nf4 æˆ– fp4
    bnb_4bit_compute_dtype=torch.bfloat16,   # è®¡ç®—æ—¶ä½¿ç”¨çš„ç²¾åº¦
    bnb_4bit_use_double_quant=True           # åŒé‡åŒ–è¿›ä¸€æ­¥å‹ç¼©
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-1.5B-Instruct",
    quantization_config=quantization_config,
    device_map="auto"
)
"""
    print(int4_config)

    # å®é™…æ¼”ç¤ºåŠ è½½
    print("\n3. å®é™…åŠ è½½ INT4 é‡åŒ–æ¨¡å‹ï¼š")
    print("-" * 40)

    # æ£€æŸ¥æ˜¯å¦æœ‰ GPU
    if not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œè·³è¿‡å®é™…åŠ è½½æ¼”ç¤º")
        print("åœ¨æœ‰ GPU çš„ç¯å¢ƒä¸­ï¼Œå¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š")
        return

    try:
        # åˆ›å»ºé‡åŒ–é…ç½®
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        # åŠ è½½å°æ¨¡å‹æ¼”ç¤º
        model_name = "Qwen/Qwen2-0.5B-Instruct"
        print(f"åŠ è½½æ¨¡å‹: {model_name}")

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name, quantization_config=quantization_config, device_map="auto"
        )

        # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

        # æµ‹è¯•æ¨ç†
        messages = [{"role": "user", "content": "ä½ å¥½ï¼Œç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}]
        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        outputs = model.generate(**inputs, max_new_tokens=50)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"æ¨¡å‹å“åº”: {response}")

    except Exception as e:
        print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–å’Œè¶³å¤Ÿçš„æ˜¾å­˜")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šGPTQ ç¦»çº¿é‡åŒ– ====================


def gptq_quantization():
    """
    GPTQ ç¦»çº¿é‡åŒ–æ–¹æ³•

    GPTQ æ˜¯ä¸€ç§é«˜è´¨é‡çš„ç¦»çº¿é‡åŒ–æ–¹æ³•ï¼Œéœ€è¦æ ¡å‡†æ•°æ®ã€‚
    """
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šGPTQ ç¦»çº¿é‡åŒ–")
    print("=" * 60)

    print("""
    ğŸ“Œ GPTQ ç‰¹ç‚¹ï¼š
    - é€å±‚é‡åŒ– + è¯¯å·®è¡¥å¿
    - éœ€è¦æ ¡å‡†æ•°æ®é›†
    - é‡åŒ–è´¨é‡é«˜
    - é‡åŒ–åå¯ç›´æ¥åŠ è½½ï¼Œæ¨ç†å¿«
    """)

    # GPTQ é‡åŒ–æµç¨‹
    print("\n1. GPTQ é‡åŒ–æµç¨‹ï¼š")
    print("-" * 40)

    gptq_code = """
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

# 1. å‡†å¤‡æ ¡å‡†æ•°æ®
calibration_data = [
    "è¿™æ˜¯ä¸€æ®µç”¨äºæ ¡å‡†çš„æ–‡æœ¬...",
    "é‡åŒ–éœ€è¦ä»£è¡¨æ€§çš„æ•°æ®...",
    # é€šå¸¸éœ€è¦ 128-512 æ¡æ ·æœ¬
]

# 2. é…ç½® GPTQ å‚æ•°
gptq_config = GPTQConfig(
    bits=4,                    # é‡åŒ–ä½æ•°: 4 æˆ– 8
    group_size=128,            # åˆ†ç»„å¤§å°ï¼Œè¶Šå°ç²¾åº¦è¶Šé«˜
    dataset=calibration_data,  # æ ¡å‡†æ•°æ®
    desc_act=True,             # æ¿€æ´»å€¼é™åºå¤„ç†
)

# 3. åŠ è½½å¹¶é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-7B-Instruct",
    quantization_config=gptq_config,
    device_map="auto"
)

# 4. ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
model.save_pretrained("./qwen2-7b-gptq-4bit")
tokenizer.save_pretrained("./qwen2-7b-gptq-4bit")
"""
    print(gptq_code)

    # åŠ è½½ GPTQ é‡åŒ–æ¨¡å‹
    print("\n2. åŠ è½½å·²é‡åŒ–çš„ GPTQ æ¨¡å‹ï¼š")
    print("-" * 40)

    load_gptq = """
from transformers import AutoModelForCausalLM, AutoTokenizer

# ç›´æ¥åŠ è½½å·²é‡åŒ–çš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-7B-Instruct-GPTQ-Int4",  # HuggingFace ä¸Šçš„é‡åŒ–ç‰ˆæœ¬
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2-7B-Instruct-GPTQ-Int4"
)
"""
    print(load_gptq)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šAWQ é‡åŒ– ====================


def awq_quantization():
    """
    AWQï¼ˆActivation-aware Weight Quantizationï¼‰é‡åŒ–

    AWQ é€šè¿‡ä¿æŠ¤é‡è¦æƒé‡æ¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚
    """
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šAWQ é‡åŒ–")
    print("=" * 60)

    print("""
    ğŸ“Œ AWQ vs GPTQï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    ç‰¹æ€§    â”‚       AWQ        â”‚       GPTQ       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚   åŸç†     â”‚ ä¿æŠ¤é‡è¦æƒé‡     â”‚ é€å±‚é‡åŒ–+è¡¥å¿    â”‚
    â”‚   ç²¾åº¦     â”‚ ç•¥å¥½             â”‚ è‰¯å¥½             â”‚
    â”‚   é€Ÿåº¦     â”‚ ç¨å¿«             â”‚ è‰¯å¥½             â”‚
    â”‚   æ¨ç†     â”‚ éœ€è¦ AutoAWQ     â”‚ éœ€è¦ AutoGPTQ    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    # AWQ é‡åŒ–æµç¨‹
    print("\n1. AWQ é‡åŒ–æµç¨‹ï¼š")
    print("-" * 40)

    awq_code = """
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 1. åŠ è½½åŸå§‹æ¨¡å‹
model_path = "Qwen/Qwen2-7B-Instruct"
quant_path = "./qwen2-7b-awq-4bit"

model = AutoAWQForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 2. é…ç½®é‡åŒ–å‚æ•°
quant_config = {
    "w_bit": 4,              # æƒé‡é‡åŒ–ä½æ•°
    "q_group_size": 128,     # é‡åŒ–åˆ†ç»„å¤§å°
    "zero_point": True,      # é›¶ç‚¹é‡åŒ–
    "version": "GEMM"        # è®¡ç®—åç«¯ç‰ˆæœ¬
}

# 3. æ‰§è¡Œé‡åŒ–
model.quantize(
    tokenizer,
    quant_config=quant_config,
    calib_data="pileval"     # ä½¿ç”¨å†…ç½®æ ¡å‡†æ•°æ®é›†
)

# 4. ä¿å­˜é‡åŒ–æ¨¡å‹
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)
"""
    print(awq_code)

    # åŠ è½½ AWQ æ¨¡å‹
    print("\n2. åŠ è½½ AWQ é‡åŒ–æ¨¡å‹ï¼š")
    print("-" * 40)

    load_awq = """
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# åŠ è½½é‡åŒ–åçš„æ¨¡å‹
model = AutoAWQForCausalLM.from_quantized(
    "Qwen/Qwen2-7B-Instruct-AWQ",
    fuse_layers=True  # èåˆå±‚ä»¥åŠ é€Ÿæ¨ç†
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B-Instruct-AWQ")
"""
    print(load_awq)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šGGUF æ ¼å¼ï¼ˆllama.cppï¼‰ ====================


def gguf_quantization():
    """
    GGUF æ ¼å¼é‡åŒ–ï¼ˆç”¨äº llama.cppï¼‰

    GGUF æ˜¯ llama.cpp ä½¿ç”¨çš„é‡åŒ–æ ¼å¼ï¼Œæ”¯æŒ CPU æ¨ç†ã€‚
    """
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šGGUF æ ¼å¼é‡åŒ–")
    print("=" * 60)

    print("""
    ğŸ“Œ GGUF ç‰¹ç‚¹ï¼š
    - llama.cpp åŸç”Ÿæ ¼å¼
    - æ”¯æŒ CPU æ¨ç†
    - å¤šç§é‡åŒ–çº§åˆ«å¯é€‰
    - é€‚åˆè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
    """)

    # é‡åŒ–çº§åˆ«è¯´æ˜
    print("\n1. GGUF é‡åŒ–çº§åˆ«ï¼š")
    print("-" * 40)

    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   é‡åŒ–çº§åˆ«   â”‚    æ¨¡å‹å¤§å°æ¯”ä¾‹   â”‚    ç²¾åº¦æŸå¤±      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    Q2_K     â”‚    ~28%          â”‚    è¾ƒå¤§          â”‚
    â”‚    Q3_K_M   â”‚    ~35%          â”‚    ä¸­ç­‰          â”‚
    â”‚    Q4_K_M   â”‚    ~45%          â”‚    å°            â”‚
    â”‚    Q5_K_M   â”‚    ~55%          â”‚    å¾ˆå°          â”‚
    â”‚    Q6_K     â”‚    ~65%          â”‚    æå°          â”‚
    â”‚    Q8_0     â”‚    ~85%          â”‚    å‡ ä¹æ—         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    æ¨èé€‰æ‹©ï¼š
    - è´¨é‡ä¼˜å…ˆï¼šQ5_K_M æˆ– Q6_K
    - å¹³è¡¡æ–¹æ¡ˆï¼šQ4_K_Mï¼ˆæœ€å¸¸ç”¨ï¼‰
    - æé™å‹ç¼©ï¼šQ3_K_M
    """)

    # è½¬æ¢æµç¨‹
    print("\n2. è½¬æ¢ä¸º GGUF æ ¼å¼ï¼š")
    print("-" * 40)

    gguf_code = """
# ä½¿ç”¨ llama.cpp çš„è½¬æ¢è„šæœ¬

# 1. å…‹éš† llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. è½¬æ¢ HuggingFace æ¨¡å‹ä¸º GGUF
python convert_hf_to_gguf.py \\
    /path/to/Qwen2-7B-Instruct \\
    --outfile qwen2-7b.gguf \\
    --outtype f16

# 4. é‡åŒ–
./llama-quantize \\
    qwen2-7b.gguf \\
    qwen2-7b-q4_k_m.gguf \\
    q4_k_m
"""
    print(gguf_code)

    # ä½¿ç”¨ GGUF æ¨¡å‹
    print("\n3. ä½¿ç”¨ GGUF æ¨¡å‹ï¼ˆé€šè¿‡ llama-cpp-pythonï¼‰ï¼š")
    print("-" * 40)

    use_gguf = """
from llama_cpp import Llama

# åŠ è½½ GGUF æ¨¡å‹
llm = Llama(
    model_path="./qwen2-7b-q4_k_m.gguf",
    n_ctx=4096,           # ä¸Šä¸‹æ–‡é•¿åº¦
    n_gpu_layers=35,      # GPU å±‚æ•°ï¼ˆ0 è¡¨ç¤ºçº¯ CPUï¼‰
    n_threads=8           # CPU çº¿ç¨‹æ•°
)

# æ¨ç†
output = llm(
    "è¯·ä»‹ç»ä¸€ä¸‹é‡åŒ–æŠ€æœ¯ï¼š",
    max_tokens=256,
    temperature=0.7
)

print(output["choices"][0]["text"])
"""
    print(use_gguf)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šé‡åŒ–è¯„ä¼°ä¸å¯¹æ¯” ====================


def quantization_evaluation():
    """
    é‡åŒ–æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”
    """
    print("\n" + "=" * 60)
    print("ç¬¬å…­éƒ¨åˆ†ï¼šé‡åŒ–è¯„ä¼°ä¸å¯¹æ¯”")
    print("=" * 60)

    # è¯„ä¼°ç»´åº¦
    print("\n1. è¯„ä¼°ç»´åº¦ï¼š")
    print("-" * 40)

    print("""
    é‡åŒ–æ¨¡å‹è¯„ä¼°éœ€è¦ä»å¤šä¸ªç»´åº¦è¿›è¡Œï¼š

    ğŸ“Š æ€§èƒ½è¯„ä¼°
    - å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ï¼šè¶Šä½è¶Šå¥½
    - åŸºå‡†æµ‹è¯•ï¼šMMLUã€C-Eval ç­‰
    - ä»»åŠ¡å‡†ç¡®ç‡ï¼šç‰¹å®šä»»åŠ¡çš„è¡¨ç°

    âš¡ æ•ˆç‡è¯„ä¼°
    - æ˜¾å­˜å ç”¨
    - æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰
    - é¦– Token å»¶è¿Ÿï¼ˆTTFTï¼‰

    ğŸ’¡ å®ç”¨æ€§è¯„ä¼°
    - ç”Ÿæˆè´¨é‡ä¸»è§‚è¯„ä¼°
    - A/B æµ‹è¯•å¯¹æ¯”
    """)

    # è¯„ä¼°ä»£ç 
    print("\n2. å›°æƒ‘åº¦è¯„ä¼°ç¤ºä¾‹ï¼š")
    print("-" * 40)

    eval_code = """
import torch
from tqdm import tqdm
from datasets import load_dataset

def evaluate_perplexity(model, tokenizer, dataset, max_samples=100):
    '''è¯„ä¼°æ¨¡å‹å›°æƒ‘åº¦'''
    model.eval()
    total_loss = 0
    total_tokens = 0

    for sample in tqdm(dataset[:max_samples]):
        inputs = tokenizer(
            sample["text"],
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(model.device)

        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss

        total_loss += loss.item() * inputs["input_ids"].size(1)
        total_tokens += inputs["input_ids"].size(1)

    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))
    return perplexity.item()

# ä½¿ç”¨
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
ppl = evaluate_perplexity(model, tokenizer, dataset)
print(f"å›°æƒ‘åº¦: {ppl:.2f}")
"""
    print(eval_code)

    # é‡åŒ–æ–¹æ³•å¯¹æ¯”
    print("\n3. é‡åŒ–æ–¹æ³•é€‰æ‹©æŒ‡å—ï¼š")
    print("-" * 40)

    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    é‡åŒ–æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        ç²¾åº¦è¦æ±‚é«˜ï¼Ÿ
                            â”‚
              â”Œâ”€â”€â”€â”€â”€æ˜¯â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€å¦â”€â”€â”€â”€â”€â”
              â†“                           â†“
           FP16/INT8                   INT4é‡åŒ–
              â”‚                           â”‚
              â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                 â†“                   â†“
              â”‚              GPUéƒ¨ç½²             CPUéƒ¨ç½²
              â”‚              AWQ/GPTQ              GGUF
              â†“                 â†“                   â†“
         æ˜¾å­˜å……è¶³æ—¶          æ˜¾å­˜å—é™æ—¶          è¾¹ç¼˜è®¾å¤‡

    å…·ä½“æ¨èï¼š
    - æœåŠ¡å™¨ GPU å……è¶³ â†’ FP16 æˆ– INT8ï¼ˆç®€å•å¿«é€Ÿï¼‰
    - æ˜¾å­˜å—é™ â†’ AWQ/GPTQ 4bitï¼ˆè´¨é‡ä¸æ•ˆç‡å¹³è¡¡ï¼‰
    - CPU/è¾¹ç¼˜è®¾å¤‡ â†’ GGUF Q4_K_M
    - éœ€è¦å¿«é€Ÿéƒ¨ç½² â†’ BitsAndBytes åŠ¨æ€é‡åŒ–
    """)


# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ ä¸æ€è€ƒ"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    exercises_text = """
    ğŸ“ ç»ƒä¹  1ï¼šå¯¹æ¯”ä¸åŒé‡åŒ–æ–¹æ³•çš„æ˜¾å­˜å ç”¨
    -------------------------
    ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹ï¼Œåˆ†åˆ«ç”¨ FP16ã€INT8ã€INT4 åŠ è½½ï¼Œ
    æ¯”è¾ƒä¸åŒç²¾åº¦ä¸‹çš„æ˜¾å­˜å ç”¨å’Œæ¨ç†é€Ÿåº¦ã€‚

    å‚è€ƒç­”æ¡ˆï¼š
    ```python
    import torch
    from transformers import AutoModelForCausalLM, BitsAndBytesConfig

    model_name = "Qwen/Qwen2-1.5B-Instruct"
    results = []

    # FP16
    torch.cuda.empty_cache()
    model_fp16 = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16, device_map="auto"
    )
    results.append(("FP16", torch.cuda.memory_allocated() / 1024**3))
    del model_fp16

    # INT8
    torch.cuda.empty_cache()
    model_int8 = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=BitsAndBytesConfig(load_in_8bit=True),
        device_map="auto"
    )
    results.append(("INT8", torch.cuda.memory_allocated() / 1024**3))
    del model_int8

    # INT4
    torch.cuda.empty_cache()
    model_int4 = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=BitsAndBytesConfig(load_in_4bit=True),
        device_map="auto"
    )
    results.append(("INT4", torch.cuda.memory_allocated() / 1024**3))

    for name, mem in results:
        print(f"{name}: {mem:.2f} GB")
    ```

    ğŸ“ ç»ƒä¹  2ï¼šè¯„ä¼°é‡åŒ–å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“
    -------------------------
    å‡†å¤‡ä¸€ç»„æµ‹è¯•é—®é¢˜ï¼Œåˆ†åˆ«ç”¨åŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œ
    äººå·¥è¯„ä¼°ç”Ÿæˆè´¨é‡çš„å·®å¼‚ã€‚

    æ€è€ƒé¢˜ï¼š
    - Q: ä»€ä¹ˆæƒ…å†µä¸‹é‡åŒ–æŸå¤±å¯ä»¥æ¥å—ï¼Ÿ
    - A: å½“ä»»åŠ¡ä¸éœ€è¦æé«˜ç²¾åº¦æ—¶ï¼ˆå¦‚é—²èŠã€ç®€å•é—®ç­”ï¼‰ï¼Œ
         æˆ–è€…èµ„æºé™åˆ¶å¿…é¡»ä½¿ç”¨é‡åŒ–æ—¶ã€‚å¯¹äºéœ€è¦ç²¾ç¡®è®¡ç®—
         æˆ–ä¸“ä¸šé¢†åŸŸçš„ä»»åŠ¡ï¼Œå»ºè®®è°¨æ…è¯„ä¼°é‡åŒ–å½±å“ã€‚

    - Q: å¦‚ä½•åœ¨é‡åŒ–åæ¢å¤éƒ¨åˆ†ç²¾åº¦æŸå¤±ï¼Ÿ
    - A: å¯ä»¥è€ƒè™‘ï¼š
         1. ä½¿ç”¨æ›´é«˜çš„é‡åŒ–ä½æ•°ï¼ˆå¦‚ Q5 ä»£æ›¿ Q4ï¼‰
         2. ä½¿ç”¨ QLoRA è¿›è¡Œé‡åŒ–åå¾®è°ƒ
         3. é€‰æ‹©æ›´å¥½çš„é‡åŒ–æ–¹æ³•ï¼ˆå¦‚ AWQ é€šå¸¸æ¯” GPTQ ç•¥å¥½ï¼‰
    """
    print(exercises_text)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•° - æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰éƒ¨åˆ†"""
    introduction()
    bitsandbytes_quantization()
    gptq_quantization()
    awq_quantization()
    gguf_quantization()
    quantization_evaluation()
    exercises()

    print("\n" + "=" * 60)
    print("è¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š02-vllm-deployment.py")
    print("=" * 60)


if __name__ == "__main__":
    main()
