"""
ç›‘æ§ä¸æ—¥å¿—
==========

å­¦ä¹ ç›®æ ‡ï¼š
    1. è®¾è®¡ LLM æœåŠ¡ç›‘æ§æŒ‡æ ‡
    2. å®ç° Prometheus æŒ‡æ ‡æš´éœ²
    3. é…ç½®æ—¥å¿—è§„èŒƒ

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Prometheusï¼šæŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ
    - Grafanaï¼šå¯è§†åŒ–é¢æ¿
    - ç»“æ„åŒ–æ—¥å¿—ï¼šJSON æ ¼å¼æ—¥å¿—

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install prometheus-client
    - Prometheus + Grafanaï¼ˆå¯é€‰ï¼‰
"""

import time
import json
import logging


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘æ§æŒ‡æ ‡ä½“ç³» ====================


def introduction():
    """ç›‘æ§æŒ‡æ ‡ä½“ç³»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šç›‘æ§æŒ‡æ ‡ä½“ç³»")
    print("=" * 60)

    print("""
    ğŸ“Œ ç›‘æ§æŒ‡æ ‡é‡‘å­—å¡”ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
    â”‚                    â”‚ ä¸šåŠ¡æŒ‡æ ‡ â”‚ â† æˆåŠŸç‡ã€ç”¨æˆ·æ»¡æ„åº¦    â”‚
    â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                          â”‚
    â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
    â”‚               â”‚     æ€§èƒ½æŒ‡æ ‡       â”‚ â† å»¶è¿Ÿ/åå        â”‚
    â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚          â”‚         èµ„æºæŒ‡æ ‡            â”‚ â† GPU/å†…å­˜     â”‚
    â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
    â”‚     â”‚              åŸºç¡€è®¾æ–½                 â”‚ â† èŠ‚ç‚¹    â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“Œ å…³é”®æŒ‡æ ‡ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       æŒ‡æ ‡åç§°       â”‚   ç±»å‹   â”‚   å‘Šè­¦é˜ˆå€¼  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ llm_request_latency â”‚ Histogramâ”‚ P99 > 30s  â”‚
    â”‚ llm_ttft_seconds    â”‚ Histogramâ”‚ P95 > 3s   â”‚
    â”‚ llm_tokens_per_sec  â”‚ Gauge    â”‚ < 10       â”‚
    â”‚ llm_queue_size      â”‚ Gauge    â”‚ > 100      â”‚
    â”‚ gpu_memory_used     â”‚ Gauge    â”‚ > 90%      â”‚
    â”‚ llm_error_total     â”‚ Counter  â”‚ é”™è¯¯ç‡>1%  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šPrometheus é›†æˆ ====================


def prometheus_integration():
    """Prometheus é›†æˆ"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šPrometheus é›†æˆ")
    print("=" * 60)

    code = """
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Response
import time

app = FastAPI()

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter(
    "llm_request_total", "Total requests",
    ["model", "status"]
)
REQUEST_LATENCY = Histogram(
    "llm_request_latency_seconds", "Request latency",
    buckets=[0.5, 1, 2, 5, 10, 30, 60]
)
TTFT = Histogram(
    "llm_ttft_seconds", "Time to first token",
    buckets=[0.1, 0.5, 1, 2, 5]
)
QUEUE_SIZE = Gauge(
    "llm_queue_size", "Current queue size"
)

# æš´éœ²æŒ‡æ ‡ç«¯ç‚¹
@app.get("/metrics")
async def metrics():
    return Response(
        generate_latest(),
        media_type="text/plain"
    )

# ä½¿ç”¨ç¤ºä¾‹
@app.post("/v1/chat/completions")
async def chat(request):
    start = time.time()
    try:
        result = await generate(request)
        REQUEST_COUNT.labels(model="qwen2", status="success").inc()
        return result
    finally:
        REQUEST_LATENCY.observe(time.time() - start)
"""
    print(code)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»“æ„åŒ–æ—¥å¿— ====================


def structured_logging():
    """ç»“æ„åŒ–æ—¥å¿—"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»“æ„åŒ–æ—¥å¿—")
    print("=" * 60)

    print("""
    ğŸ“Œ æ—¥å¿—æ ¼å¼è§„èŒƒï¼ˆJSONï¼‰ï¼š

    è¯·æ±‚æ—¥å¿—ï¼š
    {
      "timestamp": "2024-01-15T10:30:00.000Z",
      "level": "INFO",
      "trace_id": "abc-123",
      "event": "inference_complete",
      "model": "qwen2-7b",
      "input_tokens": 256,
      "output_tokens": 512,
      "ttft_ms": 180,
      "total_ms": 3200,
      "status": "success"
    }

    é”™è¯¯æ—¥å¿—ï¼š
    {
      "timestamp": "2024-01-15T10:31:00.000Z",
      "level": "ERROR",
      "trace_id": "xyz-789",
      "event": "inference_failed",
      "error_type": "OOMError",
      "error_message": "CUDA out of memory"
    }
    """)

    code = """
import structlog
import logging

# é…ç½®ç»“æ„åŒ–æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    wrapper_class=structlog.BoundLogger,
    logger_factory=structlog.PrintLoggerFactory()
)

logger = structlog.get_logger()

# ä½¿ç”¨
logger.info(
    "inference_complete",
    model="qwen2-7b",
    input_tokens=256,
    output_tokens=512,
    total_ms=3200
)
"""
    print(code)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šç›‘æ§æ ˆéƒ¨ç½² ====================


def monitoring_stack():
    """ç›‘æ§æ ˆéƒ¨ç½²"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šç›‘æ§æ ˆéƒ¨ç½²")
    print("=" * 60)

    print("""
    ğŸ“Œ å¯è§‚æµ‹æ€§æ¶æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  åº”ç”¨æŒ‡æ ‡ â”€â”€â†’ Prometheus â”€â”€â†’ Grafana                   â”‚
    â”‚                    â†“                                    â”‚
    â”‚              AlertManager â”€â”€â†’ é’‰é’‰/Slack               â”‚
    â”‚                                                         â”‚
    â”‚  åº”ç”¨æ—¥å¿— â”€â”€â†’ Fluent Bit â”€â”€â†’ Loki â”€â”€â†’ Grafana          â”‚
    â”‚                                                         â”‚
    â”‚  GPUç›‘æ§ â”€â”€â†’ DCGM Exporter â”€â”€â†’ Prometheus              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    # Prometheus é…ç½®ï¼ˆprometheus.ymlï¼‰
    scrape_configs:
      - job_name: 'llm-service'
        static_configs:
          - targets: ['localhost:8000']
        metrics_path: /metrics
        scrape_interval: 15s
    """)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç»ƒä¹  ====================


def exercises():
    """ç»ƒä¹ """
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    print("""
    ç»ƒä¹  1ï¼šä¸º FastAPI æœåŠ¡æ·»åŠ  Prometheus æŒ‡æ ‡

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```python
        from fastapi import FastAPI, Response
        from prometheus_client import (
            Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
        )
        import time
        
        app = FastAPI()
        
        # å®šä¹‰æŒ‡æ ‡
        REQUEST_TOTAL = Counter(
            "llm_request_total",
            "Total number of requests",
            ["model", "status"]
        )
        
        REQUEST_LATENCY = Histogram(
            "llm_request_latency_seconds",
            "Request latency in seconds",
            buckets=[0.5, 1, 2, 5, 10, 30, 60, 120]
        )
        
        TTFT = Histogram(
            "llm_ttft_seconds",
            "Time to first token",
            buckets=[0.1, 0.2, 0.5, 1, 2, 5]
        )
        
        TOKENS_PER_SECOND = Gauge(
            "llm_tokens_per_second",
            "Token generation speed"
        )
        
        QUEUE_SIZE = Gauge(
            "llm_queue_size",
            "Current request queue size"
        )
        
        # æš´éœ²æŒ‡æ ‡ç«¯ç‚¹
        @app.get("/metrics")
        def metrics():
            return Response(
                generate_latest(),
                media_type=CONTENT_TYPE_LATEST
            )
        
        # ä½¿ç”¨è£…é¥°å™¨è®°å½•æŒ‡æ ‡
        @app.post("/v1/chat/completions")
        async def chat(request: ChatRequest):
            start = time.time()
            try:
                result = await generate(request)
                REQUEST_TOTAL.labels(
                    model=request.model,
                    status="success"
                ).inc()
                return result
            except Exception as e:
                REQUEST_TOTAL.labels(
                    model=request.model,
                    status="error"
                ).inc()
                raise
            finally:
                REQUEST_LATENCY.observe(time.time() - start)
        ```
    
    ç»ƒä¹  2ï¼šé…ç½® Grafana ä»ªè¡¨æ¿å±•ç¤º LLM å…³é”®æŒ‡æ ‡

        âœ… å‚è€ƒç­”æ¡ˆï¼š
        ```json
        {
          "dashboard": {
            "title": "LLM Service Dashboard",
            "panels": [
              {
                "title": "è¯·æ±‚ QPS",
                "type": "graph",
                "targets": [{
                  "expr": "rate(llm_request_total[1m])"
                }]
              },
              {
                "title": "P99 å»¶è¿Ÿ",
                "type": "graph",
                "targets": [{
                  "expr": "histogram_quantile(0.99, rate(llm_request_latency_seconds_bucket[5m]))"
                }]
              },
              {
                "title": "TTFT P95",
                "type": "graph",
                "targets": [{
                  "expr": "histogram_quantile(0.95, rate(llm_ttft_seconds_bucket[5m]))"
                }]
              },
              {
                "title": "é˜Ÿåˆ—å¤§å°",
                "type": "stat",
                "targets": [{
                  "expr": "llm_queue_size"
                }]
              },
              {
                "title": "é”™è¯¯ç‡",
                "type": "graph",
                "targets": [{
                  "expr": "rate(llm_request_total{status='error'}[5m]) / rate(llm_request_total[5m])"
                }]
              }
            ]
          }
        }
        ```

    æ€è€ƒé¢˜ï¼šTTFTï¼ˆé¦– Token å»¶è¿Ÿï¼‰ä¸ºä»€ä¹ˆæ˜¯é‡è¦æŒ‡æ ‡ï¼Ÿ

        âœ… ç­”ï¼š
        1. ç”¨æˆ·æ„ŸçŸ¥ - ç›´æ¥å½±å“ç”¨æˆ·ç­‰å¾…ä½“éªŒ
        2. æµå¼æ ¸å¿ƒ - å†³å®šæµå¼å“åº”ä½•æ—¶å¼€å§‹
        3. é¢„å¡«å……æ€§èƒ½ - åæ˜ æ¨¡å‹åŠ è½½å’Œé¢„å¤„ç†æ•ˆç‡
        4. é•¿çŸ­è¯·æ±‚åŒºåˆ† - ä¸æ€»å»¶è¿Ÿé…åˆåˆ†æè¯·æ±‚ç±»å‹
        5. SLA å…³é”®æŒ‡æ ‡ - é€šå¸¸ç”¨äºå®šä¹‰æœåŠ¡è´¨é‡
    """)


def main():
    introduction()
    prometheus_integration()
    structured_logging()
    monitoring_stack()
    exercises()
    print("\nè¯¾ç¨‹å®Œæˆï¼ä¸‹ä¸€æ­¥ï¼š10-security-guardrails.py")


if __name__ == "__main__":
    main()
