"""
LLM API å‚æ•°è¯¦è§£ (Gemini ç‰ˆæœ¬)
==============================

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£ temperature å‚æ•°å¯¹è¾“å‡ºçš„å½±å“
    2. æŒæ¡ top_p æ ¸é‡‡æ ·çš„åŸç†
    3. å­¦ä¼šä½¿ç”¨ max_output_tokens æ§åˆ¶è¾“å‡ºé•¿åº¦
    4. äº†è§£ Gemini ç‰¹æœ‰çš„å‚æ•°è®¾ç½®

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Temperatureï¼šæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§/åˆ›é€ æ€§
    - Top_pï¼šæ ¸é‡‡æ ·ï¼Œå¦ä¸€ç§æ§åˆ¶éšæœºæ€§çš„æ–¹å¼
    - Max_output_tokensï¼šé™åˆ¶è¾“å‡ºçš„æœ€å¤§ token æ•°
    - Top_kï¼šGemini ç‰¹æœ‰ï¼Œé™åˆ¶å€™é€‰è¯æ•°é‡

å‰ç½®çŸ¥è¯†ï¼š
    - å®Œæˆ 01-openai-api-basics.py

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install google-generativeai python-dotenv
"""

import os
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šTemperature å‚æ•° ====================


def explore_temperature():
    """æ¢ç´¢ temperature å‚æ•°çš„å½±å“"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šTemperature å‚æ•°")
    print("=" * 60)

    import google.generativeai as genai

    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    print("""
Temperature å‚æ•°è¯´æ˜ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å€¼             â”‚ æ•ˆæœ                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0.0            â”‚ ç¡®å®šæ€§è¾“å‡ºï¼Œæ¯æ¬¡ç»“æœç›¸åŒ             â”‚
â”‚ 0.5 - 0.7      â”‚ å¹³è¡¡åˆ›é€ æ€§å’Œä¸€è‡´æ€§ï¼ˆæ¨èé»˜è®¤å€¼ï¼‰     â”‚
â”‚ 1.0            â”‚ æ›´å…·åˆ›é€ æ€§ï¼Œè¾“å‡ºå¤šæ ·                 â”‚
â”‚ 1.5 - 2.0      â”‚ é«˜åº¦éšæœºï¼Œå¯èƒ½ä¸è¿è´¯                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    prompt = "ç”¨ä¸€ä¸ªè¯æè¿°å¤ªé˜³"
    temperatures = [0.0, 0.5, 1.0, 1.5]

    print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: '{prompt}'")
    print("-" * 40)

    for temp in temperatures:
        print(f"\nğŸŒ¡ï¸ Temperature = {temp}")

        model = genai.GenerativeModel("gemini-2.0-flash")

        # åŒä¸€ä¸ª temperature è°ƒç”¨ 3 æ¬¡ï¼Œè§‚å¯Ÿä¸€è‡´æ€§
        results = []
        for i in range(3):
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=temp,
                    max_output_tokens=20,
                ),
            )
            results.append(response.text.strip())

        for i, result in enumerate(results, 1):
            print(f"   ç¬¬{i}æ¬¡: {result}")

    print("\nğŸ’¡ è§‚å¯Ÿï¼štemperature=0 æ—¶ï¼Œè¾“å‡ºå®Œå…¨ä¸€è‡´ï¼›å€¼è¶Šé«˜ï¼Œå˜åŒ–è¶Šå¤§")


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šTop_p æ ¸é‡‡æ · ====================


def explore_top_p():
    """æ¢ç´¢ top_p å‚æ•°"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šTop_p æ ¸é‡‡æ ·")
    print("=" * 60)

    import google.generativeai as genai

    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    print("""
Top_p å‚æ•°è¯´æ˜ï¼š
- ä¹Ÿç§°ä¸º"æ ¸é‡‡æ ·"(nucleus sampling)
- æ§åˆ¶ä»æ¦‚ç‡æœ€é«˜çš„ tokens ä¸­é‡‡æ ·
- top_p=0.1: åªä»ç´¯è®¡æ¦‚ç‡å‰ 10% çš„ tokens é‡‡æ ·
- top_p=1.0: è€ƒè™‘æ‰€æœ‰ tokens

âš ï¸ æ³¨æ„ï¼šä¸€èˆ¬åªè°ƒæ•´ temperature æˆ– top_p å…¶ä¸­ä¹‹ä¸€
    """)

    prompt = "å†™ä¸€ä¸ªå…³äºæœˆäº®çš„çŸ­å¥"
    top_p_values = [0.1, 0.5, 0.9]

    print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: '{prompt}'")
    print("-" * 40)

    model = genai.GenerativeModel("gemini-2.0-flash")

    for top_p in top_p_values:
        print(f"\nğŸ¯ Top_p = {top_p}")

        for i in range(2):
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=1.0,  # å›ºå®š temperature
                    top_p=top_p,
                    max_output_tokens=30,
                ),
            )
            print(f"   ç¬¬{i + 1}æ¬¡: {response.text.strip()}")

    print("\nğŸ’¡ top_p è¶Šå°ï¼Œè¾“å‡ºè¶Šä¿å®ˆï¼›è¶Šå¤§ï¼Œè¶Šå¤šæ ·")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šMax_output_tokens å‚æ•° ====================


def explore_max_tokens():
    """æ¢ç´¢ max_output_tokens å‚æ•°"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šMax_output_tokens å‚æ•°")
    print("=" * 60)

    import google.generativeai as genai

    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    print("""
Max_output_tokens å‚æ•°è¯´æ˜ï¼š
- é™åˆ¶ AI å›å¤çš„æœ€å¤§ token æ•°
- å¦‚æœå›å¤è¢«æˆªæ–­ï¼Œfinish_reason ä¼šæ˜¯ 'MAX_TOKENS'
- 1 ä¸ªä¸­æ–‡å­—ç¬¦çº¦ 1-2 tokens
- 1 ä¸ªè‹±æ–‡å•è¯çº¦ 1-2 tokens
    """)

    prompt = "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼ŒåŒ…æ‹¬å…¶å†å²ã€åº”ç”¨å’Œæœªæ¥å‘å±•ã€‚"
    max_tokens_values = [20, 50, 200]

    print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: '{prompt}'")
    print("-" * 40)

    model = genai.GenerativeModel("gemini-2.0-flash")

    for max_tokens in max_tokens_values:
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
            ),
        )

        content = response.text
        finish_reason = response.candidates[0].finish_reason.name

        print(f"\nğŸ“ Max_output_tokens = {max_tokens}")
        print(f"   å›å¤: {content[:100]}{'...' if len(content) > 100 else ''}")
        if hasattr(response, "usage_metadata"):
            print(f"   å®é™… tokens: {response.usage_metadata.candidates_token_count}")
        print(f"   ç»“æŸåŸå› : {finish_reason}")

        if finish_reason == "MAX_TOKENS":
            print("   âš ï¸ å›å¤è¢«æˆªæ–­!")


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šTop_k å‚æ•° (Gemini ç‰¹æœ‰) ====================


def explore_top_k():
    """æ¢ç´¢ top_k å‚æ•° (Gemini ç‰¹æœ‰)"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šTop_k å‚æ•° (Gemini ç‰¹æœ‰)")
    print("=" * 60)

    import google.generativeai as genai

    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    print("""
Top_k å‚æ•°è¯´æ˜ï¼š
- Gemini ç‰¹æœ‰çš„é‡‡æ ·å‚æ•°
- åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª tokens ä¸­é‡‡æ ·
- top_k=1: ç›¸å½“äºè´ªå©ªé‡‡æ · (åªé€‰æœ€é«˜æ¦‚ç‡)
- top_k=40: ä»å‰ 40 ä¸ªå€™é€‰ä¸­é‡‡æ ·

ä¸ top_p çš„åŒºåˆ«ï¼š
- top_k æ˜¯å›ºå®šæ•°é‡
- top_p æ˜¯ç´¯è®¡æ¦‚ç‡
    """)

    prompt = "ç»™è¿™åªçŒ«èµ·ä¸€ä¸ªæœ‰è¶£çš„åå­—"
    top_k_values = [1, 10, 40]

    print(f"ğŸ“ æµ‹è¯•æç¤ºè¯: '{prompt}'")
    print("-" * 40)

    model = genai.GenerativeModel("gemini-2.0-flash")

    for top_k in top_k_values:
        print(f"\nğŸ”¢ Top_k = {top_k}")

        for i in range(2):
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=1.0,
                    top_k=top_k,
                    max_output_tokens=20,
                ),
            )
            print(f"   ç¬¬{i + 1}æ¬¡: {response.text.strip()}")


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šå‚æ•°ç»„åˆå»ºè®® ====================


def parameter_recommendations():
    """ä¸åŒåœºæ™¯ä¸‹çš„å‚æ•°ç»„åˆå»ºè®®"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šå‚æ•°ç»„åˆå»ºè®®")
    print("=" * 60)

    print("""
ğŸ“‹ ä¸åŒåœºæ™¯æ¨èå‚æ•° (Gemini)ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åœºæ™¯             â”‚ temperature â”‚ top_p  â”‚ top_k  â”‚ è¯´æ˜             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä»£ç ç”Ÿæˆ         â”‚ 0.0 - 0.2   â”‚ 1.0    â”‚ 40     â”‚ éœ€è¦ç²¾ç¡®æ€§       â”‚
â”‚ æ•°æ®æå–/è§£æ    â”‚ 0.0         â”‚ 1.0    â”‚ 1      â”‚ éœ€è¦ç¡®å®šæ€§ç»“æœ   â”‚
â”‚ ç¿»è¯‘             â”‚ 0.2 - 0.4   â”‚ 1.0    â”‚ 40     â”‚ ä¿æŒå‡†ç¡®ä½†æœ‰çµæ´» â”‚
â”‚ å®¢æœå¯¹è¯         â”‚ 0.5 - 0.7   â”‚ 1.0    â”‚ 40     â”‚ è‡ªç„¶ä½†å¯æ§       â”‚
â”‚ åˆ›æ„å†™ä½œ         â”‚ 0.8 - 1.2   â”‚ 0.9    â”‚ 100    â”‚ éœ€è¦å¤šæ ·æ€§       â”‚
â”‚ å¤´è„‘é£æš´         â”‚ 1.0 - 1.5   â”‚ 0.8    â”‚ 100    â”‚ æœ€å¤§åˆ›é€ æ€§       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ å°è´´å£«ï¼š
1. å…ˆä»é»˜è®¤å€¼å¼€å§‹ï¼ˆtemperature=1.0ï¼‰
2. æ ¹æ®éœ€è¦è°ƒæ•´ä¸€ä¸ªå‚æ•°ï¼Œä¸è¦åŒæ—¶è°ƒæ•´å¤šä¸ª
3. ä»£ç å’Œæ•°æ®ä»»åŠ¡ç”¨ä½ temperature
4. åˆ›æ„ä»»åŠ¡å¯ä»¥æé«˜ temperature å’Œ top_k
5. ä½¿ç”¨ max_output_tokens é˜²æ­¢è¾“å‡ºè¿‡é•¿

ğŸ“ OpenAI vs Gemini å‚æ•°å¯¹æ¯”ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI             â”‚ Gemini                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ temperature        â”‚ temperature            â”‚
â”‚ top_p              â”‚ top_p                  â”‚
â”‚ max_tokens         â”‚ max_output_tokens      â”‚
â”‚ frequency_penalty  â”‚ (æ— ç›´æ¥å¯¹åº”)           â”‚
â”‚ presence_penalty   â”‚ (æ— ç›´æ¥å¯¹åº”)           â”‚
â”‚ (æ— )               â”‚ top_k                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    exercises_text = """
ç»ƒä¹  1ï¼šå‚æ•°å¯¹æ¯”å®éªŒ
    é€‰æ‹©ä¸€ä¸ªä»»åŠ¡ï¼ˆå¦‚å†™è¯—ã€å†™ä»£ç ã€å›ç­”é—®é¢˜ï¼‰ï¼Œ
    åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„ temperature å€¼ï¼ˆ0, 0.5, 1.0, 1.5ï¼‰ï¼Œ
    è®°å½•è¾“å‡ºè´¨é‡å’Œä¸€è‡´æ€§çš„å˜åŒ–ã€‚

ç»ƒä¹  2ï¼šæ‰¾åˆ°æœ€ä½³å‚æ•°
    å‡è®¾ä½ è¦æ„å»ºä¸€ä¸ªä»£ç ç”ŸæˆåŠ©æ‰‹ï¼Œ
    å®éªŒæ‰¾å‡ºæœ€é€‚åˆç”Ÿæˆ Python ä»£ç çš„å‚æ•°ç»„åˆã€‚

ç»ƒä¹  3ï¼šTop_k vs Top_p
    ä½¿ç”¨ç›¸åŒçš„æç¤ºè¯ï¼Œå¯¹æ¯” top_k å’Œ top_p çš„æ•ˆæœå·®å¼‚ã€‚
    æ€è€ƒï¼šä»€ä¹ˆåœºæ™¯ä¸‹ç”¨ top_k æ›´å¥½ï¼Ÿ

æ€è€ƒé¢˜ï¼š
    1. ä¸ºä»€ä¹ˆ temperature=0 æ—¶ä»ç„¶å«"é‡‡æ ·"ï¼Ÿ
    2. top_k å’Œ top_p å¯ä»¥åŒæ—¶ä½¿ç”¨å—ï¼Ÿæ•ˆæœå¦‚ä½•ï¼Ÿ
    3. Gemini æ²¡æœ‰ penalty å‚æ•°ï¼Œå¦‚ä½•é¿å…è¾“å‡ºé‡å¤ï¼Ÿ
    """
    print(exercises_text)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LLM API å‚æ•°è¯¦è§£ (Gemini ç‰ˆæœ¬)")
    print("=" * 60)
    print("âš ï¸ æ³¨æ„ï¼šæœ¬è¯¾ç¨‹å°†å¤šæ¬¡è°ƒç”¨ APIï¼Œé¢„ä¼°æ¶ˆè€—çº¦ 2000-3000 tokens")
    print("=" * 60)

    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ è¯·å…ˆé…ç½® GOOGLE_API_KEY ç¯å¢ƒå˜é‡")
        return

    try:
        explore_temperature()
        explore_top_p()
        explore_max_tokens()
        explore_top_k()
        parameter_recommendations()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼š03-streaming-responses.pyï¼ˆæµå¼å“åº”ï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    main()
