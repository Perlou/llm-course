"""
æµå¼å“åº”å¤„ç†
============

å­¦ä¹ ç›®æ ‡ï¼š
    1. ç†è§£æµå¼å“åº”çš„ä¼˜åŠ¿
    2. æŒæ¡ OpenAI æµå¼ API çš„ä½¿ç”¨æ–¹æ³•
    3. å­¦ä¼šå¤„ç†æµå¼å“åº”çš„æ•°æ®ç»“æ„
    4. å®ç°æµå¼è¾“å‡ºçš„ç”¨æˆ·ç•Œé¢

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Streamingï¼šé€å—è¿”å›å“åº”ï¼Œè€Œéç­‰å¾…å®Œæ•´ç»“æœ
    - Deltaï¼šæ¯ä¸ªæµå¼å—ä¸­çš„å¢é‡å†…å®¹
    - é¦–å­—å»¶è¿Ÿï¼ˆTTFTï¼‰ï¼šTime To First Token

å‰ç½®çŸ¥è¯†ï¼š
    - å®Œæˆ 01-openai-api-basics.py
    - å®Œæˆ 02-openai-parameters.py

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install openai python-dotenv
"""

import os
import time
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæµå¼ vs éæµå¼å¯¹æ¯” ====================


def compare_streaming_modes():
    """å¯¹æ¯”æµå¼å’Œéæµå¼å“åº”"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæµå¼ vs éæµå¼å¯¹æ¯”")
    print("=" * 60)

    client = OpenAI()
    prompt = "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„å››å¥è¯—"

    print("""
æµå¼å“åº”çš„ä¼˜åŠ¿ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç‰¹ç‚¹            â”‚ è¯´æ˜                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ›´å¿«çš„é¦–å­—å“åº”  â”‚ ç”¨æˆ·ç«‹å³çœ‹åˆ°è¾“å‡ºå¼€å§‹                â”‚
â”‚ æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ  â”‚ é€å­—è¾“å‡ºæ›´è‡ªç„¶ï¼Œåƒæ‰“å­—æœºæ•ˆæœ        â”‚
â”‚ èŠ‚çœç­‰å¾…æ—¶é—´    â”‚ æ— éœ€ç­‰å¾…å®Œæ•´å“åº”                    â”‚
â”‚ å¯æå‰ç»ˆæ­¢      â”‚ å¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å–æ¶ˆ                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    # éæµå¼è°ƒç”¨
    print("ğŸ“Œ éæµå¼è°ƒç”¨ï¼ˆéœ€è¦ç­‰å¾…å®Œæ•´å“åº”ï¼‰ï¼š")
    print("-" * 40)

    start_time = time.time()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        stream=False,
    )
    end_time = time.time()

    print(f"å›å¤: {response.choices[0].message.content}")
    print(f"â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

    # æµå¼è°ƒç”¨
    print("\nğŸ“Œ æµå¼è°ƒç”¨ï¼ˆé€å­—è¾“å‡ºï¼‰ï¼š")
    print("-" * 40)

    start_time = time.time()
    first_token_time = None

    stream = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        stream=True,
    )

    print("å›å¤: ", end="", flush=True)
    for chunk in stream:
        if chunk.choices[0].delta.content:
            if first_token_time is None:
                first_token_time = time.time()
            print(chunk.choices[0].delta.content, end="", flush=True)

    print()  # æ¢è¡Œ
    end_time = time.time()

    print(f"â±ï¸ é¦–å­—å»¶è¿Ÿ (TTFT): {first_token_time - start_time:.2f} ç§’")
    print(f"â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print("\nğŸ’¡ æ³¨æ„ï¼šæµå¼æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·ç¬¬ä¸€æ—¶é—´å°±èƒ½çœ‹åˆ°è¾“å‡ºå¼€å§‹ï¼")


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæµå¼å“åº”æ•°æ®ç»“æ„ ====================


def examine_stream_structure():
    """è¯¦ç»†æŸ¥çœ‹æµå¼å“åº”çš„æ•°æ®ç»“æ„"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæµå¼å“åº”æ•°æ®ç»“æ„")
    print("=" * 60)

    client = OpenAI()

    print("""
æµå¼å“åº”çš„ chunk ç»“æ„ï¼š
- æ¯ä¸ª chunk åªåŒ…å«å¢é‡å†…å®¹ (delta)
- ç¬¬ä¸€ä¸ª chunk åŒ…å« role ä¿¡æ¯
- æœ€åä¸€ä¸ª chunk çš„ finish_reason ä¸ä¸º None
    """)

    stream = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "è¯´3ä¸ªæ•°å­—"}],
        stream=True,
    )

    print("ğŸ“¦ å„ä¸ª chunk çš„å†…å®¹ï¼š")
    print("-" * 40)

    chunk_count = 0
    for chunk in stream:
        chunk_count += 1
        delta = chunk.choices[0].delta
        finish_reason = chunk.choices[0].finish_reason

        # æ ¼å¼åŒ–è¾“å‡º
        content_str = repr(delta.content) if delta.content else "None"
        role_str = delta.role if delta.role else "None"

        print(
            f"Chunk {chunk_count:2d}: role={role_str:10s} content={content_str:10s} finish_reason={finish_reason}"
        )

    print(f"\nğŸ“Š å…±æ”¶åˆ° {chunk_count} ä¸ª chunks")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´çš„æµå¼å¤„ç†å‡½æ•° ====================


def stream_with_full_handling():
    """å¸¦å®Œæ•´å¤„ç†çš„æµå¼å‡½æ•°"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´çš„æµå¼å¤„ç†å‡½æ•°")
    print("=" * 60)

    client = OpenAI()

    print("ä¸‹é¢æ˜¯ä¸€ä¸ªç”Ÿäº§çº§çš„æµå¼å¤„ç†å‡½æ•°ç¤ºä¾‹ï¼š\n")

    def stream_chat(messages, on_token=None, on_complete=None):
        """
        æµå¼èŠå¤©å‡½æ•°

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            on_token: æ¯æ”¶åˆ°ä¸€ä¸ª token æ—¶çš„å›è°ƒå‡½æ•°
            on_complete: å®Œæˆæ—¶çš„å›è°ƒå‡½æ•°

        Returns:
            å®Œæ•´çš„å›å¤å†…å®¹
        """
        stream = client.chat.completions.create(
            model="gpt-3.5-turbo", messages=messages, stream=True
        )

        collected_content = []

        for chunk in stream:
            delta = chunk.choices[0].delta
            finish_reason = chunk.choices[0].finish_reason

            if delta.content:
                collected_content.append(delta.content)
                if on_token:
                    on_token(delta.content)

            if finish_reason == "stop":
                full_content = "".join(collected_content)
                if on_complete:
                    on_complete(full_content)
                return full_content

        return "".join(collected_content)

    # ä½¿ç”¨ç¤ºä¾‹
    print("ğŸ“ ä½¿ç”¨ç¤ºä¾‹ï¼š")
    print("-" * 40)

    def print_token(token):
        print(token, end="", flush=True)

    def on_done(full_text):
        print(f"\n\nâœ… å®Œæˆï¼å…± {len(full_text)} ä¸ªå­—ç¬¦")

    print("å›å¤: ", end="")
    result = stream_chat(
        messages=[{"role": "user", "content": "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}],
        on_token=print_token,
        on_complete=on_done,
    )


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šå¼‚æ­¥æµå¼å¤„ç† ====================


def async_streaming_intro():
    """å¼‚æ­¥æµå¼å¤„ç†ä»‹ç»ï¼ˆä»…å±•ç¤ºæ¦‚å¿µï¼‰"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šå¼‚æ­¥æµå¼å¤„ç†ï¼ˆæ¦‚å¿µä»‹ç»ï¼‰")
    print("=" * 60)

    print("""
åœ¨ Web åº”ç”¨ä¸­ï¼Œé€šå¸¸ä½¿ç”¨å¼‚æ­¥æµå¼å¤„ç†ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # å¼‚æ­¥æµå¼ç¤ºä¾‹ (FastAPI + SSE)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ from fastapi import FastAPI                                 â”‚
â”‚ from fastapi.responses import StreamingResponse             â”‚
â”‚ from openai import AsyncOpenAI                              â”‚
â”‚                                                             â”‚
â”‚ app = FastAPI()                                             â”‚
â”‚ client = AsyncOpenAI()                                      â”‚
â”‚                                                             â”‚
â”‚ @app.post("/chat/stream")                                   â”‚
â”‚ async def chat_stream(message: str):                        â”‚
â”‚     async def generate():                                   â”‚
â”‚         stream = await client.chat.completions.create(      â”‚
â”‚             model="gpt-3.5-turbo",                          â”‚
â”‚             messages=[{"role": "user", "content": message}],â”‚
â”‚             stream=True                                     â”‚
â”‚         )                                                   â”‚
â”‚         async for chunk in stream:                          â”‚
â”‚             if chunk.choices[0].delta.content:              â”‚
â”‚                 yield chunk.choices[0].delta.content        â”‚
â”‚                                                             â”‚
â”‚     return StreamingResponse(generate())                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¡ å…³é”®ç‚¹ï¼š
1. ä½¿ç”¨ AsyncOpenAI å®¢æˆ·ç«¯
2. ä½¿ç”¨ async for éå†æµ
3. ä½¿ç”¨ yield ç”Ÿæˆ SSE (Server-Sent Events)
4. å‰ç«¯ä½¿ç”¨ EventSource æˆ– fetch æ¥æ”¶æµ
    """)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ ====================


def typewriter_effect():
    """å®ç°æ‰“å­—æœºæ•ˆæœ"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ")
    print("=" * 60)

    client = OpenAI()

    print("ğŸ“ æ‰“å­—æœºæ•ˆæœæ¼”ç¤ºï¼š")
    print("-" * 40)
    print()

    stream = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªè®²æ•…äº‹çš„äººï¼Œç”¨ç®€çŸ­æœ‰è¶£çš„æ–¹å¼è®²æ•…äº‹ã€‚",
            },
            {"role": "user", "content": "è®²ä¸€ä¸ªå…³äºä¸€åªå‹‡æ•¢çš„å°çŒ«çš„50å­—å°æ•…äº‹"},
        ],
        stream=True,
    )

    # æ·»åŠ å°‘é‡å»¶è¿Ÿå¢å¼ºæ‰“å­—æœºæ•ˆæœ
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            for char in content:
                print(char, end="", flush=True)
                time.sleep(0.02)  # æ¯ä¸ªå­—ç¬¦å»¶è¿Ÿ 20ms

    print("\n")


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    exercises_text = """
ç»ƒä¹  1ï¼šæµ‹é‡é¦–å­—å»¶è¿Ÿ
    ä¿®æ”¹ compare_streaming_modes() å‡½æ•°ï¼Œ
    è®°å½•å¹¶å¯¹æ¯”ä¸åŒæç¤ºè¯é•¿åº¦ä¸‹çš„é¦–å­—å»¶è¿Ÿ (TTFT)ã€‚

ç»ƒä¹  2ï¼šå®ç°æµå¼èŠå¤©æœºå™¨äºº
    åŸºäº stream_with_full_handling() ä¸­çš„ stream_chat å‡½æ•°ï¼Œ
    å®ç°ä¸€ä¸ªäº¤äº’å¼çš„å‘½ä»¤è¡ŒèŠå¤©æœºå™¨äººã€‚

ç»ƒä¹  3ï¼šç»Ÿè®¡æµå¼ Token
    åœ¨æµå¼è¾“å‡ºæ—¶ï¼Œå®æ—¶æ˜¾ç¤ºå·²ç”Ÿæˆçš„ token æ•°é‡ã€‚
    æç¤ºï¼šå¯ä»¥ç®€å•åœ°ç»Ÿè®¡æ”¶åˆ°çš„ chunk æ•°ã€‚

ç»ƒä¹  4ï¼ˆè¿›é˜¶ï¼‰ï¼šå®ç°ä¸­æ–­åŠŸèƒ½
    å®ç°ä¸€ä¸ªå¯ä»¥é€šè¿‡é”®ç›˜ä¸­æ–­ï¼ˆCtrl+Cï¼‰æ¥åœæ­¢ç”Ÿæˆçš„æµå¼å¯¹è¯ã€‚

æ€è€ƒé¢˜ï¼š
    1. æµå¼è¾“å‡ºæ—¶ï¼ŒAPI æ˜¯å¦‚ä½•çŸ¥é“ä½•æ—¶ç»“æŸçš„ï¼Ÿ
    2. æµå¼æ¨¡å¼æ˜¯å¦ä¼šå½±å“ token è®¡è´¹ï¼Ÿ
    3. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ï¼Œéæµå¼å¯èƒ½æ¯”æµå¼æ›´åˆé€‚ï¼Ÿ
    """
    print(exercises_text)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµå¼å“åº”å¤„ç†")
    print("=" * 60)
    print("âš ï¸ æ³¨æ„ï¼šæœ¬è¯¾ç¨‹ä¼šå¤šæ¬¡è°ƒç”¨ APIï¼Œé¢„ä¼°æ¶ˆè€—çº¦ 1000-2000 tokens")
    print("=" * 60)

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ è¯·å…ˆé…ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return

    try:
        compare_streaming_modes()
        examine_stream_structure()
        stream_with_full_handling()
        async_streaming_intro()
        typewriter_effect()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼š04-anthropic-claude.pyï¼ˆClaude API ä½¿ç”¨ï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    main()
