"""
æœ¬åœ° LLM éƒ¨ç½²ï¼šOllama ä½¿ç”¨
==========================

å­¦ä¹ ç›®æ ‡ï¼š
    1. äº†è§£æœ¬åœ° LLM çš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯
    2. æŒæ¡ Ollama çš„å®‰è£…å’Œä½¿ç”¨
    3. å­¦ä¼šä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨æœ¬åœ°æ¨¡å‹
    4. äº†è§£å¸¸ç”¨å¼€æºæ¨¡å‹

æ ¸å¿ƒæ¦‚å¿µï¼š
    - Ollamaï¼šç®€å•æ˜“ç”¨çš„æœ¬åœ° LLM è¿è¡Œå·¥å…·
    - å¼€æºæ¨¡å‹ï¼šLlamaã€Qwenã€Mistral ç­‰
    - OpenAI å…¼å®¹ APIï¼šä½¿ç”¨ç›¸åŒä»£ç è°ƒç”¨ä¸åŒæ¨¡å‹

å‰ç½®çŸ¥è¯†ï¼š
    - å®Œæˆå‰é¢çš„ API è¯¾ç¨‹
    - äº†è§£åŸºæœ¬çš„å‘½ä»¤è¡Œæ“ä½œ

ç¯å¢ƒè¦æ±‚ï¼š
    - å®‰è£… Ollama (https://ollama.ai)
    - pip install openaiï¼ˆç”¨äºå…¼å®¹æ¥å£ï¼‰
"""

import os
import subprocess
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ° LLM ====================


def why_local_llm():
    """ä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ° LLM"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ° LLM")
    print("=" * 60)

    print("""
ä½¿ç”¨æœ¬åœ° LLM çš„ç†ç”±ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ä¼˜åŠ¿            â”‚ è¯´æ˜                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”’ æ•°æ®éšç§     â”‚ æ•°æ®ä¸ç¦»å¼€æœ¬åœ°ï¼Œé€‚åˆæ•æ„Ÿä¿¡æ¯             â”‚
â”‚ ğŸ’° é›¶ API æˆæœ¬  â”‚ åªéœ€è¦ç¡¬ä»¶æˆæœ¬ï¼Œæ— è°ƒç”¨è´¹ç”¨               â”‚
â”‚ ğŸŒ ç¦»çº¿ä½¿ç”¨     â”‚ æ— éœ€ç½‘ç»œè¿æ¥                             â”‚
â”‚ âš¡ ä½å»¶è¿Ÿ       â”‚ æ— ç½‘ç»œè¯·æ±‚å»¶è¿Ÿ                           â”‚
â”‚ ğŸ›ï¸ å®Œå…¨æ§åˆ¶    â”‚ å¯ä»¥ä¿®æ”¹ã€å¾®è°ƒæ¨¡å‹                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æœ¬åœ° LLM çš„å±€é™ï¼š
- éœ€è¦è¾ƒå¼ºçš„ GPUï¼ˆè‡³å°‘ 8GB æ˜¾å­˜è¿è¡Œ 7B æ¨¡å‹ï¼‰
- èƒ½åŠ›ä¸å¦‚æœ€å¼ºé—­æºæ¨¡å‹ï¼ˆä½†å·®è·åœ¨ç¼©å°ï¼‰
- éœ€è¦è‡ªå·±ç®¡ç†æ¨¡å‹æ–‡ä»¶
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šOllama å®‰è£…ä¸ä½¿ç”¨ ====================


def ollama_basics():
    """Ollama åŸºç¡€ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šOllama å®‰è£…ä¸ä½¿ç”¨")
    print("=" * 60)

    print("""
Ollama å®‰è£…ï¼š

macOS / Linux:
    curl -fsSL https://ollama.ai/install.sh | sh

Windows:
    ä» https://ollama.ai ä¸‹è½½å®‰è£…åŒ…

åŸºæœ¬å‘½ä»¤ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‘½ä»¤                       â”‚ è¯´æ˜                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ollama run llama3.1        â”‚ ä¸‹è½½å¹¶è¿è¡Œ Llama 3.1 8B        â”‚
â”‚ ollama run qwen2.5         â”‚ ä¸‹è½½å¹¶è¿è¡Œ Qwen 2.5 7B         â”‚
â”‚ ollama list                â”‚ åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹               â”‚
â”‚ ollama pull llama3.1       â”‚ ä»…ä¸‹è½½æ¨¡å‹ï¼Œä¸è¿è¡Œ             â”‚
â”‚ ollama rm llama3.1         â”‚ åˆ é™¤æ¨¡å‹                       â”‚
â”‚ ollama serve               â”‚ å¯åŠ¨ API æœåŠ¡                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    # æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…
    try:
        result = subprocess.run(
            ["ollama", "--version"], capture_output=True, text=True, timeout=5
        )
        if result.returncode == 0:
            print(f"âœ… Ollama å·²å®‰è£…: {result.stdout.strip()}")

            # åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹
            result = subprocess.run(
                ["ollama", "list"], capture_output=True, text=True, timeout=5
            )
            if result.stdout.strip():
                print(f"\nğŸ“¦ å·²ä¸‹è½½çš„æ¨¡å‹:\n{result.stdout}")
            else:
                print("\nğŸ“¦ å°šæœªä¸‹è½½ä»»ä½•æ¨¡å‹")
                print("   è¿è¡Œ 'ollama pull llama3.1' ä¸‹è½½æ¨¡å‹")
        else:
            print("âš ï¸ Ollama æœªæ­£ç¡®å®‰è£…")
    except FileNotFoundError:
        print("âš ï¸ Ollama æœªå®‰è£…")
        print("   è¯·è®¿é—® https://ollama.ai ä¸‹è½½å®‰è£…")
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥ Ollama æ—¶å‡ºé”™: {e}")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¸¸ç”¨å¼€æºæ¨¡å‹ ====================


def popular_models():
    """å¸¸ç”¨å¼€æºæ¨¡å‹ä»‹ç»"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¸¸ç”¨å¼€æºæ¨¡å‹")
    print("=" * 60)

    print("""
æ¨èæ¨¡å‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¨¡å‹             â”‚ å‚æ•°é‡  â”‚ æ˜¾å­˜éœ€æ±‚   â”‚ ç‰¹ç‚¹                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llama3.1:8b      â”‚ 8B      â”‚ ~8GB       â”‚ Meta å®˜æ–¹ï¼Œç»¼åˆèƒ½åŠ›å¼º  â”‚
â”‚ llama3.1:70b     â”‚ 70B     â”‚ ~48GB      â”‚ æ¥è¿‘ GPT-4 æ°´å¹³        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ qwen2.5:7b       â”‚ 7B      â”‚ ~8GB       â”‚ ä¸­æ–‡èƒ½åŠ›æœ€å¼º           â”‚
â”‚ qwen2.5:14b      â”‚ 14B     â”‚ ~16GB      â”‚ ä¸­æ–‡æ›´å¼ºï¼Œä»£ç èƒ½åŠ›å¥½   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ mistral          â”‚ 7B      â”‚ ~8GB       â”‚ é€Ÿåº¦å¿«ï¼Œæ¬§æ´²å¼€å‘       â”‚
â”‚ mixtral          â”‚ 8x7B    â”‚ ~48GB      â”‚ MoE æ¶æ„ï¼Œæ•ˆæœå¥½       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ codellama        â”‚ 7B/34B  â”‚ 8GB/~24GB  â”‚ ä»£ç ä¸“ç”¨               â”‚
â”‚ deepseek-coder   â”‚ 6.7B    â”‚ ~8GB       â”‚ ä»£ç èƒ½åŠ›å¼º             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ phi3             â”‚ 3.8B    â”‚ ~4GB       â”‚ å¾®è½¯ï¼Œå°è€Œç²¾           â”‚
â”‚ gemma2           â”‚ 2B/9B   â”‚ ~4GB/~12GB â”‚ Google å¼€æº            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‰æ‹©å»ºè®®ï¼š
- 8GB æ˜¾å­˜ â†’ llama3.1:8b æˆ– qwen2.5:7b
- 16GB æ˜¾å­˜ â†’ qwen2.5:14b
- ä¸­æ–‡ä¸ºä¸» â†’ qwen2.5 ç³»åˆ—
- ä»£ç ä¸ºä¸» â†’ codellama æˆ– deepseek-coder
- ä½é…è®¾å¤‡ â†’ phi3 æˆ– gemma2:2b
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ ====================


def openai_compatible_api():
    """ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨ Ollama"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šOpenAI å…¼å®¹æ¥å£")
    print("=" * 60)

    print("""
ğŸ’¡ Ollama æä¾› OpenAI å…¼å®¹çš„ API æ¥å£
   åªéœ€ä¿®æ”¹ base_urlï¼Œå³å¯å¤ç”¨ OpenAI çš„ä»£ç ï¼

æ¥å£åœ°å€ï¼šhttp://localhost:11434/v1
    """)

    # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
    try:
        from openai import OpenAI

        client = OpenAI(
            base_url="http://localhost:11434/v1",
            api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå®çš„ API key
        )

        print("ğŸ“¤ å°è¯•è°ƒç”¨æœ¬åœ°æ¨¡å‹...")

        # å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
        try:
            response = client.chat.completions.create(
                model="qwen2.5:7b",  # æˆ–å…¶ä»–å·²ä¸‹è½½çš„æ¨¡å‹
                messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"}],
                max_tokens=100,
            )

            print(f"\nğŸ“¥ æ”¶åˆ°å›å¤:")
            print(f"   {response.choices[0].message.content}")
            print("\nâœ… æœ¬åœ°æ¨¡å‹è°ƒç”¨æˆåŠŸï¼")

        except Exception as e:
            if "qwen2.5:7b" in str(e) or "model" in str(e).lower():
                print(f"âš ï¸ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ: ollama pull qwen2.5:7b")
            else:
                print(f"âš ï¸ è°ƒç”¨å¤±è´¥: {e}")
                print("   è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve")

    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Ollama: {e}")
        print("   è¯·ç¡®ä¿ Ollama å·²å®‰è£…å¹¶è¿è¡Œ")

    print("""
ä»£ç ç¤ºä¾‹ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from openai import OpenAI

# åªéœ€ä¿®æ”¹ base_url å³å¯ä½¿ç”¨æœ¬åœ°æ¨¡å‹
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)

response = client.chat.completions.create(
    model="llama3.1",  # ä½¿ç”¨ Ollama ä¸­çš„æ¨¡å‹å
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ]
)
print(response.choices[0].message.content)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹åˆ‡æ¢å°è£… ====================


def model_switching():
    """å°è£…æ¨¡å‹åˆ‡æ¢é€»è¾‘"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹åˆ‡æ¢å°è£…")
    print("=" * 60)

    print("""
ğŸ’¡ å®ç”¨æŠ€å·§ï¼šå°è£…ä¸€ä¸ªæ”¯æŒå¤šæ¨¡å‹çš„å®¢æˆ·ç«¯

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from openai import OpenAI
import os

def get_llm_client(provider="openai"):
    '''è·å– LLM å®¢æˆ·ç«¯'''
    
    configs = {
        "openai": {
            "base_url": "https://api.openai.com/v1",
            "api_key": os.getenv("OPENAI_API_KEY"),
            "model": "gpt-3.5-turbo"
        },
        "ollama": {
            "base_url": "http://localhost:11434/v1",
            "api_key": "ollama",
            "model": "llama3.1"
        },
        "deepseek": {
            "base_url": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
            "model": "deepseek-chat"
        }
    }
    
    config = configs.get(provider, configs["openai"])
    client = OpenAI(
        base_url=config["base_url"],
        api_key=config["api_key"]
    )
    return client, config["model"]

# ä½¿ç”¨ç¤ºä¾‹
client, model = get_llm_client("ollama")
response = client.chat.completions.create(
    model=model,
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

è¿™æ ·åªéœ€ä¿®æ”¹ provider å‚æ•°ï¼Œå°±èƒ½åˆ‡æ¢ä¸åŒçš„æ¨¡å‹ï¼
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    exercises_text = """
ç»ƒä¹  1ï¼šæ¨¡å‹å¯¹æ¯”
    ä¸‹è½½ llama3.1 å’Œ qwen2.5ï¼Œå¯¹åŒä¸€é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼Œ
    å¯¹æ¯”ä¸­è‹±æ–‡å›ç­”è´¨é‡ã€‚

ç»ƒä¹  2ï¼šæ€§èƒ½æµ‹è¯•
    æµ‹é‡æœ¬åœ°æ¨¡å‹çš„ï¼š
    - é¦–å­—å»¶è¿Ÿ (TTFT)
    - ç”Ÿæˆé€Ÿåº¦ (tokens/s)
    å¹¶ä¸ OpenAI API å¯¹æ¯”ã€‚

ç»ƒä¹  3ï¼šå°è£… LLM æœåŠ¡
    åŸºäºç¬¬äº”éƒ¨åˆ†çš„ä»£ç ï¼Œåˆ›å»ºä¸€ä¸ªå®Œæ•´çš„ LLMService ç±»ï¼Œ
    æ”¯æŒ OpenAIã€Claudeã€Gemini å’Œ Ollamaã€‚

ç»ƒä¹  4ï¼šé‡åŒ–æ¨¡å‹
    å°è¯•ä¸‹è½½é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹ï¼ˆå¦‚ llama3.1:8b-q4_0ï¼‰ï¼Œ
    å¯¹æ¯”ä¸åŸç‰ˆçš„å·®å¼‚ã€‚

æ€è€ƒé¢˜ï¼š
    1. ä¸ºä»€ä¹ˆ Ollama å¯ä»¥æä¾› OpenAI å…¼å®¹æ¥å£ï¼Ÿ
    2. æœ¬åœ°æ¨¡å‹å’Œäº‘ç«¯ API çš„å»¶è¿Ÿå·®å¼‚ä¸»è¦æ¥è‡ªå“ªé‡Œï¼Ÿ
    3. ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©æœ¬åœ°éƒ¨ç½²è€Œä¸æ˜¯ API è°ƒç”¨ï¼Ÿ
    """
    print(exercises_text)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æœ¬åœ° LLM éƒ¨ç½²ï¼šOllama ä½¿ç”¨")
    print("=" * 60)
    print("ğŸ’¡ æœ¬è¯¾ç¨‹ä»‹ç»å¦‚ä½•ä½¿ç”¨ Ollama è¿è¡Œæœ¬åœ°å¤§æ¨¡å‹")
    print("=" * 60)

    try:
        why_local_llm()
        ollama_basics()
        popular_models()
        openai_compatible_api()
        model_switching()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼š07-error-handling.pyï¼ˆé”™è¯¯å¤„ç†ä¸é‡è¯•ï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    main()
