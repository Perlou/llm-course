"""
é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶
==================

å­¦ä¹ ç›®æ ‡ï¼š
    1. äº†è§£ LLM API å¸¸è§é”™è¯¯ç±»å‹
    2. æŒæ¡é”™è¯¯å¤„ç†çš„æœ€ä½³å®è·µ
    3. å­¦ä¼šå®ç°æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
    4. ä½¿ç”¨ tenacity åº“è¿›è¡Œé‡è¯•

æ ¸å¿ƒæ¦‚å¿µï¼š
    - API é”™è¯¯ç±»å‹ï¼šè®¤è¯é”™è¯¯ã€é€Ÿç‡é™åˆ¶ã€æœåŠ¡å™¨é”™è¯¯ç­‰
    - æŒ‡æ•°é€€é¿ï¼šæ¯æ¬¡é‡è¯•ç­‰å¾…æ—¶é—´ç¿»å€
    - å¹‚ç­‰æ€§ï¼šç›¸åŒè¯·æ±‚å¯ä»¥å®‰å…¨é‡è¯•

å‰ç½®çŸ¥è¯†ï¼š
    - å®Œæˆå‰é¢çš„ API è¯¾ç¨‹

ç¯å¢ƒè¦æ±‚ï¼š
    - pip install openai tenacity python-dotenv
"""

import os
import time
import random
from dotenv import load_dotenv

load_dotenv()


# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå¸¸è§é”™è¯¯ç±»å‹ ====================


def error_types_introduction():
    """å¸¸è§ API é”™è¯¯ç±»å‹ä»‹ç»"""
    print("=" * 60)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå¸¸è§é”™è¯¯ç±»å‹")
    print("=" * 60)

    print("""
LLM API å¸¸è§é”™è¯¯ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é”™è¯¯ç±»å‹           â”‚ HTTP ç   â”‚ å¤„ç†æ–¹å¼                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AuthenticationErrorâ”‚ 401      â”‚ æ£€æŸ¥ API Key                   â”‚
â”‚ PermissionDenied   â”‚ 403      â”‚ æ£€æŸ¥è´¦æˆ·æƒé™                   â”‚
â”‚ RateLimitError     â”‚ 429      â”‚ ç­‰å¾…åé‡è¯•                     â”‚
â”‚ ServerError        â”‚ 500      â”‚ ç­‰å¾…åé‡è¯•                     â”‚
â”‚ ServiceUnavailable â”‚ 503      â”‚ ç­‰å¾…åé‡è¯•                     â”‚
â”‚ Timeout            â”‚ -        â”‚ å¢åŠ è¶…æ—¶æ—¶é—´æˆ–é‡è¯•             â”‚
â”‚ ConnectionError    â”‚ -        â”‚ æ£€æŸ¥ç½‘ç»œæˆ–é‡è¯•                 â”‚
â”‚ BadRequestError    â”‚ 400      â”‚ æ£€æŸ¥è¯·æ±‚å‚æ•°                   â”‚
â”‚ ContentFilterError â”‚ -        â”‚ ä¿®æ”¹è¾“å…¥å†…å®¹                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¯é‡è¯•çš„é”™è¯¯ï¼š
âœ… RateLimitError (429) - ç­‰å¾…åé‡è¯•
âœ… ServerError (500, 502, 503, 504) - ç¨åé‡è¯•
âœ… Timeout - å¯ä»¥é‡è¯•
âœ… ConnectionError - å¯ä»¥é‡è¯•

ä¸å¯é‡è¯•çš„é”™è¯¯ï¼š
âŒ AuthenticationError (401) - éœ€è¦ä¿®å¤ API Key
âŒ PermissionDenied (403) - éœ€è¦å‡çº§è´¦æˆ·
âŒ BadRequestError (400) - éœ€è¦ä¿®æ”¹è¯·æ±‚å‚æ•°
    """)


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€é”™è¯¯å¤„ç† ====================


def basic_error_handling():
    """åŸºç¡€é”™è¯¯å¤„ç†"""
    print("\n" + "=" * 60)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€é”™è¯¯å¤„ç†")
    print("=" * 60)

    from openai import (
        OpenAI,
        APIError,
        APIConnectionError,
        RateLimitError,
        AuthenticationError,
    )

    print("""
åŸºç¡€é”™è¯¯å¤„ç†æ¨¡æ¿ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from openai import (
    OpenAI,
    APIError,
    APIConnectionError,
    RateLimitError,
    AuthenticationError,
)

client = OpenAI()

try:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "ä½ å¥½"}]
    )
    print(response.choices[0].message.content)
    
except AuthenticationError:
    print("âŒ API Key æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®")
    
except RateLimitError:
    print("âš ï¸ è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
    
except APIConnectionError:
    print("âš ï¸ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
    
except APIError as e:
    print(f"âŒ API é”™è¯¯: {e}")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)

    # å®é™…æ¼”ç¤º
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡å®é™…æ¼”ç¤º")
        return

    client = OpenAI()

    print("\nğŸ“ å®é™…æµ‹è¯•ï¼ˆæ­£å¸¸è¯·æ±‚ï¼‰ï¼š")
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "è¯´ OK"}],
            max_tokens=10,
        )
        print(f"   âœ… æˆåŠŸ: {response.choices[0].message.content}")
    except Exception as e:
        print(f"   âŒ é”™è¯¯: {e}")


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰‹åŠ¨å®ç°é‡è¯• ====================


def manual_retry_implementation():
    """æ‰‹åŠ¨å®ç°é‡è¯•æœºåˆ¶"""
    print("\n" + "=" * 60)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰‹åŠ¨å®ç°é‡è¯•æœºåˆ¶")
    print("=" * 60)

    print("""
ğŸ’¡ æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥ï¼š
   æ¯æ¬¡å¤±è´¥åï¼Œç­‰å¾…æ—¶é—´ç¿»å€ï¼ŒåŠ ä¸ŠéšæœºæŠ–åŠ¨

ç­‰å¾…æ—¶é—´ = min(base_delay * 2^attempt + random_jitter, max_delay)

ç¤ºä¾‹ï¼š
   ç¬¬1æ¬¡å¤±è´¥ â†’ ç­‰å¾… 1-2 ç§’
   ç¬¬2æ¬¡å¤±è´¥ â†’ ç­‰å¾… 2-4 ç§’
   ç¬¬3æ¬¡å¤±è´¥ â†’ ç­‰å¾… 4-8 ç§’
   ç¬¬4æ¬¡å¤±è´¥ â†’ ç­‰å¾… 8-16 ç§’
    """)

    from openai import OpenAI, RateLimitError, APIError, APIConnectionError

    def call_with_retry(
        client,
        messages,
        model="gpt-3.5-turbo",
        max_retries=3,
        base_delay=1.0,
        max_delay=60.0,
    ):
        """å¸¦é‡è¯•æœºåˆ¶çš„ API è°ƒç”¨"""

        last_exception = None

        for attempt in range(max_retries + 1):
            try:
                response = client.chat.completions.create(
                    model=model, messages=messages
                )
                return response

            except (RateLimitError, APIConnectionError) as e:
                last_exception = e

                if attempt == max_retries:
                    print(f"   âŒ æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    raise

                # è®¡ç®—ç­‰å¾…æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰
                delay = min(base_delay * (2**attempt), max_delay)
                jitter = random.uniform(0, delay * 0.1)
                wait_time = delay + jitter

                print(f"   âš ï¸ ç¬¬ {attempt + 1} æ¬¡å¤±è´¥: {type(e).__name__}")
                print(f"   â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)

            except APIError as e:
                # æœåŠ¡å™¨é”™è¯¯å¯ä»¥é‡è¯•
                if hasattr(e, "status_code") and e.status_code >= 500:
                    last_exception = e
                    if attempt < max_retries:
                        delay = min(base_delay * (2**attempt), max_delay)
                        print(f"   âš ï¸ æœåŠ¡å™¨é”™è¯¯ {e.status_code}ï¼Œç­‰å¾… {delay:.1f} ç§’")
                        time.sleep(delay)
                        continue
                raise  # å…¶ä»– API é”™è¯¯ä¸é‡è¯•

        raise last_exception

    # æ¼”ç¤ºä»£ç ï¼ˆä¸å®é™…è¿è¡Œï¼Œé¿å…æµªè´¹ API è°ƒç”¨ï¼‰
    print("""
ä½¿ç”¨ç¤ºä¾‹ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
response = call_with_retry(
    client=client,
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    max_retries=3,
    base_delay=1.0
)
print(response.choices[0].message.content)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ Tenacity åº“ ====================


def tenacity_usage():
    """ä½¿ç”¨ Tenacity åº“è¿›è¡Œé‡è¯•"""
    print("\n" + "=" * 60)
    print("ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨ Tenacity åº“")
    print("=" * 60)

    print("""
ğŸ’¡ Tenacity æ˜¯ Python æœ€æµè¡Œçš„é‡è¯•åº“
   æä¾›ä¸°å¯Œçš„é‡è¯•ç­–ç•¥é…ç½®

å®‰è£…ï¼špip install tenacity
    """)

    from tenacity import (
        retry,
        stop_after_attempt,
        wait_exponential,
        retry_if_exception_type,
        before_sleep_log,
    )
    import logging

    print("""
Tenacity å¸¸ç”¨é…ç½®ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é…ç½®                     â”‚ è¯´æ˜                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ stop_after_attempt(n)    â”‚ æœ€å¤šé‡è¯• n æ¬¡                      â”‚
â”‚ stop_after_delay(s)      â”‚ æ€»å…±æœ€å¤šç­‰å¾… s ç§’                  â”‚
â”‚ wait_exponential(...)    â”‚ æŒ‡æ•°é€€é¿ç­‰å¾…                       â”‚
â”‚ wait_random(min, max)    â”‚ éšæœºç­‰å¾…                           â”‚
â”‚ retry_if_exception_type  â”‚ åªå¯¹ç‰¹å®šå¼‚å¸¸é‡è¯•                   â”‚
â”‚ before_sleep_log         â”‚ é‡è¯•å‰è®°å½•æ—¥å¿—                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    print("""
æ¨èé…ç½®ç¤ºä¾‹ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from openai import RateLimitError, APIConnectionError

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    retry=retry_if_exception_type((RateLimitError, APIConnectionError)),
)
def call_llm(client, messages):
    return client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages
    )

# ä½¿ç”¨
response = call_llm(client, [{"role": "user", "content": "ä½ å¥½"}])
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)

    # å®é™…æ¼”ç¤º
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡å®é™…æ¼”ç¤º")
        return

    from openai import OpenAI, RateLimitError, APIConnectionError

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((RateLimitError, APIConnectionError)),
    )
    def call_llm_with_retry(client, messages):
        return client.chat.completions.create(
            model="gpt-3.5-turbo", messages=messages, max_tokens=20
        )

    print("\nğŸ“ ä½¿ç”¨ Tenacity å®é™…æµ‹è¯•ï¼š")
    try:
        client = OpenAI()
        response = call_llm_with_retry(
            client, [{"role": "user", "content": "è¯´ Hello"}]
        )
        print(f"   âœ… æˆåŠŸ: {response.choices[0].message.content}")
    except Exception as e:
        print(f"   âŒ æœ€ç»ˆå¤±è´¥: {e}")


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿäº§çº§é”™è¯¯å¤„ç† ====================


def production_error_handling():
    """ç”Ÿäº§çº§é”™è¯¯å¤„ç†æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿäº§çº§é”™è¯¯å¤„ç†æ¨¡å¼")
    print("=" * 60)

    print("""
ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import logging
from typing import Optional
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    after_log,
)
from openai import (
    OpenAI,
    RateLimitError,
    APIConnectionError,
    APITimeoutError,
)

logger = logging.getLogger(__name__)

class LLMService:
    '''ç”Ÿäº§çº§ LLM æœåŠ¡å°è£…'''
    
    def __init__(self, api_key: Optional[str] = None):
        self.client = OpenAI(api_key=api_key)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type((
            RateLimitError,
            APIConnectionError,
            APITimeoutError,
        )),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        after=after_log(logger, logging.DEBUG),
    )
    def chat(
        self,
        messages: list,
        model: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        timeout: float = 30.0,
    ) -> str:
        '''å‘é€èŠå¤©è¯·æ±‚'''
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )
        return response.choices[0].message.content
    
    def safe_chat(
        self,
        messages: list,
        **kwargs
    ) -> tuple[str, Optional[Exception]]:
        '''å®‰å…¨è°ƒç”¨ï¼Œè¿”å›ç»“æœå’Œé”™è¯¯'''
        try:
            result = self.chat(messages, **kwargs)
            return result, None
        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return "", e

# ä½¿ç”¨ç¤ºä¾‹
service = LLMService()

# æ–¹å¼1ï¼šå¯èƒ½æŠ›å‡ºå¼‚å¸¸
try:
    result = service.chat([{"role": "user", "content": "ä½ å¥½"}])
except Exception as e:
    print(f"å¤±è´¥: {e}")

# æ–¹å¼2ï¼šå®‰å…¨è°ƒç”¨
result, error = service.safe_chat([{"role": "user", "content": "ä½ å¥½"}])
if error:
    # å¤„ç†é”™è¯¯ï¼ˆå¦‚è¿”å›é»˜è®¤å›å¤ï¼‰
    result = "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šç»ƒä¹ ä¸æ€è€ƒ ====================


def exercises():
    """ç»ƒä¹ é¢˜"""
    print("\n" + "=" * 60)
    print("ç»ƒä¹ ä¸æ€è€ƒ")
    print("=" * 60)

    exercises_text = """
ç»ƒä¹  1ï¼šè‡ªå®šä¹‰é‡è¯•ç­–ç•¥
    åˆ›å»ºä¸€ä¸ªé‡è¯•å‡½æ•°ï¼Œæ»¡è¶³ï¼š
    - æœ€å¤šé‡è¯• 5 æ¬¡
    - æŒ‡æ•°é€€é¿ï¼Œåˆå§‹ 2 ç§’ï¼Œæœ€å¤§ 30 ç§’
    - åªå¯¹ RateLimitError å’Œç½‘ç»œé”™è¯¯é‡è¯•
    - æ¯æ¬¡é‡è¯•æ‰“å°æ—¥å¿—

ç»ƒä¹  2ï¼šæ•…éšœè½¬ç§»
    å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œå½“ OpenAI è°ƒç”¨å¤±è´¥æ—¶ï¼Œ
    è‡ªåŠ¨åˆ‡æ¢åˆ° Ollama æœ¬åœ°æ¨¡å‹ã€‚

ç»ƒä¹  3ï¼šç†”æ–­å™¨æ¨¡å¼
    å®ç°ä¸€ä¸ªç®€å•çš„ç†”æ–­å™¨ï¼š
    - è¿ç»­å¤±è´¥ 5 æ¬¡åï¼Œæš‚åœè°ƒç”¨ 60 ç§’
    - 60 ç§’åå°è¯•æ¢å¤

ç»ƒä¹  4ï¼šç›‘æ§å’Œå‘Šè­¦
    è®°å½•æ¯æ¬¡ API è°ƒç”¨çš„ç»“æœï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰ï¼Œ
    å½“é”™è¯¯ç‡è¶…è¿‡ 10% æ—¶æ‰“å°å‘Šè­¦ã€‚

æ€è€ƒé¢˜ï¼š
    1. ä¸ºä»€ä¹ˆ 429 é”™è¯¯éœ€è¦ç­‰å¾…è€Œä¸æ˜¯ç«‹å³é‡è¯•ï¼Ÿ
    2. å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªè¯·æ±‚æ˜¯å¦"å¹‚ç­‰"ï¼Œå¯ä»¥å®‰å…¨é‡è¯•ï¼Ÿ
    3. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥æ”¾å¼ƒé‡è¯•ï¼Œç›´æ¥è¿”å›é”™è¯¯ï¼Ÿ
    """
    print(exercises_text)


# ==================== ä¸»å‡½æ•° ====================


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶")
    print("=" * 60)
    print("ğŸ’¡ æœ¬è¯¾ç¨‹ä»‹ç» LLM API çš„é”™è¯¯å¤„ç†æœ€ä½³å®è·µ")
    print("=" * 60)

    try:
        error_types_introduction()
        basic_error_handling()
        manual_retry_implementation()
        tenacity_usage()
        production_error_handling()
        exercises()
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        return

    print("\n" + "=" * 60)
    print("âœ… è¯¾ç¨‹å®Œæˆï¼")
    print("ä¸‹ä¸€æ­¥ï¼š08-rate-limiting.pyï¼ˆé€Ÿç‡é™åˆ¶ä¸å¹¶å‘æ§åˆ¶ï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    main()
